{"version":3,"sources":["index.js","create_detector.js","blazepose_mediapipe/detector.js","constants.js","shared/calculators/mask_util.js","blazepose_mediapipe/detector_utils.js","blazepose_mediapipe/constants.js","blazepose_tfjs/detector.js","shared/calculators/calculate_alignment_points_rects.js","shared/calculators/detection_to_rect.js","shared/calculators/image_utils.js","shared/calculators/calculate_inverse_matrix.js","shared/calculators/calculate_landmark_projection.js","shared/calculators/calculate_score_copy.js","shared/calculators/calculate_world_landmark_projection.js","shared/calculators/constants.js","shared/calculators/convert_image_to_tensor.js","shared/calculators/get_rotated_sub_rect_to_rect_transformation_matrix.js","shared/calculators/shift_image_value.js","shared/calculators/create_ssd_anchors.js","shared/calculators/detector_result.js","shared/calculators/split_detection_result.js","shared/calculators/is_video.js","shared/calculators/landmarks_to_detection.js","shared/calculators/non_max_suppression.js","shared/calculators/normalized_keypoints_to_keypoints.js","shared/calculators/refine_landmarks_from_heatmap.js","shared/calculators/remove_detection_letterbox.js","shared/calculators/remove_landmark_letterbox.js","shared/calculators/segmentation_smoothing.js","shared/calculators/tensors_to_detections.js","shared/calculators/tensors_to_landmarks.js","shared/calculators/sigmoid.js","shared/calculators/tensors_to_segmentation.js","shared/calculators/transform_rect.js","shared/filters/keypoints_smoothing.js","shared/calculators/get_object_scale.js","shared/calculators/keypoints_to_normalized_keypoints.js","shared/filters/keypoints_one_euro_filter.js","shared/filters/one_euro_filter.js","shared/filters/low_pass_filter.js","shared/filters/keypoints_velocity_filter.js","shared/filters/relative_velocity_filter.js","shared/filters/visibility_smoothing.js","blazepose_tfjs/constants.js","blazepose_tfjs/detector_utils.js","movenet/detector.js","calculators/bounding_box_tracker.js","calculators/tracker.js","calculators/tracker_utils.js","calculators/keypoint_tracker.js","calculators/types.js","types.js","util.js","movenet/constants.js","movenet/crop_utils.js","movenet/detector_utils.js","posenet/detector.js","posenet/calculators/decode_multiple_poses.js","posenet/constants.js","posenet/calculators/build_part_with_score_queue.js","posenet/calculators/max_heap.js","posenet/calculators/decode_multiple_poses_util.js","posenet/calculators/decode_single_pose.js","posenet/calculators/decode_single_pose_util.js","posenet/calculators/flip_poses.js","posenet/calculators/scale_poses.js","posenet/detector_utils.js","posenet/load_utils.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;ACFA,ADGA;ACFA,ADGA;ACFA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,ACHA,AFMA,ADGA,AIZA;AFOA,ACHA,AFMA,ADGA,AIZA;AFOA,ACHA,AFMA,ADGA,AIZA;AFOA,AGTA,AFMA,AFMA,ADGA,AIZA;AFOA,AGTA,AFMA,AFMA,ADGA,AIZA;AFOA,AGTA,AFMA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AFMA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AFMA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AFMA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AIZA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AJYA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AJYA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AJYA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ALeA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ALeA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ALeA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ACHA,ANkBA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ACHA,ANkBA;AELA,AJYA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,ACHA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AGTA,AFMA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AGTA,AFMA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AGTA,AFMA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AGTA,ACHA,AHSA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,ADGA,AQxBA,AGTA,ACHA,AHSA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,AHSA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,AJYA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,AJYA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,AJYA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ALeA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ALeA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ALeA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ANkBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ANkBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ANkBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,ACHA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA;AFOA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,APqBA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AQxBA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA;AhBiDA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,ANkBA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,AlBsDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,AlBsDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,AlBsDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AOrBA,AGTA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,Ac1CA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ANkBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ANkBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ANkBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,APqBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,APqBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,APqBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ARwBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ARwBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ARwBA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,AT2BA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,AT2BA,AGTA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ANkBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,APqBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,APqBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,APqBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AYpCA,ACHA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA;AnB0DA,AGTA,AENA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AS3BA;A5BqFA,AKfA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AS3BA;A5BqFA,AKfA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AWjCA,AHSA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AS3BA;A5BqFA,AKfA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AS3BA,ACHA;A7BwFA,AKfA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AS3BA,ACHA;A7BwFA,AKfA,AJYA,AFMA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA;A7BwFA,AKfA,ANkBA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA;A7BwFA,AKfA,ANkBA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA;A7BwFA,AKfA,ANkBA,AU9BA,ACHA,ACHA,ACHA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA;A7BwFA,AKfA,ANkBA,AU9BA,ACHA,AENA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA;A/B8FA,AKfA,AIZA,ACHA,AENA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,ACHA,ACHA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA;A/B8FA,AKfA,AIZA,AGTA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA;A/B8FA,AKfA,AIZA,AGTA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA;AhCiGA,AKfA,AIZA,AGTA,AENA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA;AhCiGA,AKfA,AIZA,AKfA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA;AhCiGA,AKfA,AIZA,AKfA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,AQxBA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,A2BjFA,AnByDA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,A2BjFA,AnByDA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,A2BjFA,AnByDA,APqBA,AavCA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AKfA,AGTA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AQxBA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AIZA,AQxBA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,ACHA;AjCoGA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA;AjCoGA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA;AjCoGA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,Ad0CA,AnByDA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA;AjCoGA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA,AIZA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA,AIZA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,AGTA,AFMA,ACHA,AENA,ACHA,AIZA,AHSA,AIZA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AKfA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AKfA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AKfA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,A2BjFA,AnByDA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,AQxBA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA;ArCgHA,AKfA,AYpCA,AV8BA,AQxBA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA;AxCyHA,AKfA,AYpCA,AV8BA,AQxBA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA;AxCyHA,AKfA,AYpCA,AV8BA,AQxBA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA;AxCyHA,AKfA,AYpCA,AV8BA,AQxBA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,AKfA,AYpCA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,AKfA,AYpCA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AYpCA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AYpCA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AYpCA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,A1B8EA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,A1B8EA,AFMA,APqBA,A2BjFA,AjCmGA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,A1B8EA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,ACHA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,ACHA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,ACHA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ADGA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ADGA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ADGA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,AFMA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,AFMA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,AFMA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,ACHA,AENA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AGTA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AGTA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,ACHA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AGTA,ACHA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AHSA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA;AzC4HA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA;AlDuJA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA;AlDuJA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA;AlDuJA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,ALeA,A3BiFA,AFMA,APqBA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ARwBA,A3BiFA,AT2BA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AENA,ACHA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ARwBA,A3BiFA,AT2BA,ANkBA,AoB5DA,AENA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ARwBA,A3BiFA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,A3BiFA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AS3BA,ACHA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,A3BiFA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,A3BiFA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ArC+GA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ArC+GA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ArC+GA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ACHA,AtCkHA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ACHA,AtCkHA,AT2BA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,ACHA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ADGA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ADGA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ADGA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ACHA,AFMA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AGTA,AIZA,AHSA,AMlBA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ACHA,AFMA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AENA,ACHA,AFMA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AENA,AGTA,ACHA,AT2BA,AU9BA,AIZA,AFMA,ACHA,AFMA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,ACHA,AFMA,A/C6IA,ANkBA,AsBlEA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,ACHA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AGTA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AGTA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AGTA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,ACHA,AU9BA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,ACHA,AKfA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,A/C6IA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ADGA,ADGA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AzD2KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,AFMA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AzD2KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,AFMA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AzD2KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,AFMA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,AFMA,AGTA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ACHA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,ACHA,AOrBA,AGTA,ACHA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,AQxBA,AGTA,ACHA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,AQxBA,AIZA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,AQxBA,AIZA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1D8KA,AgBhDA,AIZA,AQxBA,AIZA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AQxBA,AIZA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AENA,AGTA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA,AWjCA;AnD0JA,A0C9HA,ArC+GA,AsClHA,AKfA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA;AxCyHA,A0C9HA,ArC+GA,AsClHA,AKfA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,A1C8HA,AIZA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA,AYpCA;AxCyHA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AMlBA,ACHA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ACHA,AtCkHA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;A5BqFA,A0C9HA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,A2CjIA,AFMA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AczCA,ArC+GA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AU9BA,AENA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ACHA,ANkBA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ALeA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ALeA,AU9BA,ArC+GA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AYpCA,AIZA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA,A3BiFA;AvBsEA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AyC3HA,AOrBA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AgDhJA,AT2BA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA,ALeA;AlDuJA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA,AgBhDA;AvDsKA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AvCsHA,AuCrHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\nvar __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    Object.defineProperty(o, k2, { enumerable: true, get: function() { return m[k]; } });\n}) : (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    o[k2] = m[k];\n}));\nvar __exportStar = (this && this.__exportStar) || function(m, exports) {\n    for (var p in m) if (p !== \"default\" && !exports.hasOwnProperty(p)) __createBinding(exports, m, p);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.movenet = exports.calculators = exports.util = void 0;\nvar create_detector_1 = require(\"./create_detector\");\nObject.defineProperty(exports, \"createDetector\", { enumerable: true, get: function () { return create_detector_1.createDetector; } });\n// Supported models enum.\n__exportStar(require(\"./types\"), exports);\nvar types_1 = require(\"./calculators/types\");\nObject.defineProperty(exports, \"TrackerType\", { enumerable: true, get: function () { return types_1.TrackerType; } });\n// Second level exports.\n// Utils for rendering.\nvar util = require(\"./util\");\nexports.util = util;\n// General calculators.\nvar keypoints_to_normalized_keypoints_1 = require(\"./shared/calculators/keypoints_to_normalized_keypoints\");\nvar calculators = { keypointsToNormalizedKeypoints: keypoints_to_normalized_keypoints_1.keypointsToNormalizedKeypoints };\nexports.calculators = calculators;\n// MoveNet model types.\nvar constants_1 = require(\"./movenet/constants\");\nvar movenet = {\n    modelType: {\n        'SINGLEPOSE_LIGHTNING': constants_1.SINGLEPOSE_LIGHTNING,\n        'SINGLEPOSE_THUNDER': constants_1.SINGLEPOSE_THUNDER,\n        'MULTIPOSE_LIGHTNING': constants_1.MULTIPOSE_LIGHTNING\n    }\n};\nexports.movenet = movenet;\n//# sourceMappingURL=index.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.createDetector = void 0;\nvar detector_1 = require(\"./blazepose_mediapipe/detector\");\nvar detector_2 = require(\"./blazepose_tfjs/detector\");\nvar detector_3 = require(\"./movenet/detector\");\nvar detector_4 = require(\"./posenet/detector\");\nvar types_1 = require(\"./types\");\n/**\n * Create a pose detector instance.\n *\n * @param model The name of the pipeline to load.\n */\nfunction createDetector(model, modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, runtime;\n        return __generator(this, function (_a) {\n            switch (model) {\n                case types_1.SupportedModels.PoseNet:\n                    return [2 /*return*/, detector_4.load(modelConfig)];\n                case types_1.SupportedModels.BlazePose:\n                    config = modelConfig;\n                    runtime = void 0;\n                    if (config != null) {\n                        if (config.runtime === 'tfjs') {\n                            return [2 /*return*/, detector_2.load(modelConfig)];\n                        }\n                        if (config.runtime === 'mediapipe') {\n                            return [2 /*return*/, detector_1.load(modelConfig)];\n                        }\n                        runtime = config.runtime;\n                    }\n                    throw new Error(\"Expect modelConfig.runtime to be either 'tfjs' \" +\n                        (\"or 'mediapipe', but got \" + runtime));\n                case types_1.SupportedModels.MoveNet:\n                    return [2 /*return*/, detector_3.load(modelConfig)];\n                default:\n                    throw new Error(model + \" is not a supported model name.\");\n            }\n            return [2 /*return*/];\n        });\n    });\n}\nexports.createDetector = createDetector;\n//# sourceMappingURL=create_detector.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.load = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar pose = require(\"@mediapipe/pose\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../constants\");\nvar mask_util_1 = require(\"../shared/calculators/mask_util\");\nvar detector_utils_1 = require(\"./detector_utils\");\nvar BlazePoseMediaPipeMask = /** @class */ (function () {\n    function BlazePoseMediaPipeMask(mask) {\n        this.mask = mask;\n    }\n    BlazePoseMediaPipeMask.prototype.toCanvasImageSource = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, this.mask];\n            });\n        });\n    };\n    BlazePoseMediaPipeMask.prototype.toImageData = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, mask_util_1.toImageDataLossy(this.mask)];\n            });\n        });\n    };\n    BlazePoseMediaPipeMask.prototype.toTensor = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, mask_util_1.toTensorLossy(this.mask)];\n            });\n        });\n    };\n    BlazePoseMediaPipeMask.prototype.getUnderlyingType = function () {\n        return 'canvasimagesource';\n    };\n    return BlazePoseMediaPipeMask;\n}());\nfunction maskValueToLabel(maskValue) {\n    mask_util_1.assertMaskValue(maskValue);\n    return 'person';\n}\n/**\n * MediaPipe detector class.\n */\nvar BlazePoseMediaPipeDetector = /** @class */ (function () {\n    // Should not be called outside.\n    function BlazePoseMediaPipeDetector(config) {\n        var _this = this;\n        // This will be filled out by asynchronous calls to onResults. They will be\n        // stable after `await send` is called on the pose solution.\n        this.width = 0;\n        this.height = 0;\n        this.selfieMode = false;\n        this.poseSolution = new pose.Pose({\n            locateFile: function (path, base) {\n                if (config.solutionPath) {\n                    var solutionPath = config.solutionPath.replace(/\\/+$/, '');\n                    return solutionPath + \"/\" + path;\n                }\n                return base + \"/\" + path;\n            }\n        });\n        var modelComplexity;\n        switch (config.modelType) {\n            case 'lite':\n                modelComplexity = 0;\n                break;\n            case 'heavy':\n                modelComplexity = 2;\n                break;\n            case 'full':\n            default:\n                modelComplexity = 1;\n                break;\n        }\n        this.poseSolution.setOptions({\n            modelComplexity: modelComplexity,\n            smoothLandmarks: config.enableSmoothing,\n            enableSegmentation: config.enableSegmentation,\n            smoothSegmentation: config.smoothSegmentation,\n            selfieMode: this.selfieMode,\n        });\n        this.poseSolution.onResults(function (results) {\n            _this.height = results.image.height;\n            _this.width = results.image.width;\n            if (results.poseLandmarks == null) {\n                _this.poses = [];\n            }\n            else {\n                var pose_1 = _this.translateOutput(results.poseLandmarks, results.poseWorldLandmarks);\n                if (results.segmentationMask) {\n                    pose_1.segmentation = {\n                        maskValueToLabel: maskValueToLabel,\n                        mask: new BlazePoseMediaPipeMask(results.segmentationMask)\n                    };\n                }\n                _this.poses = [pose_1];\n            }\n        });\n    }\n    BlazePoseMediaPipeDetector.prototype.translateOutput = function (pose, pose3D) {\n        var _this = this;\n        var output = {\n            keypoints: pose.map(function (landmark, i) { return ({\n                x: landmark.x * _this.width,\n                y: landmark.y * _this.height,\n                z: landmark.z,\n                score: landmark.visibility,\n                name: constants_1.BLAZEPOSE_KEYPOINTS[i]\n            }); })\n        };\n        if (pose3D != null) {\n            output.keypoints3D = pose3D.map(function (landmark, i) { return ({\n                x: landmark.x,\n                y: landmark.y,\n                z: landmark.z,\n                score: landmark.visibility,\n                name: constants_1.BLAZEPOSE_KEYPOINTS[i]\n            }); });\n        }\n        return output;\n    };\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * It returns a single pose or multiple poses based on the maxPose parameter\n     * from the `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param config Optional.\n     *       maxPoses: Optional. Max number of poses to estimate.\n     *       When maxPoses = 1, a single pose is detected, it is usually much\n     *       more efficient than maxPoses > 1. When maxPoses > 1, multiple poses\n     *       are detected.\n     *\n     *       flipHorizontal: Optional. Default to false. When image data comes\n     *       from camera, the result has to flip horizontally.\n     *\n     *       enableSmoothing: Optional. Default to true. Smooth pose landmarks\n     *       coordinates and visibility scores to reduce jitter.\n     *\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     *     a tensor, which doesn't have timestamp info. Or to override timestamp\n     *     in a video.\n     *\n     * @return An array of `Pose`s.\n     */\n    BlazePoseMediaPipeDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            var _a, _b;\n            return __generator(this, function (_c) {\n                switch (_c.label) {\n                    case 0:\n                        if (estimationConfig && estimationConfig.flipHorizontal &&\n                            (estimationConfig.flipHorizontal !== this.selfieMode)) {\n                            this.selfieMode = estimationConfig.flipHorizontal;\n                            this.poseSolution.setOptions({\n                                selfieMode: this.selfieMode,\n                            });\n                        }\n                        if (!(image instanceof tf.Tensor)) return [3 /*break*/, 2];\n                        _b = ImageData.bind;\n                        return [4 /*yield*/, tf.browser.toPixels(image)];\n                    case 1:\n                        _a = new (_b.apply(ImageData, [void 0, _c.sent(), image.shape[1], image.shape[0]]))();\n                        return [3 /*break*/, 3];\n                    case 2:\n                        _a = image;\n                        _c.label = 3;\n                    case 3:\n                        // Cast to GL TexImageSource types.\n                        image = _a;\n                        return [4 /*yield*/, this.poseSolution.send({ image: image }, timestamp)];\n                    case 4:\n                        _c.sent();\n                        return [2 /*return*/, this.poses];\n                }\n            });\n        });\n    };\n    BlazePoseMediaPipeDetector.prototype.dispose = function () {\n        this.poseSolution.close();\n    };\n    BlazePoseMediaPipeDetector.prototype.reset = function () {\n        this.poseSolution.reset();\n    };\n    BlazePoseMediaPipeDetector.prototype.initialize = function () {\n        return this.poseSolution.initialize();\n    };\n    return BlazePoseMediaPipeDetector;\n}());\n/**\n * Loads the MediaPipe solution.\n *\n * @param modelConfig ModelConfig object that contains parameters for\n * the BlazePose loading process. Please find more details of each parameters\n * in the documentation of the `BlazePoseMediaPipeModelConfig` interface.\n */\nfunction load(modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, result;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    result = new BlazePoseMediaPipeDetector(config);\n                    return [4 /*yield*/, result.initialize()];\n                case 1:\n                    _a.sent();\n                    return [2 /*return*/, result];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BLAZEPOSE_CONNECTED_KEYPOINTS_PAIRS = exports.COCO_CONNECTED_KEYPOINTS_PAIRS = exports.COCO_KEYPOINTS_BY_SIDE = exports.BLAZEPOSE_KEYPOINTS_BY_SIDE = exports.BLAZEPOSE_KEYPOINTS = exports.COCO_KEYPOINTS = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Don't change the order. The order needs to be consistent with the model\n// keypoint result list.\nexports.COCO_KEYPOINTS = [\n    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder',\n    'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist',\n    'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle',\n    'right_ankle'\n];\n// Don't change the order. The order needs to be consistent with the model\n// keypoint result list.\nexports.BLAZEPOSE_KEYPOINTS = [\n    'nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index'\n];\nexports.BLAZEPOSE_KEYPOINTS_BY_SIDE = {\n    left: [1, 2, 3, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31],\n    right: [4, 5, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32],\n    middle: [0]\n};\nexports.COCO_KEYPOINTS_BY_SIDE = {\n    left: [1, 3, 5, 7, 9, 11, 13, 15],\n    right: [2, 4, 6, 8, 10, 12, 14, 16],\n    middle: [0]\n};\nexports.COCO_CONNECTED_KEYPOINTS_PAIRS = [\n    [0, 1], [0, 2], [1, 3], [2, 4], [5, 6], [5, 7], [5, 11], [6, 8], [6, 12],\n    [7, 9], [8, 10], [11, 12], [11, 13], [12, 14], [13, 15], [14, 16]\n];\nexports.BLAZEPOSE_CONNECTED_KEYPOINTS_PAIRS = [\n    [0, 1], [0, 4], [1, 2], [2, 3], [3, 7], [4, 5],\n    [5, 6], [6, 8], [9, 10], [11, 12], [11, 13], [11, 23],\n    [12, 14], [14, 16], [12, 24], [13, 15], [15, 17], [16, 18],\n    [16, 20], [15, 17], [15, 19], [15, 21], [16, 22], [17, 19],\n    [18, 20], [23, 25], [23, 24], [24, 26], [25, 27], [26, 28],\n    [27, 29], [28, 30], [27, 31], [28, 32], [29, 31], [30, 32]\n];\n//# sourceMappingURL=constants.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.assertMaskValue = exports.toTensorLossy = exports.toImageDataLossy = exports.toHTMLCanvasElementLossy = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction toNumber(value) {\n    return value instanceof SVGAnimatedLength ? value.baseVal.value : value;\n}\n/**\n * Converts input image to an HTMLCanvasElement. Note that converting\n * back from the output of this function to imageData or a Tensor will be lossy\n * due to premultiplied alpha color values. For more details please reference:\n * https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/putImageData#data_loss_due_to_browser_optimization\n * @param image Input image.\n *\n * @returns Converted HTMLCanvasElement.\n */\nfunction toHTMLCanvasElementLossy(image) {\n    return __awaiter(this, void 0, void 0, function () {\n        var canvas, ctx;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    canvas = document.createElement('canvas');\n                    if (!(image instanceof tf.Tensor)) return [3 /*break*/, 2];\n                    return [4 /*yield*/, tf.browser.toPixels(image, canvas)];\n                case 1:\n                    _a.sent();\n                    return [3 /*break*/, 3];\n                case 2:\n                    canvas.width = toNumber(image.width);\n                    canvas.height = toNumber(image.height);\n                    ctx = canvas.getContext('2d');\n                    if (image instanceof ImageData) {\n                        ctx.putImageData(image, 0, 0);\n                    }\n                    else {\n                        ctx.drawImage(image, 0, 0);\n                    }\n                    _a.label = 3;\n                case 3: return [2 /*return*/, canvas];\n            }\n        });\n    });\n}\nexports.toHTMLCanvasElementLossy = toHTMLCanvasElementLossy;\n/**\n * Converts input image to ImageData. Note that converting\n * from a CanvasImageSource will be lossy due to premultiplied alpha color\n * values. For more details please reference:\n * https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/putImageData#data_loss_due_to_browser_optimization\n * @param image Input image.\n *\n * @returns Converted ImageData.\n */\nfunction toImageDataLossy(image) {\n    return __awaiter(this, void 0, void 0, function () {\n        var _a, height, width, _b, canvas, ctx;\n        return __generator(this, function (_c) {\n            switch (_c.label) {\n                case 0:\n                    if (!(image instanceof tf.Tensor)) return [3 /*break*/, 2];\n                    _a = image.shape.slice(0, 2), height = _a[0], width = _a[1];\n                    _b = ImageData.bind;\n                    return [4 /*yield*/, tf.browser.toPixels(image)];\n                case 1: return [2 /*return*/, new (_b.apply(ImageData, [void 0, _c.sent(), width, height]))()];\n                case 2:\n                    canvas = document.createElement('canvas');\n                    ctx = canvas.getContext('2d');\n                    canvas.width = toNumber(image.width);\n                    canvas.height = toNumber(image.height);\n                    ctx.drawImage(image, 0, 0);\n                    return [2 /*return*/, ctx.getImageData(0, 0, canvas.width, canvas.height)];\n            }\n        });\n    });\n}\nexports.toImageDataLossy = toImageDataLossy;\n/**\n * Converts input image to Tensor. Note that converting\n * from a CanvasImageSource will be lossy due to premultiplied alpha color\n * values. For more details please reference:\n * https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/putImageData#data_loss_due_to_browser_optimization\n * @param image Input image.\n *\n * @returns Converted Tensor.\n */\nfunction toTensorLossy(image) {\n    return __awaiter(this, void 0, void 0, function () {\n        var pixelsInput, _a;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    if (!(image instanceof SVGImageElement || image instanceof OffscreenCanvas)) return [3 /*break*/, 2];\n                    return [4 /*yield*/, toHTMLCanvasElementLossy(image)];\n                case 1:\n                    _a = _b.sent();\n                    return [3 /*break*/, 3];\n                case 2:\n                    _a = image;\n                    _b.label = 3;\n                case 3:\n                    pixelsInput = _a;\n                    return [2 /*return*/, tf.browser.fromPixels(pixelsInput, 4)];\n            }\n        });\n    });\n}\nexports.toTensorLossy = toTensorLossy;\nfunction assertMaskValue(maskValue) {\n    if (maskValue < 0 || maskValue >= 256) {\n        throw new Error(\"Mask value must be in range [0, 255] but got \" + maskValue);\n    }\n    if (!Number.isInteger(maskValue)) {\n        throw new Error(\"Mask value must be an integer but got \" + maskValue);\n    }\n}\nexports.assertMaskValue = assertMaskValue;\n//# sourceMappingURL=mask_util.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.validateEstimationConfig = exports.validateModelConfig = void 0;\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    if (modelConfig == null) {\n        return __assign({}, constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG);\n    }\n    var config = __assign({}, modelConfig);\n    config.runtime = 'mediapipe';\n    if (config.enableSegmentation == null) {\n        config.enableSegmentation =\n            constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSegmentation;\n    }\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSmoothing;\n    }\n    if (config.smoothSegmentation == null) {\n        config.smoothSegmentation =\n            constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.smoothSegmentation;\n    }\n    if (config.modelType == null) {\n        config.modelType = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.modelType;\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    if (estimationConfig == null) {\n        return __assign({}, constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG);\n    }\n    var config = __assign({}, estimationConfig);\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        throw new Error('Multi-pose detection is not implemented yet. Please set maxPoses ' +\n            'to 1.');\n    }\n    if (config.flipHorizontal == null) {\n        config.flipHorizontal = constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG.flipHorizontal;\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = exports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = void 0;\nexports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = {\n    runtime: 'mediapipe',\n    enableSmoothing: true,\n    enableSegmentation: false,\n    smoothSegmentation: true,\n    modelType: 'full'\n};\nexports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.load = void 0;\nvar tfconv = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../constants\");\nvar calculate_alignment_points_rects_1 = require(\"../shared/calculators/calculate_alignment_points_rects\");\nvar calculate_inverse_matrix_1 = require(\"../shared/calculators/calculate_inverse_matrix\");\nvar calculate_landmark_projection_1 = require(\"../shared/calculators/calculate_landmark_projection\");\nvar calculate_score_copy_1 = require(\"../shared/calculators/calculate_score_copy\");\nvar calculate_world_landmark_projection_1 = require(\"../shared/calculators/calculate_world_landmark_projection\");\nvar constants_2 = require(\"../shared/calculators/constants\");\nvar convert_image_to_tensor_1 = require(\"../shared/calculators/convert_image_to_tensor\");\nvar create_ssd_anchors_1 = require(\"../shared/calculators/create_ssd_anchors\");\nvar detector_result_1 = require(\"../shared/calculators/detector_result\");\nvar image_utils_1 = require(\"../shared/calculators/image_utils\");\nvar is_video_1 = require(\"../shared/calculators/is_video\");\nvar landmarks_to_detection_1 = require(\"../shared/calculators/landmarks_to_detection\");\nvar mask_util_1 = require(\"../shared/calculators/mask_util\");\nvar non_max_suppression_1 = require(\"../shared/calculators/non_max_suppression\");\nvar normalized_keypoints_to_keypoints_1 = require(\"../shared/calculators/normalized_keypoints_to_keypoints\");\nvar refine_landmarks_from_heatmap_1 = require(\"../shared/calculators/refine_landmarks_from_heatmap\");\nvar remove_detection_letterbox_1 = require(\"../shared/calculators/remove_detection_letterbox\");\nvar remove_landmark_letterbox_1 = require(\"../shared/calculators/remove_landmark_letterbox\");\nvar segmentation_smoothing_1 = require(\"../shared/calculators/segmentation_smoothing\");\nvar tensors_to_detections_1 = require(\"../shared/calculators/tensors_to_detections\");\nvar tensors_to_landmarks_1 = require(\"../shared/calculators/tensors_to_landmarks\");\nvar tensors_to_segmentation_1 = require(\"../shared/calculators/tensors_to_segmentation\");\nvar transform_rect_1 = require(\"../shared/calculators/transform_rect\");\nvar keypoints_smoothing_1 = require(\"../shared/filters/keypoints_smoothing\");\nvar visibility_smoothing_1 = require(\"../shared/filters/visibility_smoothing\");\nvar constants = require(\"./constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\nvar BlazePoseTfjsMask = /** @class */ (function () {\n    function BlazePoseTfjsMask(mask) {\n        this.mask = mask;\n    }\n    BlazePoseTfjsMask.prototype.toCanvasImageSource = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, mask_util_1.toHTMLCanvasElementLossy(this.mask)];\n            });\n        });\n    };\n    BlazePoseTfjsMask.prototype.toImageData = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, mask_util_1.toImageDataLossy(this.mask)];\n            });\n        });\n    };\n    BlazePoseTfjsMask.prototype.toTensor = function () {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                return [2 /*return*/, this.mask];\n            });\n        });\n    };\n    BlazePoseTfjsMask.prototype.getUnderlyingType = function () {\n        return 'tensor';\n    };\n    return BlazePoseTfjsMask;\n}());\nfunction maskValueToLabel(maskValue) {\n    mask_util_1.assertMaskValue(maskValue);\n    return 'person';\n}\n/**\n * BlazePose detector class.\n */\nvar BlazePoseTfjsDetector = /** @class */ (function () {\n    function BlazePoseTfjsDetector(detectorModel, landmarkModel, enableSmoothing, enableSegmentation, smoothSegmentation, modelType) {\n        this.detectorModel = detectorModel;\n        this.landmarkModel = landmarkModel;\n        this.enableSmoothing = enableSmoothing;\n        this.enableSegmentation = enableSegmentation;\n        this.smoothSegmentation = smoothSegmentation;\n        this.modelType = modelType;\n        // Store global states.\n        this.regionOfInterest = null;\n        this.prevFilteredSegmentationMask = null;\n        this.anchors =\n            create_ssd_anchors_1.createSsdAnchors(constants.BLAZEPOSE_DETECTOR_ANCHOR_CONFIGURATION);\n        var anchorW = tf.tensor1d(this.anchors.map(function (a) { return a.width; }));\n        var anchorH = tf.tensor1d(this.anchors.map(function (a) { return a.height; }));\n        var anchorX = tf.tensor1d(this.anchors.map(function (a) { return a.xCenter; }));\n        var anchorY = tf.tensor1d(this.anchors.map(function (a) { return a.yCenter; }));\n        this.anchorTensor = { x: anchorX, y: anchorY, w: anchorW, h: anchorH };\n        this.prevFilteredSegmentationMask =\n            this.enableSegmentation ? tf.tensor2d([], [0, 0]) : null;\n    }\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * It returns a single pose or multiple poses based on the maxPose parameter\n     * from the `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param estimationConfig Optional. See `BlazePoseTfjsEstimationConfig`\n     *       documentation for detail.\n     *\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     *     a tensor, which doesn't have timestamp info. Or to override timestamp\n     *     in a video.\n     *\n     * @return An array of `Pose`s.\n     */\n    // TF.js implementation of the mediapipe pose detection pipeline.\n    // ref graph:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_cpu.pbtxt\n    BlazePoseTfjsDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            var config, imageSize, image3d, poseRect, detections, firstDetection, poseLandmarksByRoiResult, unfilteredPoseLandmarks, unfilteredAuxiliaryLandmarks, poseScore, unfilteredWorldLandmarks, unfilteredSegmentationMask, _a, poseLandmarks, auxiliaryLandmarks, poseWorldLandmarks, poseRectFromLandmarks, filteredSegmentationMask, keypoints, keypoints3D, pose, rgbaMask, segmentation;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        config = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        this.maxPoses = config.maxPoses;\n                        // User provided timestamp will override video's timestamp.\n                        if (timestamp != null) {\n                            this.timestamp = timestamp * constants_2.MILLISECOND_TO_MICRO_SECONDS;\n                        }\n                        else {\n                            // For static images, timestamp should be null.\n                            this.timestamp =\n                                is_video_1.isVideo(image) ? image.currentTime * constants_2.SECOND_TO_MICRO_SECONDS : null;\n                        }\n                        imageSize = image_utils_1.getImageSize(image);\n                        image3d = tf.tidy(function () { return tf.cast(image_utils_1.toImageTensor(image), 'float32'); });\n                        poseRect = this.regionOfInterest;\n                        if (!(poseRect == null)) return [3 /*break*/, 2];\n                        return [4 /*yield*/, this.detectPose(image3d)];\n                    case 1:\n                        detections = _b.sent();\n                        if (detections.length === 0) {\n                            this.reset();\n                            image3d.dispose();\n                            return [2 /*return*/, []];\n                        }\n                        firstDetection = detections[0];\n                        // Calculates region of interest based on pose detection, so that can be\n                        // used to detect landmarks.\n                        poseRect = this.poseDetectionToRoi(firstDetection, imageSize);\n                        _b.label = 2;\n                    case 2: return [4 /*yield*/, this.poseLandmarksByRoi(poseRect, image3d)];\n                    case 3:\n                        poseLandmarksByRoiResult = _b.sent();\n                        image3d.dispose();\n                        if (poseLandmarksByRoiResult == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        unfilteredPoseLandmarks = poseLandmarksByRoiResult.landmarks, unfilteredAuxiliaryLandmarks = poseLandmarksByRoiResult.auxiliaryLandmarks, poseScore = poseLandmarksByRoiResult.poseScore, unfilteredWorldLandmarks = poseLandmarksByRoiResult.worldLandmarks, unfilteredSegmentationMask = poseLandmarksByRoiResult.segmentationMask;\n                        _a = this.poseLandmarkFiltering(unfilteredPoseLandmarks, unfilteredAuxiliaryLandmarks, unfilteredWorldLandmarks, imageSize), poseLandmarks = _a.actualLandmarksFiltered, auxiliaryLandmarks = _a.auxiliaryLandmarksFiltered, poseWorldLandmarks = _a.actualWorldLandmarksFiltered;\n                        poseRectFromLandmarks = this.poseLandmarksToRoi(auxiliaryLandmarks, imageSize);\n                        // Cache roi for next image.\n                        this.regionOfInterest = poseRectFromLandmarks;\n                        filteredSegmentationMask = this.smoothSegmentation && unfilteredSegmentationMask != null ?\n                            this.poseSegmentationFiltering(unfilteredSegmentationMask) :\n                            unfilteredSegmentationMask;\n                        keypoints = poseLandmarks != null ?\n                            normalized_keypoints_to_keypoints_1.normalizedKeypointsToKeypoints(poseLandmarks, imageSize) :\n                            null;\n                        // Add keypoint name.\n                        if (keypoints != null) {\n                            keypoints.forEach(function (keypoint, i) {\n                                keypoint.name = constants_1.BLAZEPOSE_KEYPOINTS[i];\n                            });\n                        }\n                        keypoints3D = poseWorldLandmarks;\n                        // Add keypoint name.\n                        if (keypoints3D != null) {\n                            keypoints3D.forEach(function (keypoint3D, i) {\n                                keypoint3D.name = constants_1.BLAZEPOSE_KEYPOINTS[i];\n                            });\n                        }\n                        pose = { score: poseScore, keypoints: keypoints, keypoints3D: keypoints3D };\n                        if (filteredSegmentationMask !== null) {\n                            rgbaMask = tf.tidy(function () {\n                                var mask3D = \n                                // tslint:disable-next-line: no-unnecessary-type-assertion\n                                tf.expandDims(filteredSegmentationMask, 2);\n                                // Pads a pixel [r] to [r, 0].\n                                var rgMask = tf.pad(mask3D, [[0, 0], [0, 0], [0, 1]]);\n                                // Pads a pixel [r, 0] to [r, 0, 0, r].\n                                return tf.mirrorPad(rgMask, [[0, 0], [0, 0], [0, 2]], 'symmetric');\n                            });\n                            if (!this.smoothSegmentation) {\n                                tf.dispose(filteredSegmentationMask);\n                            }\n                            segmentation = {\n                                maskValueToLabel: maskValueToLabel,\n                                mask: new BlazePoseTfjsMask(rgbaMask)\n                            };\n                            pose.segmentation = segmentation;\n                        }\n                        return [2 /*return*/, [pose]];\n                }\n            });\n        });\n    };\n    BlazePoseTfjsDetector.prototype.poseSegmentationFiltering = function (segmentationMask) {\n        var prevMask = this.prevFilteredSegmentationMask;\n        if (prevMask.size === 0) {\n            this.prevFilteredSegmentationMask = segmentationMask;\n        }\n        else {\n            this.prevFilteredSegmentationMask = segmentation_smoothing_1.smoothSegmentation(prevMask, segmentationMask, constants.BLAZEPOSE_SEGMENTATION_SMOOTHING_CONFIG);\n            tf.dispose(segmentationMask);\n        }\n        tf.dispose(prevMask);\n        return this.prevFilteredSegmentationMask;\n    };\n    BlazePoseTfjsDetector.prototype.dispose = function () {\n        this.detectorModel.dispose();\n        this.landmarkModel.dispose();\n        tf.dispose([\n            this.anchorTensor.x, this.anchorTensor.y, this.anchorTensor.w,\n            this.anchorTensor.h, this.prevFilteredSegmentationMask\n        ]);\n    };\n    BlazePoseTfjsDetector.prototype.reset = function () {\n        this.regionOfInterest = null;\n        if (this.enableSegmentation) {\n            tf.dispose(this.prevFilteredSegmentationMask);\n            this.prevFilteredSegmentationMask = tf.tensor2d([], [0, 0]);\n        }\n        this.visibilitySmoothingFilterActual = null;\n        this.visibilitySmoothingFilterAuxiliary = null;\n        this.landmarksSmoothingFilterActual = null;\n        this.landmarksSmoothingFilterAuxiliary = null;\n    };\n    // Detects poses.\n    // Subgraph: PoseDetectionCpu.\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt\n    BlazePoseTfjsDetector.prototype.detectPose = function (image) {\n        return __awaiter(this, void 0, void 0, function () {\n            var _a, imageValueShifted, padding, detectionResult, _b, boxes, logits, detections, selectedDetections, newDetections;\n            return __generator(this, function (_c) {\n                switch (_c.label) {\n                    case 0:\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, constants.BLAZEPOSE_DETECTOR_IMAGE_TO_TENSOR_CONFIG), imageValueShifted = _a.imageTensor, padding = _a.padding;\n                        detectionResult = this.detectorModel.predict(imageValueShifted);\n                        _b = detector_result_1.detectorResult(detectionResult), boxes = _b.boxes, logits = _b.logits;\n                        return [4 /*yield*/, tensors_to_detections_1.tensorsToDetections([logits, boxes], this.anchorTensor, constants.BLAZEPOSE_TENSORS_TO_DETECTION_CONFIGURATION)];\n                    case 1:\n                        detections = _c.sent();\n                        if (detections.length === 0) {\n                            tf.dispose([imageValueShifted, detectionResult, logits, boxes]);\n                            return [2 /*return*/, detections];\n                        }\n                        return [4 /*yield*/, non_max_suppression_1.nonMaxSuppression(detections, this.maxPoses, constants.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION\n                                .minSuppressionThreshold, constants.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION\n                                .overlapType)];\n                    case 2:\n                        selectedDetections = _c.sent();\n                        newDetections = remove_detection_letterbox_1.removeDetectionLetterbox(selectedDetections, padding);\n                        tf.dispose([imageValueShifted, detectionResult, logits, boxes]);\n                        return [2 /*return*/, newDetections];\n                }\n            });\n        });\n    };\n    // Calculates region of interest from a detection.\n    // Subgraph: PoseDetectionToRoi.\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_detection_to_roi.pbtxt\n    // If detection is not null, imageSize should not be null either.\n    BlazePoseTfjsDetector.prototype.poseDetectionToRoi = function (detection, imageSize) {\n        var startKeypointIndex;\n        var endKeypointIndex;\n        // Converts pose detection into a rectangle based on center and scale\n        // alignment points.\n        startKeypointIndex = 0;\n        endKeypointIndex = 1;\n        // PoseDetectionToRoi: AlignmentPointsRectsCalculator.\n        var rawRoi = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(detection, imageSize, {\n            rotationVectorEndKeypointIndex: endKeypointIndex,\n            rotationVectorStartKeypointIndex: startKeypointIndex,\n            rotationVectorTargetAngleDegree: 90\n        });\n        // Expands pose rect with marging used during training.\n        // PoseDetectionToRoi: RectTransformationCalculation.\n        var roi = transform_rect_1.transformNormalizedRect(rawRoi, imageSize, constants.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG);\n        return roi;\n    };\n    // Predict pose landmarks  and optionally segmentation within an ROI\n    // subgraph: PoseLandmarksByRoiCpu\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt\n    // When poseRect is not null, image should not be null either.\n    BlazePoseTfjsDetector.prototype.poseLandmarksByRoi = function (roi, image) {\n        return __awaiter(this, void 0, void 0, function () {\n            var imageSize, _a, imageValueShifted, letterboxPadding, transformationMatrix, outputs, outputTensor, tensorsToPoseLandmarksAndSegmentationResult, roiLandmarks, roiAuxiliaryLandmarks, poseScore, roiWorldLandmarks, roiSegmentationMask, poseLandmarksAndSegmentationInverseProjectionResults;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        imageSize = image_utils_1.getImageSize(image);\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, constants.BLAZEPOSE_LANDMARK_IMAGE_TO_TENSOR_CONFIG, roi), imageValueShifted = _a.imageTensor, letterboxPadding = _a.padding, transformationMatrix = _a.transformationMatrix;\n                        if (this.modelType !== 'lite' && this.modelType !== 'full' &&\n                            this.modelType !== 'heavy') {\n                            throw new Error('Model type must be one of lite, full or heavy,' +\n                                (\"but got \" + this.modelType));\n                        }\n                        outputs = ['ld_3d', 'output_poseflag', 'activation_heatmap', 'world_3d'];\n                        if (this.enableSegmentation) {\n                            outputs.push('activation_segmentation');\n                        }\n                        outputTensor = this.landmarkModel.execute(imageValueShifted, outputs);\n                        return [4 /*yield*/, this.tensorsToPoseLandmarksAndSegmentation(outputTensor)];\n                    case 1:\n                        tensorsToPoseLandmarksAndSegmentationResult = _b.sent();\n                        if (tensorsToPoseLandmarksAndSegmentationResult == null) {\n                            tf.dispose(outputTensor);\n                            tf.dispose(imageValueShifted);\n                            return [2 /*return*/, null];\n                        }\n                        roiLandmarks = tensorsToPoseLandmarksAndSegmentationResult.landmarks, roiAuxiliaryLandmarks = tensorsToPoseLandmarksAndSegmentationResult.auxiliaryLandmarks, poseScore = tensorsToPoseLandmarksAndSegmentationResult.poseScore, roiWorldLandmarks = tensorsToPoseLandmarksAndSegmentationResult.worldLandmarks, roiSegmentationMask = tensorsToPoseLandmarksAndSegmentationResult.segmentationMask;\n                        return [4 /*yield*/, this.poseLandmarksAndSegmentationInverseProjection(imageSize, roi, letterboxPadding, transformationMatrix, roiLandmarks, roiAuxiliaryLandmarks, roiWorldLandmarks, roiSegmentationMask)];\n                    case 2:\n                        poseLandmarksAndSegmentationInverseProjectionResults = _b.sent();\n                        tf.dispose(outputTensor);\n                        tf.dispose(imageValueShifted);\n                        return [2 /*return*/, __assign({ poseScore: poseScore }, poseLandmarksAndSegmentationInverseProjectionResults)];\n                }\n            });\n        });\n    };\n    BlazePoseTfjsDetector.prototype.poseLandmarksAndSegmentationInverseProjection = function (imageSize, roi, letterboxPadding, transformationMatrix, roiLandmarks, roiAuxiliaryLandmarks, roiWorldLandmarks, roiSegmentationMask) {\n        return __awaiter(this, void 0, void 0, function () {\n            var adjustedLandmarks, adjustedAuxiliaryLandmarks, landmarks, auxiliaryLandmarks, worldLandmarks, segmentationMask;\n            return __generator(this, function (_a) {\n                adjustedLandmarks = remove_landmark_letterbox_1.removeLandmarkLetterbox(roiLandmarks, letterboxPadding);\n                adjustedAuxiliaryLandmarks = remove_landmark_letterbox_1.removeLandmarkLetterbox(roiAuxiliaryLandmarks, letterboxPadding);\n                landmarks = calculate_landmark_projection_1.calculateLandmarkProjection(adjustedLandmarks, roi);\n                auxiliaryLandmarks = calculate_landmark_projection_1.calculateLandmarkProjection(adjustedAuxiliaryLandmarks, roi);\n                worldLandmarks = calculate_world_landmark_projection_1.calculateWorldLandmarkProjection(roiWorldLandmarks, roi);\n                segmentationMask = null;\n                if (this.enableSegmentation) {\n                    segmentationMask = tf.tidy(function () {\n                        var _a = roiSegmentationMask.shape, inputHeight = _a[0], inputWidth = _a[1];\n                        // Calculates the inverse transformation matrix.\n                        // PoseLandmarksAndSegmentationInverseProjection:\n                        // InverseMatrixCalculator.\n                        var inverseTransformationMatrix = calculate_inverse_matrix_1.calculateInverseMatrix(transformationMatrix);\n                        var projectiveTransform = tf.tensor2d(image_utils_1.getProjectiveTransformMatrix(inverseTransformationMatrix, { width: inputWidth, height: inputHeight }, imageSize), [1, 8]);\n                        // Projects the segmentation mask from the letterboxed ROI back to the\n                        // full image.\n                        // PoseLandmarksAndSegmentationInverseProjection: WarpAffineCalculator.\n                        var shape4D = [1, inputHeight, inputWidth, 1];\n                        return tf.squeeze(tf.image.transform(tf.reshape(roiSegmentationMask, shape4D), projectiveTransform, 'bilinear', 'constant', 0, [imageSize.height, imageSize.width]), [0, 3]);\n                    });\n                    tf.dispose(roiSegmentationMask);\n                }\n                return [2 /*return*/, { landmarks: landmarks, auxiliaryLandmarks: auxiliaryLandmarks, worldLandmarks: worldLandmarks, segmentationMask: segmentationMask }];\n            });\n        });\n    };\n    BlazePoseTfjsDetector.prototype.tensorsToPoseLandmarksAndSegmentation = function (tensors) {\n        return __awaiter(this, void 0, void 0, function () {\n            var landmarkTensor, poseFlagTensor, heatmapTensor, worldLandmarkTensor, segmentationTensor, poseScore, rawLandmarks, allLandmarks, landmarks, auxiliaryLandmarks, allWorldLandmarks, worldLandmarksWithoutVisibility, worldLandmarks, segmentationMask;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        landmarkTensor = tensors[0], poseFlagTensor = tensors[1], heatmapTensor = tensors[2], worldLandmarkTensor = tensors[3], segmentationTensor = (this.enableSegmentation ? tensors[4] : null);\n                        return [4 /*yield*/, poseFlagTensor.data()];\n                    case 1:\n                        poseScore = (_a.sent())[0];\n                        // Applies a threshold to the confidence score to determine whether a pose\n                        // is present.\n                        if (poseScore < constants.BLAZEPOSE_POSE_PRESENCE_SCORE) {\n                            return [2 /*return*/, null];\n                        }\n                        return [4 /*yield*/, tensors_to_landmarks_1.tensorsToLandmarks(landmarkTensor, constants.BLAZEPOSE_TENSORS_TO_LANDMARKS_CONFIG)];\n                    case 2:\n                        rawLandmarks = _a.sent();\n                        return [4 /*yield*/, refine_landmarks_from_heatmap_1.refineLandmarksFromHeatmap(rawLandmarks, heatmapTensor, constants.BLAZEPOSE_REFINE_LANDMARKS_FROM_HEATMAP_CONFIG)];\n                    case 3:\n                        allLandmarks = _a.sent();\n                        landmarks = allLandmarks.slice(0, constants.BLAZEPOSE_NUM_KEYPOINTS);\n                        auxiliaryLandmarks = allLandmarks.slice(constants.BLAZEPOSE_NUM_KEYPOINTS, constants.BLAZEPOSE_NUM_AUXILIARY_KEYPOINTS);\n                        return [4 /*yield*/, tensors_to_landmarks_1.tensorsToLandmarks(worldLandmarkTensor, constants.BLAZEPOSE_TENSORS_TO_WORLD_LANDMARKS_CONFIG)];\n                    case 4:\n                        allWorldLandmarks = _a.sent();\n                        worldLandmarksWithoutVisibility = allWorldLandmarks.slice(0, constants.BLAZEPOSE_NUM_KEYPOINTS);\n                        worldLandmarks = calculate_score_copy_1.calculateScoreCopy(landmarks, worldLandmarksWithoutVisibility, true);\n                        segmentationMask = this.enableSegmentation ?\n                            tensors_to_segmentation_1.tensorsToSegmentation(segmentationTensor, constants.BLAZEPOSE_TENSORS_TO_SEGMENTATION_CONFIG) :\n                            null;\n                        return [2 /*return*/, {\n                                landmarks: landmarks,\n                                auxiliaryLandmarks: auxiliaryLandmarks,\n                                poseScore: poseScore,\n                                worldLandmarks: worldLandmarks,\n                                segmentationMask: segmentationMask\n                            }];\n                }\n            });\n        });\n    };\n    // Calculate region of interest (ROI) from landmarks.\n    // Subgraph: PoseLandmarksToRoiCpu\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmarks_to_roi.pbtxt\n    // When landmarks is not null, imageSize should not be null either.\n    BlazePoseTfjsDetector.prototype.poseLandmarksToRoi = function (landmarks, imageSize) {\n        // PoseLandmarksToRoi: LandmarksToDetectionCalculator.\n        var detection = landmarks_to_detection_1.landmarksToDetection(landmarks);\n        // Converts detection into a rectangle based on center and scale alignment\n        // points.\n        // PoseLandmarksToRoi: AlignmentPointsRectsCalculator.\n        var rawRoi = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(detection, imageSize, {\n            rotationVectorStartKeypointIndex: 0,\n            rotationVectorEndKeypointIndex: 1,\n            rotationVectorTargetAngleDegree: 90\n        });\n        // Expands pose rect with marging used during training.\n        // PoseLandmarksToRoi: RectTransformationCalculator.\n        var roi = transform_rect_1.transformNormalizedRect(rawRoi, imageSize, constants.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG);\n        return roi;\n    };\n    // Filter landmarks temporally to reduce jitter.\n    // Subgraph: PoseLandmarkFiltering\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_filtering.pbtxt\n    BlazePoseTfjsDetector.prototype.poseLandmarkFiltering = function (actualLandmarks, auxiliaryLandmarks, actualWorldLandmarks, imageSize) {\n        var actualLandmarksFiltered;\n        var auxiliaryLandmarksFiltered;\n        var actualWorldLandmarksFiltered;\n        if (this.timestamp == null || !this.enableSmoothing) {\n            actualLandmarksFiltered = actualLandmarks;\n            auxiliaryLandmarksFiltered = auxiliaryLandmarks;\n            actualWorldLandmarksFiltered = actualWorldLandmarks;\n        }\n        else {\n            var auxDetection = landmarks_to_detection_1.landmarksToDetection(auxiliaryLandmarks);\n            var objectScaleROI = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(auxDetection, imageSize, {\n                rotationVectorEndKeypointIndex: 0,\n                rotationVectorStartKeypointIndex: 1,\n                rotationVectorTargetAngleDegree: 90\n            });\n            // Smoothes pose landmark visibilities to reduce jitter.\n            if (this.visibilitySmoothingFilterActual == null) {\n                this.visibilitySmoothingFilterActual = new visibility_smoothing_1.LowPassVisibilityFilter(constants.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG);\n            }\n            actualLandmarksFiltered =\n                this.visibilitySmoothingFilterActual.apply(actualLandmarks);\n            if (this.visibilitySmoothingFilterAuxiliary == null) {\n                this.visibilitySmoothingFilterAuxiliary = new visibility_smoothing_1.LowPassVisibilityFilter(constants.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG);\n            }\n            auxiliaryLandmarksFiltered =\n                this.visibilitySmoothingFilterAuxiliary.apply(auxiliaryLandmarks);\n            actualWorldLandmarksFiltered =\n                this.visibilitySmoothingFilterActual.apply(actualWorldLandmarks);\n            // Smoothes pose landmark coordinates to reduce jitter.\n            if (this.landmarksSmoothingFilterActual == null) {\n                this.landmarksSmoothingFilterActual = new keypoints_smoothing_1.KeypointsSmoothingFilter(constants.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_ACTUAL);\n            }\n            actualLandmarksFiltered = this.landmarksSmoothingFilterActual.apply(actualLandmarksFiltered, this.timestamp, imageSize, true /* normalized */, objectScaleROI);\n            if (this.landmarksSmoothingFilterAuxiliary == null) {\n                this.landmarksSmoothingFilterAuxiliary = new keypoints_smoothing_1.KeypointsSmoothingFilter(constants.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_AUXILIARY);\n            }\n            auxiliaryLandmarksFiltered = this.landmarksSmoothingFilterAuxiliary.apply(auxiliaryLandmarksFiltered, this.timestamp, imageSize, true /* normalized */, objectScaleROI);\n            // Smoothes pose world landmark coordinates to reduce jitter.\n            if (this.worldLandmarksSmoothingFilterActual == null) {\n                this.worldLandmarksSmoothingFilterActual = new keypoints_smoothing_1.KeypointsSmoothingFilter(constants.BLAZEPOSE_WORLD_LANDMARKS_SMOOTHING_CONFIG_ACTUAL);\n            }\n            actualWorldLandmarksFiltered =\n                this.worldLandmarksSmoothingFilterActual.apply(actualWorldLandmarks, this.timestamp);\n        }\n        return {\n            actualLandmarksFiltered: actualLandmarksFiltered,\n            auxiliaryLandmarksFiltered: auxiliaryLandmarksFiltered,\n            actualWorldLandmarksFiltered: actualWorldLandmarksFiltered\n        };\n    };\n    return BlazePoseTfjsDetector;\n}());\n/**\n * Loads the BlazePose model.\n *\n * @param modelConfig ModelConfig object that contains parameters for\n * the BlazePose loading process. Please find more details of each parameters\n * in the documentation of the `BlazePoseTfjsModelConfig` interface.\n */\nfunction load(modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, detectorFromTFHub, landmarkFromTFHub, _a, detectorModel, landmarkModel;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    detectorFromTFHub = typeof config.detectorModelUrl === 'string' &&\n                        (config.detectorModelUrl.indexOf('https://tfhub.dev') > -1);\n                    landmarkFromTFHub = typeof config.landmarkModelUrl === 'string' &&\n                        (config.landmarkModelUrl.indexOf('https://tfhub.dev') > -1);\n                    return [4 /*yield*/, Promise.all([\n                            tfconv.loadGraphModel(config.detectorModelUrl, { fromTFHub: detectorFromTFHub }),\n                            tfconv.loadGraphModel(config.landmarkModelUrl, { fromTFHub: landmarkFromTFHub })\n                        ])];\n                case 1:\n                    _a = _b.sent(), detectorModel = _a[0], landmarkModel = _a[1];\n                    return [2 /*return*/, new BlazePoseTfjsDetector(detectorModel, landmarkModel, config.enableSmoothing, config.enableSegmentation, config.smoothSegmentation, config.modelType)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateAlignmentPointsRects = void 0;\nvar detection_to_rect_1 = require(\"./detection_to_rect\");\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/alignment_points_to_rects_calculator.cc\nfunction calculateAlignmentPointsRects(detection, imageSize, config) {\n    var startKeypoint = config.rotationVectorStartKeypointIndex;\n    var endKeypoint = config.rotationVectorEndKeypointIndex;\n    var locationData = detection.locationData;\n    var xCenter = locationData.relativeKeypoints[startKeypoint].x * imageSize.width;\n    var yCenter = locationData.relativeKeypoints[startKeypoint].y * imageSize.height;\n    var xScale = locationData.relativeKeypoints[endKeypoint].x * imageSize.width;\n    var yScale = locationData.relativeKeypoints[endKeypoint].y * imageSize.height;\n    // Bounding box size as double distance from center to scale point.\n    var boxSize = Math.sqrt((xScale - xCenter) * (xScale - xCenter) +\n        (yScale - yCenter) * (yScale - yCenter)) *\n        2;\n    var rotation = detection_to_rect_1.computeRotation(detection, imageSize, config);\n    // Set resulting bounding box.\n    return {\n        xCenter: xCenter / imageSize.width,\n        yCenter: yCenter / imageSize.height,\n        width: boxSize / imageSize.width,\n        height: boxSize / imageSize.height,\n        rotation: rotation\n    };\n}\nexports.calculateAlignmentPointsRects = calculateAlignmentPointsRects;\n//# sourceMappingURL=calculate_alignment_points_rects.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateDetectionsToRects = exports.computeRotation = void 0;\nvar image_utils_1 = require(\"./image_utils\");\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/detections_to_rects_calculator.cc\nfunction computeRotation(detection, imageSize, config) {\n    var locationData = detection.locationData;\n    var startKeypoint = config.rotationVectorStartKeypointIndex;\n    var endKeypoint = config.rotationVectorEndKeypointIndex;\n    var targetAngle;\n    if (config.rotationVectorTargetAngle) {\n        targetAngle = config.rotationVectorTargetAngle;\n    }\n    else {\n        targetAngle = Math.PI * config.rotationVectorTargetAngleDegree / 180;\n    }\n    var x0 = locationData.relativeKeypoints[startKeypoint].x * imageSize.width;\n    var y0 = locationData.relativeKeypoints[startKeypoint].y * imageSize.height;\n    var x1 = locationData.relativeKeypoints[endKeypoint].x * imageSize.width;\n    var y1 = locationData.relativeKeypoints[endKeypoint].y * imageSize.height;\n    var rotation = image_utils_1.normalizeRadians(targetAngle - Math.atan2(-(y1 - y0), x1 - x0));\n    return rotation;\n}\nexports.computeRotation = computeRotation;\nfunction rectFromBox(box) {\n    return {\n        xCenter: box.xMin + box.width / 2,\n        yCenter: box.yMin + box.height / 2,\n        width: box.width,\n        height: box.height,\n    };\n}\nfunction normRectFromKeypoints(locationData) {\n    var keypoints = locationData.relativeKeypoints;\n    if (keypoints.length <= 1) {\n        throw new Error('2 or more keypoints required to calculate a rect.');\n    }\n    var xMin = Number.MAX_VALUE, yMin = Number.MAX_VALUE, xMax = Number.MIN_VALUE, yMax = Number.MIN_VALUE;\n    keypoints.forEach(function (keypoint) {\n        xMin = Math.min(xMin, keypoint.x);\n        xMax = Math.max(xMax, keypoint.x);\n        yMin = Math.min(yMin, keypoint.y);\n        yMax = Math.max(yMax, keypoint.y);\n    });\n    return {\n        xCenter: (xMin + xMax) / 2,\n        yCenter: (yMin + yMax) / 2,\n        width: xMax - xMin,\n        height: yMax - yMin\n    };\n}\nfunction detectionToNormalizedRect(detection, conversionMode) {\n    var locationData = detection.locationData;\n    return conversionMode === 'boundingbox' ?\n        rectFromBox(locationData.relativeBoundingBox) :\n        normRectFromKeypoints(locationData);\n}\nfunction detectionToRect(detection, conversionMode, imageSize) {\n    var locationData = detection.locationData;\n    var rect;\n    if (conversionMode === 'boundingbox') {\n        rect = rectFromBox(locationData.boundingBox);\n    }\n    else {\n        rect = normRectFromKeypoints(locationData);\n        var width = imageSize.width, height = imageSize.height;\n        rect.xCenter = Math.round(rect.xCenter * width);\n        rect.yCenter = Math.round(rect.yCenter * height);\n        rect.width = Math.round(rect.width * width);\n        rect.height = Math.round(rect.height * height);\n    }\n    return rect;\n}\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/detections_to_rects_calculator.cc\nfunction calculateDetectionsToRects(detection, conversionMode, outputType, imageSize, rotationConfig) {\n    var rect = outputType === 'rect' ?\n        detectionToRect(detection, conversionMode, imageSize) :\n        detectionToNormalizedRect(detection, conversionMode);\n    if (rotationConfig) {\n        rect.rotation = computeRotation(detection, imageSize, rotationConfig);\n    }\n    return rect;\n}\nexports.calculateDetectionsToRects = calculateDetectionsToRects;\n//# sourceMappingURL=detection_to_rect.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getProjectiveTransformMatrix = exports.getRoi = exports.padRoi = exports.toImageTensor = exports.transformValueRange = exports.normalizeRadians = exports.getImageSize = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction getImageSize(input) {\n    if (input instanceof tf.Tensor) {\n        return { height: input.shape[0], width: input.shape[1] };\n    }\n    else {\n        return { height: input.height, width: input.width };\n    }\n}\nexports.getImageSize = getImageSize;\n/**\n * Normalizes the provided angle to the range -pi to pi.\n * @param angle The angle in radians to be normalized.\n */\nfunction normalizeRadians(angle) {\n    return angle - 2 * Math.PI * Math.floor((angle + Math.PI) / (2 * Math.PI));\n}\nexports.normalizeRadians = normalizeRadians;\n/**\n * Transform value ranges.\n * @param fromMin Min of original value range.\n * @param fromMax Max of original value range.\n * @param toMin New min of transformed value range.\n * @param toMax New max of transformed value range.\n */\nfunction transformValueRange(fromMin, fromMax, toMin, toMax) {\n    var fromRange = fromMax - fromMin;\n    var toRange = toMax - toMin;\n    if (fromRange === 0) {\n        throw new Error(\"Original min and max are both \" + fromMin + \", range cannot be 0.\");\n    }\n    var scale = toRange / fromRange;\n    var offset = toMin - fromMin * scale;\n    return { scale: scale, offset: offset };\n}\nexports.transformValueRange = transformValueRange;\n/**\n * Convert an image to an image tensor representation.\n *\n * The image tensor has a shape [1, height, width, colorChannel].\n *\n * @param input An image, video frame, or image tensor.\n */\nfunction toImageTensor(input) {\n    return input instanceof tf.Tensor ? input : tf.browser.fromPixels(input);\n}\nexports.toImageTensor = toImageTensor;\n/**\n * Padding ratio of left, top, right, bottom, based on the output dimensions.\n *\n * The padding values are non-zero only when the \"keep_aspect_ratio\" is true.\n *\n * For instance, when the input image is 10x10 (width x height) and the\n * output dimensions is 20x40 and \"keep_aspect_ratio\" is true, we should scale\n * the input image to 20x20 and places it in the middle of the output image with\n * an equal padding of 10 pixels at the top and the bottom. The result is\n * therefore {left: 0, top: 0.25, right: 0, bottom: 0.25} (10/40 = 0.25f).\n * @param roi The original rectangle to pad.\n * @param targetSize The target width and height of the result rectangle.\n * @param keepAspectRatio Whether keep aspect ratio. Default to false.\n */\nfunction padRoi(roi, targetSize, keepAspectRatio) {\n    if (keepAspectRatio === void 0) { keepAspectRatio = false; }\n    if (!keepAspectRatio) {\n        return { top: 0, left: 0, right: 0, bottom: 0 };\n    }\n    var targetH = targetSize.height;\n    var targetW = targetSize.width;\n    validateSize(targetSize, 'targetSize');\n    validateSize(roi, 'roi');\n    var tensorAspectRatio = targetH / targetW;\n    var roiAspectRatio = roi.height / roi.width;\n    var newWidth;\n    var newHeight;\n    var horizontalPadding = 0;\n    var verticalPadding = 0;\n    if (tensorAspectRatio > roiAspectRatio) {\n        // pad height;\n        newWidth = roi.width;\n        newHeight = roi.width * tensorAspectRatio;\n        verticalPadding = (1 - roiAspectRatio / tensorAspectRatio) / 2;\n    }\n    else {\n        // pad width.\n        newWidth = roi.height / tensorAspectRatio;\n        newHeight = roi.height;\n        horizontalPadding = (1 - tensorAspectRatio / roiAspectRatio) / 2;\n    }\n    roi.width = newWidth;\n    roi.height = newHeight;\n    return {\n        top: verticalPadding,\n        left: horizontalPadding,\n        right: horizontalPadding,\n        bottom: verticalPadding\n    };\n}\nexports.padRoi = padRoi;\n/**\n * Get the rectangle information of an image, including xCenter, yCenter, width,\n * height and rotation.\n *\n * @param imageSize imageSize is used to calculate the rectangle.\n * @param normRect Optional. If normRect is not null, it will be used to get\n *     a subarea rectangle information in the image. `imageSize` is used to\n *     calculate the actual non-normalized coordinates.\n */\nfunction getRoi(imageSize, normRect) {\n    if (normRect) {\n        return {\n            xCenter: normRect.xCenter * imageSize.width,\n            yCenter: normRect.yCenter * imageSize.height,\n            width: normRect.width * imageSize.width,\n            height: normRect.height * imageSize.height,\n            rotation: normRect.rotation\n        };\n    }\n    else {\n        return {\n            xCenter: 0.5 * imageSize.width,\n            yCenter: 0.5 * imageSize.height,\n            width: imageSize.width,\n            height: imageSize.height,\n            rotation: 0\n        };\n    }\n}\nexports.getRoi = getRoi;\n/**\n * Generate the projective transformation matrix to be used for `tf.transform`.\n *\n * See more documentation in `tf.transform`.\n *\n * @param matrix The transformation matrix mapping subRect to rect, can be\n *     computed using `getRotatedSubRectToRectTransformMatrix` calculator.\n * @param imageSize The original image height and width.\n * @param inputResolution The target height and width.\n */\nfunction getProjectiveTransformMatrix(matrix, imageSize, inputResolution) {\n    validateSize(inputResolution, 'inputResolution');\n    // To use M with regular x, y coordinates, we need to normalize them first.\n    // Because x' = a0 * x + a1 * y + a2, y' = b0 * x + b1 * y + b2,\n    // we need to use factor (1/inputResolution.width) to normalize x for a0 and\n    // b0, similarly we need to use factor (1/inputResolution.height) to normalize\n    // y for a1 and b1.\n    // Also at the end, we need to de-normalize x' and y' to regular coordinates.\n    // So we need to use factor imageSize.width for a0, a1 and a2, similarly\n    // we need to use factor imageSize.height for b0, b1 and b2.\n    var a0 = (1 / inputResolution.width) * matrix[0][0] * imageSize.width;\n    var a1 = (1 / inputResolution.height) * matrix[0][1] * imageSize.width;\n    var a2 = matrix[0][3] * imageSize.width;\n    var b0 = (1 / inputResolution.width) * matrix[1][0] * imageSize.height;\n    var b1 = (1 / inputResolution.height) * matrix[1][1] * imageSize.height;\n    var b2 = matrix[1][3] * imageSize.height;\n    return [a0, a1, a2, b0, b1, b2, 0, 0];\n}\nexports.getProjectiveTransformMatrix = getProjectiveTransformMatrix;\nfunction validateSize(size, name) {\n    tf.util.assert(size.width !== 0, function () { return name + \" width cannot be 0.\"; });\n    tf.util.assert(size.height !== 0, function () { return name + \" height cannot be 0.\"; });\n}\n//# sourceMappingURL=image_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateInverseMatrix = exports.arrayToMatrix4x4 = exports.matrix4x4ToArray = void 0;\nfunction matrix4x4ToArray(matrix) {\n    return [].concat.apply([], matrix);\n}\nexports.matrix4x4ToArray = matrix4x4ToArray;\nfunction arrayToMatrix4x4(array) {\n    if (array.length !== 16) {\n        throw new Error(\"Array length must be 16 but got \" + array.length);\n    }\n    return [\n        [array[0], array[1], array[2], array[3]],\n        [array[4], array[5], array[6], array[7]],\n        [array[8], array[9], array[10], array[11]],\n        [array[12], array[13], array[14], array[15]],\n    ];\n}\nexports.arrayToMatrix4x4 = arrayToMatrix4x4;\nfunction generalDet3Helper(matrix, i1, i2, i3, j1, j2, j3) {\n    return matrix[i1][j1] *\n        (matrix[i2][j2] * matrix[i3][j3] - matrix[i2][j3] * matrix[i3][j2]);\n}\nfunction cofactor4x4(matrix, i, j) {\n    var i1 = (i + 1) % 4, i2 = (i + 2) % 4, i3 = (i + 3) % 4, j1 = (j + 1) % 4, j2 = (j + 2) % 4, j3 = (j + 3) % 4;\n    return generalDet3Helper(matrix, i1, i2, i3, j1, j2, j3) +\n        generalDet3Helper(matrix, i2, i3, i1, j1, j2, j3) +\n        generalDet3Helper(matrix, i3, i1, i2, j1, j2, j3);\n}\n/**\n * Calculates inverse of an invertible 4x4 matrix.\n * @param matrix 4x4 matrix to invert.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/inverse_matrix_calculator.cc\n// https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/LU/InverseImpl.h\nfunction calculateInverseMatrix(matrix) {\n    var inverse = arrayToMatrix4x4(new Array(16).fill(0));\n    inverse[0][0] = cofactor4x4(matrix, 0, 0);\n    inverse[1][0] = -cofactor4x4(matrix, 0, 1);\n    inverse[2][0] = cofactor4x4(matrix, 0, 2);\n    inverse[3][0] = -cofactor4x4(matrix, 0, 3);\n    inverse[0][2] = cofactor4x4(matrix, 2, 0);\n    inverse[1][2] = -cofactor4x4(matrix, 2, 1);\n    inverse[2][2] = cofactor4x4(matrix, 2, 2);\n    inverse[3][2] = -cofactor4x4(matrix, 2, 3);\n    inverse[0][1] = -cofactor4x4(matrix, 1, 0);\n    inverse[1][1] = cofactor4x4(matrix, 1, 1);\n    inverse[2][1] = -cofactor4x4(matrix, 1, 2);\n    inverse[3][1] = cofactor4x4(matrix, 1, 3);\n    inverse[0][3] = -cofactor4x4(matrix, 3, 0);\n    inverse[1][3] = cofactor4x4(matrix, 3, 1);\n    inverse[2][3] = -cofactor4x4(matrix, 3, 2);\n    inverse[3][3] = cofactor4x4(matrix, 3, 3);\n    var scale = matrix[0][0] * inverse[0][0] + matrix[1][0] * inverse[0][1] +\n        matrix[2][0] * inverse[0][2] + matrix[3][0] * inverse[0][3];\n    for (var i = 0; i < inverse.length; i++) {\n        for (var j = 0; j < inverse.length; j++) {\n            inverse[i][j] /= scale;\n        }\n    }\n    return inverse;\n}\nexports.calculateInverseMatrix = calculateInverseMatrix;\n//# sourceMappingURL=calculate_inverse_matrix.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateLandmarkProjection = void 0;\n/**\n * Projects normalized landmarks in a rectangle to its original coordinates. The\n * rectangle must also be in normalized coordinates.\n * @param landmarks A normalized Landmark list representing landmarks in a\n *     normalized rectangle.\n * @param inputRect A normalized rectangle.\n * @param config Config object has one field ignoreRotation, default to false.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmark_projection_calculator.cc\nfunction calculateLandmarkProjection(landmarks, inputRect, config) {\n    if (config === void 0) { config = {\n        ignoreRotation: false\n    }; }\n    var outputLandmarks = [];\n    for (var _i = 0, landmarks_1 = landmarks; _i < landmarks_1.length; _i++) {\n        var landmark = landmarks_1[_i];\n        var x = landmark.x - 0.5;\n        var y = landmark.y - 0.5;\n        var angle = config.ignoreRotation ? 0 : inputRect.rotation;\n        var newX = Math.cos(angle) * x - Math.sin(angle) * y;\n        var newY = Math.sin(angle) * x + Math.cos(angle) * y;\n        newX = newX * inputRect.width + inputRect.xCenter;\n        newY = newY * inputRect.height + inputRect.yCenter;\n        var newZ = landmark.z * inputRect.width; // Scale Z coordinate as x.\n        var newLandmark = __assign({}, landmark);\n        newLandmark.x = newX;\n        newLandmark.y = newY;\n        newLandmark.z = newZ;\n        outputLandmarks.push(newLandmark);\n    }\n    return outputLandmarks;\n}\nexports.calculateLandmarkProjection = calculateLandmarkProjection;\n//# sourceMappingURL=calculate_landmark_projection.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateScoreCopy = void 0;\n/**\n * A calculator to copy score between landmarks.\n *\n * Landmarks to copy from and to copy to can be of different type (normalized or\n * non-normalized), but landmarks to copy to and output landmarks should be of\n * the same type.\n * @param landmarksFrom  A list of landmarks.\n *     to copy from.\n * @param landmarksTo  A list of landmarks.\n *     to copy to.\n * @param copyScore Copy the score from the `landmarksFrom` parameter.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/visibility_copy_calculator.cc\nfunction calculateScoreCopy(landmarksFrom, landmarksTo, copyScore) {\n    if (copyScore === void 0) { copyScore = true; }\n    var outputLandmarks = [];\n    for (var i = 0; i < landmarksFrom.length; i++) {\n        // Create output landmark and copy all fields from the `to` landmarks\n        var newLandmark = __assign({}, landmarksTo[i]);\n        // Copy score from the `from` landmark.\n        if (copyScore) {\n            newLandmark.score = landmarksFrom[i].score;\n        }\n        outputLandmarks.push(newLandmark);\n    }\n    return outputLandmarks;\n}\nexports.calculateScoreCopy = calculateScoreCopy;\n//# sourceMappingURL=calculate_score_copy.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.calculateWorldLandmarkProjection = void 0;\n/**\n * Projects world landmarks from the rectangle to original coordinates.\n *\n * World landmarks are predicted in meters rather than in pixels of the image\n * and have origin in the middle of the hips rather than in the corner of the\n * pose image (cropped with given rectangle). Thus only rotation (but not scale\n * and translation) is applied to the landmarks to transform them back to\n * original coordinates.\n * @param worldLandmarks A Landmark list representing world landmarks in the\n *     rectangle.\n * @param inputRect A normalized rectangle.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmark_projection_calculator.cc\nfunction calculateWorldLandmarkProjection(worldLandmarks, inputRect) {\n    var outputLandmarks = [];\n    for (var _i = 0, worldLandmarks_1 = worldLandmarks; _i < worldLandmarks_1.length; _i++) {\n        var worldLandmark = worldLandmarks_1[_i];\n        var x = worldLandmark.x;\n        var y = worldLandmark.y;\n        var angle = inputRect.rotation;\n        var newX = Math.cos(angle) * x - Math.sin(angle) * y;\n        var newY = Math.sin(angle) * x + Math.cos(angle) * y;\n        var newLandmark = __assign({}, worldLandmark);\n        newLandmark.x = newX;\n        newLandmark.y = newY;\n        outputLandmarks.push(newLandmark);\n    }\n    return outputLandmarks;\n}\nexports.calculateWorldLandmarkProjection = calculateWorldLandmarkProjection;\n//# sourceMappingURL=calculate_world_landmark_projection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MILLISECOND_TO_MICRO_SECONDS = exports.SECOND_TO_MICRO_SECONDS = exports.MICRO_SECONDS_TO_SECOND = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexports.MICRO_SECONDS_TO_SECOND = 1e-6;\nexports.SECOND_TO_MICRO_SECONDS = 1e6;\nexports.MILLISECOND_TO_MICRO_SECONDS = 1000;\n//# sourceMappingURL=constants.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.convertImageToTensor = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar get_rotated_sub_rect_to_rect_transformation_matrix_1 = require(\"./get_rotated_sub_rect_to_rect_transformation_matrix\");\nvar image_utils_1 = require(\"./image_utils\");\nvar shift_image_value_1 = require(\"./shift_image_value\");\n/**\n * Convert an image or part of it to an image tensor.\n *\n * @param image An image, video frame or image tensor.\n * @param config\n *      inputResolution: The target height and width.\n *      keepAspectRatio?: Whether target tensor should keep aspect ratio.\n * @param normRect A normalized rectangle, representing the subarea to crop from\n *      the image. If normRect is provided, the returned image tensor represents\n *      the subarea.\n * @returns A map with the following properties:\n *     - imageTensor\n *     - padding: Padding ratio of left, top, right, bottom, based on the output\n * dimensions.\n *     - transformationMatrix: Projective transform matrix used to transform\n * input image to transformed image.\n */\nfunction convertImageToTensor(image, config, normRect) {\n    var outputTensorSize = config.outputTensorSize, keepAspectRatio = config.keepAspectRatio, borderMode = config.borderMode, outputTensorFloatRange = config.outputTensorFloatRange;\n    // Ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tensor/image_to_tensor_calculator.cc\n    var imageSize = image_utils_1.getImageSize(image);\n    var roi = image_utils_1.getRoi(imageSize, normRect);\n    var padding = image_utils_1.padRoi(roi, outputTensorSize, keepAspectRatio);\n    var transformationMatrix = get_rotated_sub_rect_to_rect_transformation_matrix_1.getRotatedSubRectToRectTransformMatrix(roi, imageSize.width, imageSize.height, false);\n    var imageTensor = tf.tidy(function () {\n        var $image = image_utils_1.toImageTensor(image);\n        var transformMatrix = tf.tensor2d(image_utils_1.getProjectiveTransformMatrix(transformationMatrix, imageSize, outputTensorSize), [1, 8]);\n        var fillMode = borderMode === 'zero' ? 'constant' : 'nearest';\n        var imageTransformed = tf.image.transform(\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        tf.expandDims(tf.cast($image, 'float32')), transformMatrix, 'bilinear', fillMode, 0, [outputTensorSize.height, outputTensorSize.width]);\n        var imageShifted = outputTensorFloatRange != null ?\n            shift_image_value_1.shiftImageValue(imageTransformed, outputTensorFloatRange) :\n            imageTransformed;\n        return imageShifted;\n    });\n    return { imageTensor: imageTensor, padding: padding, transformationMatrix: transformationMatrix };\n}\nexports.convertImageToTensor = convertImageToTensor;\n//# sourceMappingURL=convert_image_to_tensor.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getRotatedSubRectToRectTransformMatrix = void 0;\nvar calculate_inverse_matrix_1 = require(\"./calculate_inverse_matrix\");\n/**\n * Generates a 4x4 projective transform matrix M, so that for any point in the\n * subRect image p(x, y), we can use the matrix to calculate the projected point\n * in the original image p' (x', y'): p' = p * M;\n *\n * @param subRect Rotated sub rect in absolute coordinates.\n * @param rectWidth\n * @param rectHeight\n * @param flipHorizontaly Whether to flip the image horizontally.\n */\n// Ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tensor/image_to_tensor_utils.h\nfunction getRotatedSubRectToRectTransformMatrix(subRect, rectWidth, rectHeight, flipHorizontally) {\n    // The resulting matrix is multiplication of below commented out matrices:\n    //   postScaleMatrix\n    //     * translateMatrix\n    //     * rotateMatrix\n    //     * flipMatrix\n    //     * scaleMatrix\n    //     * initialTranslateMatrix\n    // For any point in the transformed image p, we can use the above matrix to\n    // calculate the projected point in the original image p'. So that:\n    // p' = p * M;\n    // Note: The transform matrix below assumes image coordinates is normalized\n    // to [0, 1] range.\n    // Matrix to convert X,Y to [-0.5, 0.5] range \"initialTranslateMatrix\"\n    // [ 1.0,  0.0, 0.0, -0.5]\n    // [ 0.0,  1.0, 0.0, -0.5]\n    // [ 0.0,  0.0, 1.0,  0.0]\n    // [ 0.0,  0.0, 0.0,  1.0]\n    var a = subRect.width;\n    var b = subRect.height;\n    // Matrix to scale X,Y,Z to sub rect \"scaleMatrix\"\n    // Z has the same scale as X.\n    // [   a, 0.0, 0.0, 0.0]\n    // [0.0,    b, 0.0, 0.0]\n    // [0.0, 0.0,    a, 0.0]\n    // [0.0, 0.0, 0.0, 1.0]\n    var flip = flipHorizontally ? -1 : 1;\n    // Matrix for optional horizontal flip around middle of output image.\n    // [ fl  , 0.0, 0.0, 0.0]\n    // [ 0.0, 1.0, 0.0, 0.0]\n    // [ 0.0, 0.0, 1.0, 0.0]\n    // [ 0.0, 0.0, 0.0, 1.0]\n    var c = Math.cos(subRect.rotation);\n    var d = Math.sin(subRect.rotation);\n    // Matrix to do rotation around Z axis \"rotateMatrix\"\n    // [    c,   -d, 0.0, 0.0]\n    // [    d,    c, 0.0, 0.0]\n    // [ 0.0, 0.0, 1.0, 0.0]\n    // [ 0.0, 0.0, 0.0, 1.0]\n    var e = subRect.xCenter;\n    var f = subRect.yCenter;\n    // Matrix to do X,Y translation of sub rect within parent rect\n    // \"translateMatrix\"\n    // [1.0, 0.0, 0.0, e   ]\n    // [0.0, 1.0, 0.0, f   ]\n    // [0.0, 0.0, 1.0, 0.0]\n    // [0.0, 0.0, 0.0, 1.0]\n    var g = 1.0 / rectWidth;\n    var h = 1.0 / rectHeight;\n    // Matrix to scale X,Y,Z to [0.0, 1.0] range \"postScaleMatrix\"\n    // [g,    0.0, 0.0, 0.0]\n    // [0.0, h,    0.0, 0.0]\n    // [0.0, 0.0,    g, 0.0]\n    // [0.0, 0.0, 0.0, 1.0]\n    var matrix = new Array(16);\n    // row 1\n    matrix[0] = a * c * flip * g;\n    matrix[1] = -b * d * g;\n    matrix[2] = 0.0;\n    matrix[3] = (-0.5 * a * c * flip + 0.5 * b * d + e) * g;\n    // row 2\n    matrix[4] = a * d * flip * h;\n    matrix[5] = b * c * h;\n    matrix[6] = 0.0;\n    matrix[7] = (-0.5 * b * c - 0.5 * a * d * flip + f) * h;\n    // row 3\n    matrix[8] = 0.0;\n    matrix[9] = 0.0;\n    matrix[10] = a * g;\n    matrix[11] = 0.0;\n    // row 4\n    matrix[12] = 0.0;\n    matrix[13] = 0.0;\n    matrix[14] = 0.0;\n    matrix[15] = 1.0;\n    return calculate_inverse_matrix_1.arrayToMatrix4x4(matrix);\n}\nexports.getRotatedSubRectToRectTransformMatrix = getRotatedSubRectToRectTransformMatrix;\n//# sourceMappingURL=get_rotated_sub_rect_to_rect_transformation_matrix.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.shiftImageValue = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar image_utils_1 = require(\"./image_utils\");\nfunction shiftImageValue(image, outputFloatRange) {\n    // Calculate the scale and offset to shift from [0, 255] to [-1, 1].\n    var valueRange = image_utils_1.transformValueRange(0, 255, outputFloatRange[0] /* min */, outputFloatRange[1] /* max */);\n    // Shift value range.\n    return tf.tidy(function () { return tf.add(tf.mul(image, valueRange.scale), valueRange.offset); });\n}\nexports.shiftImageValue = shiftImageValue;\n//# sourceMappingURL=shift_image_value.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.createSsdAnchors = void 0;\n// ref:\n// https://github.com/google/mediapipe/blob/350fbb2100ad531bc110b93aaea23d96af5a5064/mediapipe/calculators/tflite/ssd_anchors_calculator.cc\nfunction createSsdAnchors(config) {\n    // Set defaults.\n    if (config.reduceBoxesInLowestLayer == null) {\n        config.reduceBoxesInLowestLayer = false;\n    }\n    if (config.interpolatedScaleAspectRatio == null) {\n        config.interpolatedScaleAspectRatio = 1.0;\n    }\n    if (config.fixedAnchorSize == null) {\n        config.fixedAnchorSize = false;\n    }\n    var anchors = [];\n    var layerId = 0;\n    while (layerId < config.numLayers) {\n        var anchorHeight = [];\n        var anchorWidth = [];\n        var aspectRatios = [];\n        var scales = [];\n        // For same strides, we merge the anchors in the same order.\n        var lastSameStrideLayer = layerId;\n        while (lastSameStrideLayer < config.strides.length &&\n            config.strides[lastSameStrideLayer] === config.strides[layerId]) {\n            var scale = calculateScale(config.minScale, config.maxScale, lastSameStrideLayer, config.strides.length);\n            if (lastSameStrideLayer === 0 && config.reduceBoxesInLowestLayer) {\n                // For first layer, it can be specified to use predefined anchors.\n                aspectRatios.push(1);\n                aspectRatios.push(2);\n                aspectRatios.push(0.5);\n                scales.push(0.1);\n                scales.push(scale);\n                scales.push(scale);\n            }\n            else {\n                for (var aspectRatioId = 0; aspectRatioId < config.aspectRatios.length; ++aspectRatioId) {\n                    aspectRatios.push(config.aspectRatios[aspectRatioId]);\n                    scales.push(scale);\n                }\n                if (config.interpolatedScaleAspectRatio > 0.0) {\n                    var scaleNext = lastSameStrideLayer === config.strides.length - 1 ?\n                        1.0 :\n                        calculateScale(config.minScale, config.maxScale, lastSameStrideLayer + 1, config.strides.length);\n                    scales.push(Math.sqrt(scale * scaleNext));\n                    aspectRatios.push(config.interpolatedScaleAspectRatio);\n                }\n            }\n            lastSameStrideLayer++;\n        }\n        for (var i = 0; i < aspectRatios.length; ++i) {\n            var ratioSqrts = Math.sqrt(aspectRatios[i]);\n            anchorHeight.push(scales[i] / ratioSqrts);\n            anchorWidth.push(scales[i] * ratioSqrts);\n        }\n        var featureMapHeight = 0;\n        var featureMapWidth = 0;\n        if (config.featureMapHeight.length > 0) {\n            featureMapHeight = config.featureMapHeight[layerId];\n            featureMapWidth = config.featureMapWidth[layerId];\n        }\n        else {\n            var stride = config.strides[layerId];\n            featureMapHeight = Math.ceil(config.inputSizeHeight / stride);\n            featureMapWidth = Math.ceil(config.inputSizeWidth / stride);\n        }\n        for (var y = 0; y < featureMapHeight; ++y) {\n            for (var x = 0; x < featureMapWidth; ++x) {\n                for (var anchorId = 0; anchorId < anchorHeight.length; ++anchorId) {\n                    var xCenter = (x + config.anchorOffsetX) / featureMapWidth;\n                    var yCenter = (y + config.anchorOffsetY) / featureMapHeight;\n                    var newAnchor = { xCenter: xCenter, yCenter: yCenter, width: 0, height: 0 };\n                    if (config.fixedAnchorSize) {\n                        newAnchor.width = 1.0;\n                        newAnchor.height = 1.0;\n                    }\n                    else {\n                        newAnchor.width = anchorWidth[anchorId];\n                        newAnchor.height = anchorHeight[anchorId];\n                    }\n                    anchors.push(newAnchor);\n                }\n            }\n        }\n        layerId = lastSameStrideLayer;\n    }\n    return anchors;\n}\nexports.createSsdAnchors = createSsdAnchors;\nfunction calculateScale(minScale, maxScale, strideIndex, numStrides) {\n    if (numStrides === 1) {\n        return (minScale + maxScale) * 0.5;\n    }\n    else {\n        return minScale + (maxScale - minScale) * strideIndex / (numStrides - 1);\n    }\n}\n//# sourceMappingURL=create_ssd_anchors.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.detectorResult = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar split_detection_result_1 = require(\"./split_detection_result\");\nfunction detectorResult(detectionResult) {\n    return tf.tidy(function () {\n        var _a = split_detection_result_1.splitDetectionResult(detectionResult), logits = _a[0], rawBoxes = _a[1];\n        // Shape [896, 12]\n        var rawBoxes2d = tf.squeeze(rawBoxes);\n        // Shape [896]\n        var logits1d = tf.squeeze(logits);\n        return { boxes: rawBoxes2d, logits: logits1d };\n    });\n}\nexports.detectorResult = detectorResult;\n//# sourceMappingURL=detector_result.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.splitDetectionResult = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction splitDetectionResult(detectionResult) {\n    return tf.tidy(function () {\n        // logit is stored in the first element in each anchor data.\n        var logits = tf.slice(detectionResult, [0, 0, 0], [1, -1, 1]);\n        // Bounding box coords are stored in the next four elements for each anchor\n        // point.\n        var rawBoxes = tf.slice(detectionResult, [0, 0, 1], [1, -1, -1]);\n        return [logits, rawBoxes];\n    });\n}\nexports.splitDetectionResult = splitDetectionResult;\n//# sourceMappingURL=split_detection_result.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.isVideo = void 0;\nfunction isVideo(image) {\n    return (image != null) && image.currentTime != null;\n}\nexports.isVideo = isVideo;\n//# sourceMappingURL=is_video.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.landmarksToDetection = void 0;\n/**\n * Converts normalized Landmark to `Detection`. A relative bounding box will\n * be created containing all landmarks exactly.\n * @param landmarks List of normalized landmarks.\n *\n * @returns A `Detection`.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_to_detection_calculator.cc\nfunction landmarksToDetection(landmarks) {\n    var detection = { locationData: { relativeKeypoints: [] } };\n    var xMin = Number.MAX_SAFE_INTEGER;\n    var xMax = Number.MIN_SAFE_INTEGER;\n    var yMin = Number.MAX_SAFE_INTEGER;\n    var yMax = Number.MIN_SAFE_INTEGER;\n    for (var i = 0; i < landmarks.length; ++i) {\n        var landmark = landmarks[i];\n        xMin = Math.min(xMin, landmark.x);\n        xMax = Math.max(xMax, landmark.x);\n        yMin = Math.min(yMin, landmark.y);\n        yMax = Math.max(yMax, landmark.y);\n        detection.locationData.relativeKeypoints.push({ x: landmark.x, y: landmark.y });\n    }\n    detection.locationData.relativeBoundingBox =\n        { xMin: xMin, yMin: yMin, xMax: xMax, yMax: yMax, width: (xMax - xMin), height: (yMax - yMin) };\n    return detection;\n}\nexports.landmarksToDetection = landmarksToDetection;\n//# sourceMappingURL=landmarks_to_detection.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.nonMaxSuppression = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction nonMaxSuppression(detections, maxDetections, iouThreshold, \n// Currently only IOU overap is supported.\noverlapType) {\n    return __awaiter(this, void 0, void 0, function () {\n        var detectionsTensor, scoresTensor, selectedIdsTensor, selectedIds, selectedDetections;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    // Sort to match NonMaxSuppresion calculator's decreasing detection score\n                    // traversal.\n                    // NonMaxSuppresionCalculator: RetainMaxScoringLabelOnly\n                    detections.sort(function (detectionA, detectionB) {\n                        return Math.max.apply(Math, detectionB.score) - Math.max.apply(Math, detectionA.score);\n                    });\n                    detectionsTensor = tf.tensor2d(detections.map(function (d) {\n                        return [d.locationData.relativeBoundingBox.yMin,\n                            d.locationData.relativeBoundingBox.xMin,\n                            d.locationData.relativeBoundingBox.yMax,\n                            d.locationData.relativeBoundingBox.xMax];\n                    }));\n                    scoresTensor = tf.tensor1d(detections.map(function (d) { return d.score[0]; }));\n                    return [4 /*yield*/, tf.image.nonMaxSuppressionAsync(detectionsTensor, scoresTensor, maxDetections, iouThreshold)];\n                case 1:\n                    selectedIdsTensor = _a.sent();\n                    return [4 /*yield*/, selectedIdsTensor.array()];\n                case 2:\n                    selectedIds = _a.sent();\n                    selectedDetections = detections.filter(function (_, i) { return (selectedIds.indexOf(i) > -1); });\n                    tf.dispose([detectionsTensor, scoresTensor, selectedIdsTensor]);\n                    return [2 /*return*/, selectedDetections];\n            }\n        });\n    });\n}\nexports.nonMaxSuppression = nonMaxSuppression;\n//# sourceMappingURL=non_max_suppression.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.normalizedKeypointsToKeypoints = void 0;\nfunction normalizedKeypointsToKeypoints(normalizedKeypoints, imageSize) {\n    return normalizedKeypoints.map(function (normalizedKeypoint) {\n        var keypoint = __assign(__assign({}, normalizedKeypoint), { x: normalizedKeypoint.x * imageSize.width, y: normalizedKeypoint.y * imageSize.height });\n        if (normalizedKeypoint.z != null) {\n            // Scale z the same way as x (using image width).\n            keypoint.z = normalizedKeypoint.z * imageSize.width;\n        }\n        return keypoint;\n    });\n}\nexports.normalizedKeypointsToKeypoints = normalizedKeypointsToKeypoints;\n//# sourceMappingURL=normalized_keypoints_to_keypoints.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.refineLandmarksFromHeatmap = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * A calculator that refines landmarks using corresponding heatmap area.\n *\n * High level algorithm\n * For each landmark, we replace original value with a value calculated from the\n * area in heatmap close to original landmark position (the area is defined by\n * config.kernelSize). To calculate new coordinate from heatmap we calculate an\n * weighted average inside the kernel. We update the landmark if heatmap is\n * confident in it's prediction i.e. max(heatmap) in kernel is at least bigger\n * than config.minConfidenceToRefine.\n * @param landmarks List of lardmarks to refine.\n * @param heatmapTensor The heatmap for the landmarks with shape\n *     [height, width, channel]. The channel dimension has to be the same as\n *     the number of landmarks.\n * @param config The config for refineLandmarksFromHeap,\n *     see `RefineLandmarksFromHeatmapConfig` for detail.\n *\n * @returns Normalized landmarks.\n */\nfunction refineLandmarksFromHeatmap(landmarks, heatmapTensor, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var $heatmapTensor, _a, hmHeight, hmWidth, hmChannels, outLandmarks, heatmapBuf, i, landmark, outLandmark, centerCol, centerRow, offset, beginCol, endCol, beginRow, endRow, sum, weightedCol, weightedRow, maxValue, row, col, confidence;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    $heatmapTensor = tf.squeeze(heatmapTensor, [0]);\n                    _a = $heatmapTensor.shape, hmHeight = _a[0], hmWidth = _a[1], hmChannels = _a[2];\n                    if (landmarks.length !== hmChannels) {\n                        throw new Error('Expected heatmap to have same number of channels ' +\n                            'as the number of landmarks. But got landmarks length: ' +\n                            (landmarks.length + \", heatmap length: \" + hmChannels));\n                    }\n                    outLandmarks = [];\n                    return [4 /*yield*/, $heatmapTensor.buffer()];\n                case 1:\n                    heatmapBuf = _b.sent();\n                    for (i = 0; i < landmarks.length; i++) {\n                        landmark = landmarks[i];\n                        outLandmark = __assign({}, landmark);\n                        outLandmarks.push(outLandmark);\n                        centerCol = Math.trunc(outLandmark.x * hmWidth);\n                        centerRow = Math.trunc(outLandmark.y * hmHeight);\n                        // Point is outside of the image let's keep it intact.\n                        if (centerCol < 0 || centerCol >= hmWidth || centerRow < 0 ||\n                            centerCol >= hmHeight) {\n                            continue;\n                        }\n                        offset = Math.trunc((config.kernelSize - 1) / 2);\n                        beginCol = Math.max(0, centerCol - offset);\n                        endCol = Math.min(hmWidth, centerCol + offset + 1);\n                        beginRow = Math.max(0, centerRow - offset);\n                        endRow = Math.min(hmHeight, centerRow + offset + 1);\n                        sum = 0;\n                        weightedCol = 0;\n                        weightedRow = 0;\n                        maxValue = 0;\n                        // Main loop. Go over kernel and calculate weighted sum of coordinates,\n                        // sum of weights and max weights.\n                        for (row = beginRow; row < endRow; ++row) {\n                            for (col = beginCol; col < endCol; ++col) {\n                                confidence = heatmapBuf.get(row, col, i);\n                                sum += confidence;\n                                maxValue = Math.max(maxValue, confidence);\n                                weightedCol += col * confidence;\n                                weightedRow += row * confidence;\n                            }\n                        }\n                        if (maxValue >= config.minConfidenceToRefine && sum > 0) {\n                            outLandmark.x = weightedCol / hmWidth / sum;\n                            outLandmark.y = weightedRow / hmHeight / sum;\n                        }\n                    }\n                    $heatmapTensor.dispose();\n                    return [2 /*return*/, outLandmarks];\n            }\n        });\n    });\n}\nexports.refineLandmarksFromHeatmap = refineLandmarksFromHeatmap;\n//# sourceMappingURL=refine_landmarks_from_heatmap.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.removeDetectionLetterbox = void 0;\n/**\n * Adjusts detection locations on the letterboxed image to the corresponding\n * locations on the same image with the letterbox removed (the input image to\n * the graph before image transformation).\n *\n * @param detections A list of detection boxes on an letterboxed image.\n * @param letterboxPadding A `padding` object representing the letterbox padding\n *     from the 4 sides: left, top, right, bottom, of the letterboxed image,\n *     normalized by the letterboxed image dimensions.\n * @returns detections: A list of detection boxes representing detections with\n *     their locations adjusted to the letterbox-removed (non-padded) image.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/detection_letterbox_removal_calculator.cc\nfunction removeDetectionLetterbox(detections, letterboxPadding) {\n    if (detections === void 0) { detections = []; }\n    var left = letterboxPadding.left;\n    var top = letterboxPadding.top;\n    var leftAndRight = letterboxPadding.left + letterboxPadding.right;\n    var topAndBottom = letterboxPadding.top + letterboxPadding.bottom;\n    for (var i = 0; i < detections.length; i++) {\n        var detection = detections[i];\n        var relativeBoundingBox = detection.locationData.relativeBoundingBox;\n        var xMin = (relativeBoundingBox.xMin - left) / (1 - leftAndRight);\n        var yMin = (relativeBoundingBox.yMin - top) / (1 - topAndBottom);\n        var width = relativeBoundingBox.width / (1 - leftAndRight);\n        var height = relativeBoundingBox.height / (1 - topAndBottom);\n        relativeBoundingBox.xMin = xMin;\n        relativeBoundingBox.yMin = yMin;\n        relativeBoundingBox.width = width;\n        relativeBoundingBox.height = height;\n        relativeBoundingBox.xMax = xMin + width;\n        relativeBoundingBox.yMax = yMin + height;\n        var relativeKeypoints = detection.locationData.relativeKeypoints;\n        if (relativeKeypoints) {\n            relativeKeypoints.forEach(function (keypoint) {\n                var newX = (keypoint.x - left) / (1 - leftAndRight);\n                var newY = (keypoint.y - top) / (1 - topAndBottom);\n                keypoint.x = newX;\n                keypoint.y = newY;\n            });\n        }\n    }\n    return detections;\n}\nexports.removeDetectionLetterbox = removeDetectionLetterbox;\n//# sourceMappingURL=remove_detection_letterbox.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.removeLandmarkLetterbox = void 0;\n/**\n * Adjusts landmark locations on a letterboxed image to the corresponding\n * locations on the same image with the letterbox removed.\n * @param rawLandmark A NormalizedLandmarkList representing landmarks on an\n * letterboxed image.\n * @param padding A `padding` representing the letterbox padding from the 4\n *     sides, left, top, right, bottom, of the letterboxed image, normalized by\n *     the letterboxed image dimensions.\n * @returns Normalized landmarks.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmark_letterbox_removal_calculator.cc\nfunction removeLandmarkLetterbox(rawLandmark, padding) {\n    var left = padding.left;\n    var top = padding.top;\n    var leftAndRight = padding.left + padding.right;\n    var topAndBottom = padding.top + padding.bottom;\n    var outLandmarks = rawLandmark.map(function (landmark) {\n        return __assign(__assign({}, landmark), { x: (landmark.x - left) / (1 - leftAndRight), y: (landmark.y - top) / (1 - topAndBottom), z: landmark.z / (1 - leftAndRight) // Scale Z coordinate as X.\n         });\n    });\n    return outLandmarks;\n}\nexports.removeLandmarkLetterbox = removeLandmarkLetterbox;\n//# sourceMappingURL=remove_landmark_letterbox.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.smoothSegmentation = void 0;\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * A calculator for mixing two segmentation masks together, based on an\n * uncertantity probability estimate.\n * @param prevMaks Segmentation mask from previous image.\n * @param newMask Segmentation mask of current image.\n * @param config Contains ratio of amount of previous mask to blend with\n *     current.\n *\n * @returns Image mask.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/image/segmentation_smoothing_calculator.cc\nfunction smoothSegmentation(prevMask, newMask, config) {\n    if (tf.getBackend() === 'webgl') {\n        // Same as implementation in the else case but reduces number of shader\n        // calls to 1 instead of 17.\n        return smoothSegmentationWebGL(prevMask, newMask, config);\n    }\n    return tf.tidy(function () {\n        /*\n         * Assume p := newMaskValue\n         * H(p) := 1 + (p * log(p) + (1-p) * log(1-p)) / log(2)\n         * uncertainty alpha(p) =\n         *   Clamp(1 - (1 - H(p)) * (1 - H(p)), 0, 1) [squaring the\n         * uncertainty]\n         *\n         * The following polynomial approximates uncertainty alpha as a\n         * function of (p + 0.5):\n         */\n        var c1 = 5.68842;\n        var c2 = -0.748699;\n        var c3 = -57.8051;\n        var c4 = 291.309;\n        var c5 = -624.717;\n        var t = tf.sub(newMask, 0.5);\n        var x = tf.square(t);\n        // Per element calculation is: 1.0 - Math.min(1.0, x * (c1 + x * (c2 + x\n        // * (c3 + x * (c4 + x * c5))))).\n        var uncertainty = tf.sub(1, tf.minimum(1, tf.mul(x, tf.add(c1, tf.mul(x, tf.add(c2, tf.mul(x, tf.add(c3, tf.mul(x, tf.add(c4, tf.mul(x, c5)))))))))));\n        // Per element calculation is: newMaskValue + (prevMaskValue -\n        // newMaskValue) * (uncertainty * combineWithPreviousRatio).\n        return tf.add(newMask, tf.mul(tf.sub(prevMask, newMask), tf.mul(uncertainty, config.combineWithPreviousRatio)));\n    });\n}\nexports.smoothSegmentation = smoothSegmentation;\nfunction smoothSegmentationWebGL(prevMask, newMask, config) {\n    var ratio = config.combineWithPreviousRatio.toFixed(2);\n    var program = {\n        variableNames: ['prevMask', 'newMask'],\n        outputShape: prevMask.shape,\n        userCode: \"\\n  void main() {\\n      ivec2 coords = getOutputCoords();\\n      int height = coords[0];\\n      int width = coords[1];\\n\\n      float prevMaskValue = getPrevMask(height, width);\\n      float newMaskValue = getNewMask(height, width);\\n\\n      /*\\n      * Assume p := newMaskValue\\n      * H(p) := 1 + (p * log(p) + (1-p) * log(1-p)) / log(2)\\n      * uncertainty alpha(p) =\\n      *   Clamp(1 - (1 - H(p)) * (1 - H(p)), 0, 1) [squaring the\\n      * uncertainty]\\n      *\\n      * The following polynomial approximates uncertainty alpha as a\\n      * function of (p + 0.5):\\n      */\\n      const float c1 = 5.68842;\\n      const float c2 = -0.748699;\\n      const float c3 = -57.8051;\\n      const float c4 = 291.309;\\n      const float c5 = -624.717;\\n      float t = newMaskValue - 0.5;\\n      float x = t * t;\\n\\n      float uncertainty =\\n        1.0 - min(1.0, x * (c1 + x * (c2 + x * (c3 + x * (c4 + x * c5)))));\\n\\n      float outputValue = newMaskValue + (prevMaskValue - newMaskValue) *\\n                             (uncertainty * \" + ratio + \");\\n\\n      setOutput(outputValue);\\n    }\\n\"\n    };\n    var webglBackend = tf.backend();\n    return tf.tidy(function () {\n        var outputTensorInfo = webglBackend.compileAndRun(program, [prevMask, newMask]);\n        return tf.engine().makeTensorFromDataId(outputTensorInfo.dataId, outputTensorInfo.shape, outputTensorInfo.dtype);\n    });\n}\n//# sourceMappingURL=segmentation_smoothing.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.convertToDetections = exports.tensorsToDetections = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * Convert result Tensors from object detection models into Detection boxes.\n *\n * @param detectionTensors List of Tensors of type Float32. The list of tensors\n *     can have 2 or 3 tensors. First tensor is the predicted raw\n *     boxes/keypoints. The size of the values must be\n *     (num_boxes * num_predicted_values). Second tensor is the score tensor.\n *     The size of the valuse must be (num_boxes * num_classes). It's optional\n *     to pass in a third tensor for anchors (e.g. for SSD models) depend on the\n *     outputs of the detection model. The size of anchor tensor must be\n *     (num_boxes * 4).\n * @param anchor A tensor for anchors. The size of anchor tensor must be\n *     (num_boxes * 4).\n * @param config\n */\nfunction tensorsToDetections(detectionTensors, anchor, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var rawScoreTensor, rawBoxTensor, boxes, normalizedScore, outputDetections;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    rawScoreTensor = detectionTensors[0];\n                    rawBoxTensor = detectionTensors[1];\n                    boxes = decodeBoxes(rawBoxTensor, anchor, config);\n                    normalizedScore = tf.tidy(function () {\n                        var normalizedScore = rawScoreTensor;\n                        if (config.sigmoidScore) {\n                            if (config.scoreClippingThresh != null) {\n                                normalizedScore = tf.clipByValue(rawScoreTensor, -config.scoreClippingThresh, config.scoreClippingThresh);\n                            }\n                            normalizedScore = tf.sigmoid(normalizedScore);\n                            return normalizedScore;\n                        }\n                        return normalizedScore;\n                    });\n                    return [4 /*yield*/, convertToDetections(boxes, normalizedScore, config)];\n                case 1:\n                    outputDetections = _a.sent();\n                    tf.dispose([boxes, normalizedScore]);\n                    return [2 /*return*/, outputDetections];\n            }\n        });\n    });\n}\nexports.tensorsToDetections = tensorsToDetections;\nfunction convertToDetections(detectionBoxes, detectionScore, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var outputDetections, detectionBoxesData, detectionScoresData, i, boxOffset, detection, bbox, locationData, totalIdx, kpId, keypointIndex, keypoint;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    outputDetections = [];\n                    return [4 /*yield*/, detectionBoxes.data()];\n                case 1:\n                    detectionBoxesData = _a.sent();\n                    return [4 /*yield*/, detectionScore.data()];\n                case 2:\n                    detectionScoresData = _a.sent();\n                    for (i = 0; i < config.numBoxes; ++i) {\n                        if (config.minScoreThresh != null &&\n                            detectionScoresData[i] < config.minScoreThresh) {\n                            continue;\n                        }\n                        boxOffset = i * config.numCoords;\n                        detection = convertToDetection(detectionBoxesData[boxOffset + 0] /* boxYMin */, detectionBoxesData[boxOffset + 1] /* boxXMin */, detectionBoxesData[boxOffset + 2] /* boxYMax */, detectionBoxesData[boxOffset + 3] /* boxXMax */, detectionScoresData[i], config.flipVertically, i);\n                        bbox = detection.locationData.relativeBoundingBox;\n                        if (bbox.width < 0 || bbox.height < 0) {\n                            // Decoded detection boxes could have negative values for width/height\n                            // due to model prediction. Filter out those boxes since some\n                            // downstream calculators may assume non-negative values.\n                            continue;\n                        }\n                        // Add keypoints.\n                        if (config.numKeypoints > 0) {\n                            locationData = detection.locationData;\n                            locationData.relativeKeypoints = [];\n                            totalIdx = config.numKeypoints * config.numValuesPerKeypoint;\n                            for (kpId = 0; kpId < totalIdx; kpId += config.numValuesPerKeypoint) {\n                                keypointIndex = boxOffset + config.keypointCoordOffset + kpId;\n                                keypoint = {\n                                    x: detectionBoxesData[keypointIndex + 0],\n                                    y: config.flipVertically ? 1 - detectionBoxesData[keypointIndex + 1] :\n                                        detectionBoxesData[keypointIndex + 1]\n                                };\n                                locationData.relativeKeypoints.push(keypoint);\n                            }\n                        }\n                        outputDetections.push(detection);\n                    }\n                    return [2 /*return*/, outputDetections];\n            }\n        });\n    });\n}\nexports.convertToDetections = convertToDetections;\nfunction convertToDetection(boxYMin, boxXMin, boxYMax, boxXMax, score, flipVertically, i) {\n    return {\n        score: [score],\n        ind: i,\n        locationData: {\n            relativeBoundingBox: {\n                xMin: boxXMin,\n                yMin: flipVertically ? 1 - boxYMax : boxYMin,\n                xMax: boxXMax,\n                yMax: flipVertically ? 1 - boxYMin : boxYMax,\n                width: boxXMax - boxXMin,\n                height: boxYMax - boxYMin\n            }\n        }\n    };\n}\n//[xCenter, yCenter, w, h, kp1, kp2, kp3, kp4]\n//[yMin, xMin, yMax, xMax, kpX, kpY, kpX, kpY]\nfunction decodeBoxes(rawBoxes, anchor, config) {\n    return tf.tidy(function () {\n        var yCenter;\n        var xCenter;\n        var h;\n        var w;\n        if (config.reverseOutputOrder) {\n            // Shape [numOfBoxes, 1].\n            xCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 0], [-1, 1]));\n            yCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 1], [-1, 1]));\n            w = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 2], [-1, 1]));\n            h = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 3], [-1, 1]));\n        }\n        else {\n            yCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 0], [-1, 1]));\n            xCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 1], [-1, 1]));\n            h = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 2], [-1, 1]));\n            w = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 3], [-1, 1]));\n        }\n        xCenter =\n            tf.add(tf.mul(tf.div(xCenter, config.xScale), anchor.w), anchor.x);\n        yCenter =\n            tf.add(tf.mul(tf.div(yCenter, config.yScale), anchor.h), anchor.y);\n        if (config.applyExponentialOnBoxSize) {\n            h = tf.mul(tf.exp(tf.div(h, config.hScale)), anchor.h);\n            w = tf.mul(tf.exp(tf.div(w, config.wScale)), anchor.w);\n        }\n        else {\n            h = tf.mul(tf.div(h, config.hScale), anchor.h);\n            w = tf.mul(tf.div(w, config.wScale), anchor.h);\n        }\n        var yMin = tf.sub(yCenter, tf.div(h, 2));\n        var xMin = tf.sub(xCenter, tf.div(w, 2));\n        var yMax = tf.add(yCenter, tf.div(h, 2));\n        var xMax = tf.add(xCenter, tf.div(w, 2));\n        // Shape [numOfBoxes, 4].\n        var boxes = tf.concat([\n            tf.reshape(yMin, [config.numBoxes, 1]),\n            tf.reshape(xMin, [config.numBoxes, 1]),\n            tf.reshape(yMax, [config.numBoxes, 1]),\n            tf.reshape(xMax, [config.numBoxes, 1])\n        ], 1);\n        if (config.numKeypoints) {\n            for (var k = 0; k < config.numKeypoints; ++k) {\n                var keypointOffset = config.keypointCoordOffset + k * config.numValuesPerKeypoint;\n                var keypointX = void 0;\n                var keypointY = void 0;\n                if (config.reverseOutputOrder) {\n                    keypointX =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset], [-1, 1]));\n                    keypointY =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset + 1], [-1, 1]));\n                }\n                else {\n                    keypointY =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset], [-1, 1]));\n                    keypointX =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset + 1], [-1, 1]));\n                }\n                var keypointXNormalized = tf.add(tf.mul(tf.div(keypointX, config.xScale), anchor.w), anchor.x);\n                var keypointYNormalized = tf.add(tf.mul(tf.div(keypointY, config.yScale), anchor.h), anchor.y);\n                boxes = tf.concat([\n                    boxes, tf.reshape(keypointXNormalized, [config.numBoxes, 1]),\n                    tf.reshape(keypointYNormalized, [config.numBoxes, 1])\n                ], 1);\n            }\n        }\n        // Shape [numOfBoxes, 4] || [numOfBoxes, 12].\n        return boxes;\n    });\n}\n//# sourceMappingURL=tensors_to_detections.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.tensorsToLandmarks = void 0;\nvar sigmoid_1 = require(\"./sigmoid\");\nfunction applyActivation(activation, value) {\n    return activation === 'none' ? value : sigmoid_1.sigmoid(value);\n}\n/**\n * A calculator for converting Tensors from regression models into landmarks.\n * Note that if the landmarks in the tensor has more than 5 dimensions, only the\n * first 5 dimensions will be converted to [x,y,z, visibility, presence]. The\n * latter two fields may also stay unset if such attributes are not supported in\n * the model.\n * @param landmarkTensor List of Tensors of type float32. Only the first tensor\n * will be used. The size of the values must be (num_dimension x num_landmarks).\n * @param flipHorizontally Optional. Whether to flip landmarks horizontally or\n * not. Overrides corresponding field in config.\n * @param flipVertically Optional. Whether to flip landmarks vertically or not.\n * Overrides corresponding field in config.\n *\n * @param config\n *\n * @returns Normalized landmarks.\n */\nfunction tensorsToLandmarks(landmarkTensor, config, flipHorizontally, flipVertically) {\n    return __awaiter(this, void 0, void 0, function () {\n        var numValues, numDimensions, rawLandmarks, outputLandmarks, ld, offset, landmark, i, landmark;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    flipHorizontally = flipHorizontally || config.flipHorizontally || false;\n                    flipVertically = flipVertically || config.flipVertically || false;\n                    numValues = landmarkTensor.size;\n                    numDimensions = numValues / config.numLandmarks;\n                    return [4 /*yield*/, landmarkTensor.data()];\n                case 1:\n                    rawLandmarks = _a.sent();\n                    outputLandmarks = [];\n                    for (ld = 0; ld < config.numLandmarks; ++ld) {\n                        offset = ld * numDimensions;\n                        landmark = { x: 0, y: 0 };\n                        if (flipHorizontally) {\n                            landmark.x = config.inputImageWidth - rawLandmarks[offset];\n                        }\n                        else {\n                            landmark.x = rawLandmarks[offset];\n                        }\n                        if (numDimensions > 1) {\n                            if (flipVertically) {\n                                landmark.y = config.inputImageHeight - rawLandmarks[offset + 1];\n                            }\n                            else {\n                                landmark.y = rawLandmarks[offset + 1];\n                            }\n                        }\n                        if (numDimensions > 2) {\n                            landmark.z = rawLandmarks[offset + 2];\n                        }\n                        if (numDimensions > 3) {\n                            landmark.score = applyActivation(config.visibilityActivation, rawLandmarks[offset + 3]);\n                        }\n                        // presence is in rawLandmarks[offset + 4], we don't expose it.\n                        outputLandmarks.push(landmark);\n                    }\n                    for (i = 0; i < outputLandmarks.length; ++i) {\n                        landmark = outputLandmarks[i];\n                        landmark.x = landmark.x / config.inputImageWidth;\n                        landmark.y = landmark.y / config.inputImageHeight;\n                        // Scale Z coordinate as X + allow additional uniform normalization.\n                        landmark.z = landmark.z / config.inputImageWidth / (config.normalizeZ || 1);\n                    }\n                    return [2 /*return*/, outputLandmarks];\n            }\n        });\n    });\n}\nexports.tensorsToLandmarks = tensorsToLandmarks;\n//# sourceMappingURL=tensors_to_landmarks.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.sigmoid = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nfunction sigmoid(value) {\n    return 1 / (1 + Math.exp(-value));\n}\nexports.sigmoid = sigmoid;\n//# sourceMappingURL=sigmoid.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.tensorsToSegmentation = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * Converts a tensor from a segmentation model to an image mask.\n * @param segmentationTensor Output from segmentation model of shape (1, height,\n *     width, channels).\n * @param config Contains activation to apply.\n * @param outputSize Desired dimensions of output image mask.\n *\n * @returns Image mask.\n */\nfunction tensorsToSegmentation(segmentationTensor, config, outputSize) {\n    return tf.tidy(function () {\n        // Remove batch dimension.\n        var $segmentationTensor = \n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        tf.squeeze(segmentationTensor, [0]);\n        var tensorChannels = $segmentationTensor.shape[2];\n        // Process mask tensor and apply activation function.\n        if (tensorChannels === 1) {\n            // Create initial working mask.\n            var smallMaskMat = $segmentationTensor;\n            switch (config.activation) {\n                case 'none':\n                    break;\n                case 'sigmoid':\n                    smallMaskMat = tf.sigmoid(smallMaskMat);\n                    break;\n                case 'softmax':\n                    throw new Error('Softmax activation requires two channels.');\n                default:\n                    throw new Error(\"Activation not supported (\" + config.activation + \")\");\n            }\n            var outputMat = outputSize ?\n                tf.image.resizeBilinear(smallMaskMat, [outputSize.height, outputSize.width]) :\n                smallMaskMat;\n            // Remove channel dimension.\n            return tf.squeeze(outputMat, [2]);\n        }\n        else {\n            throw new Error(\"Unsupported number of tensor channels \" + tensorChannels);\n        }\n    });\n}\nexports.tensorsToSegmentation = tensorsToSegmentation;\n//# sourceMappingURL=tensors_to_segmentation.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.computeNewRotation = exports.transformNormalizedRect = void 0;\nvar image_utils_1 = require(\"./image_utils\");\n/**\n * Performs geometric transformation to the input normalized rectangle,\n * correpsonding to input normalized rectangle respectively.\n * @param rect The normalized rectangle.\n * @param imageSize The original imageSize.\n * @param config See documentation in `RectTransformationConfig`.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/rect_transformation_calculator.cc\nfunction transformNormalizedRect(rect, imageSize, config) {\n    var width = rect.width;\n    var height = rect.height;\n    var rotation = rect.rotation;\n    if (config.rotation != null || config.rotationDegree != null) {\n        rotation = computeNewRotation(rotation, config);\n    }\n    if (rotation === 0) {\n        rect.xCenter = rect.xCenter + width * config.shiftX;\n        rect.yCenter = rect.yCenter + height * config.shiftY;\n    }\n    else {\n        var xShift = (imageSize.width * width * config.shiftX * Math.cos(rotation) -\n            imageSize.height * height * config.shiftY * Math.sin(rotation)) /\n            imageSize.width;\n        var yShift = (imageSize.width * width * config.shiftX * Math.sin(rotation) +\n            imageSize.height * height * config.shiftY * Math.cos(rotation)) /\n            imageSize.height;\n        rect.xCenter = rect.xCenter + xShift;\n        rect.yCenter = rect.yCenter + yShift;\n    }\n    if (config.squareLong) {\n        var longSide = Math.max(width * imageSize.width, height * imageSize.height);\n        width = longSide / imageSize.width;\n        height = longSide / imageSize.height;\n    }\n    else if (config.squareShort) {\n        var shortSide = Math.min(width * imageSize.width, height * imageSize.height);\n        width = shortSide / imageSize.width;\n        height = shortSide / imageSize.height;\n    }\n    rect.width = width * config.scaleX;\n    rect.height = height * config.scaleY;\n    return rect;\n}\nexports.transformNormalizedRect = transformNormalizedRect;\nfunction computeNewRotation(rotation, config) {\n    if (config.rotation != null) {\n        rotation += config.rotation;\n    }\n    else if (config.rotationDegree != null) {\n        rotation += Math.PI * config.rotationDegree / 180;\n    }\n    return image_utils_1.normalizeRadians(rotation);\n}\nexports.computeNewRotation = computeNewRotation;\n//# sourceMappingURL=transform_rect.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.KeypointsSmoothingFilter = void 0;\nvar get_object_scale_1 = require(\"../calculators/get_object_scale\");\nvar keypoints_to_normalized_keypoints_1 = require(\"../calculators/keypoints_to_normalized_keypoints\");\nvar normalized_keypoints_to_keypoints_1 = require(\"../calculators/normalized_keypoints_to_keypoints\");\nvar keypoints_one_euro_filter_1 = require(\"./keypoints_one_euro_filter\");\nvar keypoints_velocity_filter_1 = require(\"./keypoints_velocity_filter\");\n/**\n * A Calculator to smooth keypoints over time.\n */\nvar KeypointsSmoothingFilter = /** @class */ (function () {\n    function KeypointsSmoothingFilter(config) {\n        if (config.velocityFilter != null) {\n            this.keypointsFilter = new keypoints_velocity_filter_1.KeypointsVelocityFilter(config.velocityFilter);\n        }\n        else if (config.oneEuroFilter != null) {\n            this.keypointsFilter = new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(config.oneEuroFilter);\n        }\n        else {\n            throw new Error('Either configure velocityFilter or oneEuroFilter, but got ' +\n                (config + \".\"));\n        }\n    }\n    /**\n     * Apply one of the stateful `KeypointsFilter` to keypoints.\n     *\n     * Currently supports `OneEuroFilter` and `VelocityFilter`.\n     * @param keypoints A list of 2D or 3D keypoints, can be normalized or\n     *     non-normalized.\n     * @param timestamp The timestamp of the video frame.\n     * @param imageSize Optional. The imageSize is useful when keypoints are\n     *     normalized.\n     * @param normalized Optional. Whether the keypoints are normalized. Default\n     *     to false.\n     * @param objectScaleROI Optional. The auxiliary ROI to calculate object\n     *     scale. If not set, objectScale defaults to 1.\n     */\n    KeypointsSmoothingFilter.prototype.apply = function (keypoints, timestamp, imageSize, normalized, objectScaleROI) {\n        if (normalized === void 0) { normalized = false; }\n        if (keypoints == null) {\n            this.keypointsFilter.reset();\n            return null;\n        }\n        var objectScale = objectScaleROI != null ? get_object_scale_1.getObjectScale(objectScaleROI, imageSize) : 1;\n        var scaledKeypoints = normalized ?\n            normalized_keypoints_to_keypoints_1.normalizedKeypointsToKeypoints(keypoints, imageSize) :\n            keypoints;\n        var scaledKeypointsFiltered = this.keypointsFilter.apply(scaledKeypoints, timestamp, objectScale);\n        return normalized ?\n            keypoints_to_normalized_keypoints_1.keypointsToNormalizedKeypoints(scaledKeypointsFiltered, imageSize) :\n            scaledKeypointsFiltered;\n    };\n    return KeypointsSmoothingFilter;\n}());\nexports.KeypointsSmoothingFilter = KeypointsSmoothingFilter;\n//# sourceMappingURL=keypoints_smoothing.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getObjectScale = void 0;\n/**\n * Estimate object scale to allow filter work similarly on nearer or futher\n * objects.\n * @param roi Normalized rectangle.\n * @param imageSize An object that contains width and height.\n * @returns A number representing the object scale.\n */\nfunction getObjectScale(roi, imageSize) {\n    var objectWidth = roi.width * imageSize.width;\n    var objectHeight = roi.height * imageSize.height;\n    return (objectWidth + objectHeight) / 2;\n}\nexports.getObjectScale = getObjectScale;\n//# sourceMappingURL=get_object_scale.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.keypointsToNormalizedKeypoints = void 0;\nfunction keypointsToNormalizedKeypoints(keypoints, imageSize) {\n    return keypoints.map(function (keypoint) {\n        var normalizedKeypoint = __assign(__assign({}, keypoint), { x: keypoint.x / imageSize.width, y: keypoint.y / imageSize.height });\n        if (keypoint.z != null) {\n            // Scale z the same way as x (using image width).\n            keypoint.z = keypoint.z / imageSize.width;\n        }\n        return normalizedKeypoint;\n    });\n}\nexports.keypointsToNormalizedKeypoints = keypointsToNormalizedKeypoints;\n//# sourceMappingURL=keypoints_to_normalized_keypoints.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __spreadArrays = (this && this.__spreadArrays) || function () {\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\n            r[k] = a[j];\n    return r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.KeypointsOneEuroFilter = void 0;\nvar one_euro_filter_1 = require(\"./one_euro_filter\");\n/**\n * A stateful filter that smoothes keypoints values overtime.\n *\n * More specifically, it uses `OneEuroFilter` to smooth every x, y, z\n * coordinates over time, which as result gives us velocity of how these values\n * change over time. With higher velocity it weights new values higher.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_smoothing_calculator.cc\nvar KeypointsOneEuroFilter = /** @class */ (function () {\n    function KeypointsOneEuroFilter(config) {\n        this.config = config;\n    }\n    KeypointsOneEuroFilter.prototype.apply = function (keypoints, microSeconds, objectScale) {\n        var _this = this;\n        if (keypoints == null) {\n            this.reset();\n            return null;\n        }\n        // Initialize filters once.\n        this.initializeFiltersIfEmpty(keypoints);\n        // Get value scale as inverse value of the object scale.\n        // If value is too small smoothing will be disabled and keypoints will be\n        // returned as is.\n        var valueScale = 1;\n        if (!this.config.disableValueScaling) {\n            if (objectScale < this.config.minAllowedObjectScale) {\n                return __spreadArrays(keypoints);\n            }\n            valueScale = 1.0 / objectScale;\n        }\n        // Filter keypoints. Every axis of every keypoint is filtered separately.\n        return keypoints.map(function (keypoint, i) {\n            var outKeypoint = __assign(__assign({}, keypoint), { x: _this.xFilters[i].apply(keypoint.x, microSeconds, valueScale), y: _this.yFilters[i].apply(keypoint.y, microSeconds, valueScale) });\n            if (keypoint.z != null) {\n                outKeypoint.z =\n                    _this.zFilters[i].apply(keypoint.z, microSeconds, valueScale);\n            }\n            return outKeypoint;\n        });\n    };\n    KeypointsOneEuroFilter.prototype.reset = function () {\n        this.xFilters = null;\n        this.yFilters = null;\n        this.zFilters = null;\n    };\n    // Initializes filters for the first time or after reset. If initialized the\n    // check the size.\n    KeypointsOneEuroFilter.prototype.initializeFiltersIfEmpty = function (keypoints) {\n        var _this = this;\n        if (this.xFilters == null || this.xFilters.length !== keypoints.length) {\n            this.xFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n            this.yFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n            this.zFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n        }\n    };\n    return KeypointsOneEuroFilter;\n}());\nexports.KeypointsOneEuroFilter = KeypointsOneEuroFilter;\n//# sourceMappingURL=keypoints_one_euro_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.OneEuroFilter = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants_1 = require(\"../calculators/constants\");\nvar low_pass_filter_1 = require(\"./low_pass_filter\");\n/**\n * OneEuroFilter.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/one_euro_filter.cc\n// Also ref original paper:\n// https://cristal.univ-lille.fr/~casiez/1euro/\nvar OneEuroFilter = /** @class */ (function () {\n    /**\n     * Constructor of `OneEuroFilter` class.\n     * @param config See documentation of `OneEuroFilterConfig`.\n     */\n    function OneEuroFilter(config) {\n        this.frequency = config.frequency;\n        this.minCutOff = config.minCutOff;\n        this.beta = config.beta;\n        this.thresholdCutOff = config.thresholdCutOff;\n        this.thresholdBeta = config.thresholdBeta;\n        this.derivateCutOff = config.derivateCutOff;\n        this.x = new low_pass_filter_1.LowPassFilter(this.getAlpha(this.minCutOff));\n        this.dx = new low_pass_filter_1.LowPassFilter(this.getAlpha(this.derivateCutOff));\n        this.lastTimestamp = 0;\n    }\n    /**\n     * Applies filter to the value.\n     * @param value valueToFilter.\n     * @param microSeconds timestamp associated with the value (for instance,\n     *     timestamp of the frame where you got value from).\n     */\n    OneEuroFilter.prototype.apply = function (value, microSeconds, valueScale) {\n        if (value == null) {\n            return value;\n        }\n        var $microSeconds = Math.trunc(microSeconds);\n        if (this.lastTimestamp >= $microSeconds) {\n            // Results are unpreditable in this case, so nothing to do but return\n            // same value.\n            return value;\n        }\n        // Update the sampling frequency based on timestamps.\n        if (this.lastTimestamp !== 0 && $microSeconds !== 0) {\n            this.frequency =\n                1 / (($microSeconds - this.lastTimestamp) * constants_1.MICRO_SECONDS_TO_SECOND);\n        }\n        this.lastTimestamp = $microSeconds;\n        // Estimate the current variation per second.\n        var dValue = this.x.hasLastRawValue() ?\n            (value - this.x.lastRawValue()) * valueScale * this.frequency :\n            0;\n        var edValue = this.dx.applyWithAlpha(dValue, this.getAlpha(this.derivateCutOff));\n        var cutOff = this.minCutOff + this.beta * Math.abs(edValue);\n        var threshold = this.thresholdCutOff != null ?\n            this.thresholdCutOff + this.thresholdBeta * Math.abs(edValue) :\n            null;\n        // filter the given value.\n        return this.x.applyWithAlpha(value, this.getAlpha(cutOff), threshold);\n    };\n    OneEuroFilter.prototype.getAlpha = function (cutoff) {\n        // te = 1.0 / this.frequency\n        // tau = 1.0 / (2 * Math.PI * cutoff)\n        // result = 1 / (1.0 + (tau / te))\n        return 1.0 / (1.0 + (this.frequency / (2 * Math.PI * cutoff)));\n    };\n    return OneEuroFilter;\n}());\nexports.OneEuroFilter = OneEuroFilter;\n//# sourceMappingURL=one_euro_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.LowPassFilter = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n/**\n * A stateful filter that smoothes values overtime.\n *\n * More specifically, it stores the previous value, and when there's a new\n * value, a coefficient 'alpha' is applied to the new value, and `1 - alpha` is\n * applied to the previous value. The smaller the alpha is, the smoother result\n * and the bigger lag.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/low_pass_filter.cc\nvar LowPassFilter = /** @class */ (function () {\n    function LowPassFilter(alpha) {\n        this.alpha = alpha;\n        this.initialized = false;\n    }\n    LowPassFilter.prototype.apply = function (value, threshold) {\n        var result;\n        if (this.initialized) {\n            if (threshold == null) {\n                // Regular lowpass filter.\n                // result = this.alpha * value + (1 - this.alpha) * this.storedValue;\n                result = this.storedValue + this.alpha * (value - this.storedValue);\n            }\n            else {\n                // We need to reformat the formula to be able to conveniently apply\n                // another optional non-linear function to the\n                // (value - this.storedValue) part.\n                // Add additional non-linearity to cap extreme value.\n                // More specifically, assume x = (value - this.storedValue), when x is\n                // close zero, the derived x is close to x, when x is several magnitudes\n                // larger, the drived x grows much slower then x. It behaves like\n                // sign(x)log(abs(x)).\n                result = this.storedValue +\n                    this.alpha * threshold *\n                        Math.asinh((value - this.storedValue) / threshold);\n            }\n        }\n        else {\n            result = value;\n            this.initialized = true;\n        }\n        this.rawValue = value;\n        this.storedValue = result;\n        return result;\n    };\n    LowPassFilter.prototype.applyWithAlpha = function (value, alpha, threshold) {\n        this.alpha = alpha;\n        return this.apply(value, threshold);\n    };\n    LowPassFilter.prototype.hasLastRawValue = function () {\n        return this.initialized;\n    };\n    LowPassFilter.prototype.lastRawValue = function () {\n        return this.rawValue;\n    };\n    LowPassFilter.prototype.reset = function () {\n        this.initialized = false;\n    };\n    return LowPassFilter;\n}());\nexports.LowPassFilter = LowPassFilter;\n//# sourceMappingURL=low_pass_filter.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __spreadArrays = (this && this.__spreadArrays) || function () {\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\n            r[k] = a[j];\n    return r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.KeypointsVelocityFilter = void 0;\nvar relative_velocity_filter_1 = require(\"./relative_velocity_filter\");\n/**\n * A stateful filter that smoothes landmark values overtime.\n *\n * More specifically, it uses `RelativeVelocityFilter` to smooth every x, y, z\n * coordinates over time, which as result gives us velocity of how these values\n * change over time. With higher velocity it weights new values higher.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_smoothing_calculator.cc\nvar KeypointsVelocityFilter = /** @class */ (function () {\n    function KeypointsVelocityFilter(config) {\n        this.config = config;\n    }\n    KeypointsVelocityFilter.prototype.apply = function (keypoints, microSeconds, objectScale) {\n        var _this = this;\n        if (keypoints == null) {\n            this.reset();\n            return null;\n        }\n        // Get value scale as inverse value of the object scale.\n        // If value is too small smoothing will be disabled and keypoints will be\n        // returned as is.\n        var valueScale = 1;\n        if (!this.config.disableValueScaling) {\n            if (objectScale < this.config.minAllowedObjectScale) {\n                return __spreadArrays(keypoints);\n            }\n            valueScale = 1 / objectScale;\n        }\n        // Initialize filters once.\n        this.initializeFiltersIfEmpty(keypoints);\n        // Filter keypoints. Every axis of every keypoint is filtered separately.\n        return keypoints.map(function (keypoint, i) {\n            var outKeypoint = __assign(__assign({}, keypoint), { x: _this.xFilters[i].apply(keypoint.x, microSeconds, valueScale), y: _this.yFilters[i].apply(keypoint.y, microSeconds, valueScale) });\n            if (keypoint.z != null) {\n                outKeypoint.z =\n                    _this.zFilters[i].apply(keypoint.z, microSeconds, valueScale);\n            }\n            return outKeypoint;\n        });\n    };\n    KeypointsVelocityFilter.prototype.reset = function () {\n        this.xFilters = null;\n        this.yFilters = null;\n        this.zFilters = null;\n    };\n    // Initializes filters for the first time or after reset. If initialized the\n    // check the size.\n    KeypointsVelocityFilter.prototype.initializeFiltersIfEmpty = function (keypoints) {\n        var _this = this;\n        if (this.xFilters == null || this.xFilters.length !== keypoints.length) {\n            this.xFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n            this.yFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n            this.zFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n        }\n    };\n    return KeypointsVelocityFilter;\n}());\nexports.KeypointsVelocityFilter = KeypointsVelocityFilter;\n//# sourceMappingURL=keypoints_velocity_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.RelativeVelocityFilter = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants_1 = require(\"../calculators/constants\");\nvar low_pass_filter_1 = require(\"./low_pass_filter\");\n/**\n * This filter keeps track (on a window of specified size) of value changes\n * over time, which as result gives us velocity of how value changes over time.\n * With higher velocity it weights new values higher.\n *\n * Use `windowSize` and `velocityScale` to tweak this filter for your use case.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/relative_velocity_filter.cc\nvar RelativeVelocityFilter = /** @class */ (function () {\n    /**\n     * Constructor of `RelativeVelocityFilter` class.\n     * @param config\n     *        `windowSize`:  Higher windowSize adds to lag and to stability.\n     *        `velocityScale`: Lower velocityScale adds to lag and to stability.\n     */\n    function RelativeVelocityFilter(config) {\n        this.config = config;\n        this.window = [];\n        this.lowPassFilter = new low_pass_filter_1.LowPassFilter(1.0);\n        this.lastValue = 0;\n        this.lastValueScale = 1;\n        this.lastTimestamp = -1;\n    }\n    /**\n     * Applies filter to the value.\n     * @param value valueToFilter.\n     * @param microSeconds timestamp associated with the value (for instance,\n     *     timestamp of the frame where you got value from).\n     * @param valueScale value scale (for instance, if your value is a distance\n     *     detected on a frame, it can look same on different devices but have\n     *     quite different absolute values due to different resolution, you\n     *     should come up with an appropriate parameter for your particular use\n     *     case).\n     */\n    RelativeVelocityFilter.prototype.apply = function (value, microSeconds, valueScale) {\n        if (value == null) {\n            return value;\n        }\n        var $microSeconds = Math.trunc(microSeconds);\n        if (this.lastTimestamp >= $microSeconds) {\n            // Results are unpreditable in this case, so nothing to do but return\n            // same value.\n            return value;\n        }\n        var alpha;\n        if (this.lastTimestamp === -1) {\n            alpha = 1;\n        }\n        else {\n            // Implement the DistanceEstimationMode.kLegacyTransition.\n            // TODO(lina128): Change to kForceCurrentScale or at least add an option\n            // that can be tweaked with parameter.\n            var distance = value * valueScale - this.lastValue * this.lastValueScale;\n            var duration = $microSeconds - this.lastTimestamp;\n            var cumulativeDistance = distance;\n            var cumulativeDuration = duration;\n            // Define max cumulative duration assuming 30 frames per second is a good\n            // frame rate, so assuming 30 values per second or 1 / 30 of a second is\n            // a good duration per window element.\n            var assumedMaxDuration = constants_1.SECOND_TO_MICRO_SECONDS / 30;\n            var maxCumulativeDuration = (1 + this.window.length) * assumedMaxDuration;\n            for (var _i = 0, _a = this.window; _i < _a.length; _i++) {\n                var el = _a[_i];\n                if (cumulativeDuration + el.duration > maxCumulativeDuration) {\n                    // This helps in cases when durations are large and outdated\n                    // window elements have bad impact on filtering results.\n                    break;\n                }\n                cumulativeDistance += el.distance;\n                cumulativeDuration += el.duration;\n            }\n            var velocity = cumulativeDistance / (cumulativeDuration * constants_1.MICRO_SECONDS_TO_SECOND);\n            alpha = 1 - 1 / (1 + this.config.velocityScale * Math.abs(velocity));\n            this.window.unshift({ distance: distance, duration: duration });\n            if (this.window.length > this.config.windowSize) {\n                this.window.pop();\n            }\n        }\n        this.lastValue = value;\n        this.lastValueScale = valueScale;\n        this.lastTimestamp = $microSeconds;\n        return this.lowPassFilter.applyWithAlpha(value, alpha);\n    };\n    return RelativeVelocityFilter;\n}());\nexports.RelativeVelocityFilter = RelativeVelocityFilter;\n//# sourceMappingURL=relative_velocity_filter.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.LowPassVisibilityFilter = void 0;\nvar low_pass_filter_1 = require(\"./low_pass_filter\");\n/**\n * Smoothing visibility using a `LowPassFilter` for each landmark.\n */\nvar LowPassVisibilityFilter = /** @class */ (function () {\n    function LowPassVisibilityFilter(config) {\n        this.alpha = config.alpha;\n    }\n    LowPassVisibilityFilter.prototype.apply = function (landmarks) {\n        var _this = this;\n        if (landmarks == null) {\n            // Reset filters.\n            this.visibilityFilters = null;\n            return null;\n        }\n        if (this.visibilityFilters == null ||\n            (this.visibilityFilters.length !== landmarks.length)) {\n            // Initialize new filters.\n            this.visibilityFilters =\n                landmarks.map(function (_) { return new low_pass_filter_1.LowPassFilter(_this.alpha); });\n        }\n        var outLandmarks = [];\n        // Filter visibilities.\n        for (var i = 0; i < landmarks.length; ++i) {\n            var landmark = landmarks[i];\n            var outLandmark = __assign({}, landmark);\n            outLandmark.score = this.visibilityFilters[i].apply(landmark.score);\n            outLandmarks.push(outLandmark);\n        }\n        return outLandmarks;\n    };\n    return LowPassVisibilityFilter;\n}());\nexports.LowPassVisibilityFilter = LowPassVisibilityFilter;\n//# sourceMappingURL=visibility_smoothing.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BLAZEPOSE_SEGMENTATION_SMOOTHING_CONFIG = exports.BLAZEPOSE_TENSORS_TO_SEGMENTATION_CONFIG = exports.BLAZEPOSE_WORLD_LANDMARKS_SMOOTHING_CONFIG_ACTUAL = exports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_AUXILIARY = exports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_ACTUAL = exports.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG = exports.BLAZEPOSE_NUM_AUXILIARY_KEYPOINTS = exports.BLAZEPOSE_NUM_KEYPOINTS = exports.BLAZEPOSE_REFINE_LANDMARKS_FROM_HEATMAP_CONFIG = exports.BLAZEPOSE_TENSORS_TO_WORLD_LANDMARKS_CONFIG = exports.BLAZEPOSE_TENSORS_TO_LANDMARKS_CONFIG = exports.BLAZEPOSE_POSE_PRESENCE_SCORE = exports.BLAZEPOSE_LANDMARK_IMAGE_TO_TENSOR_CONFIG = exports.BLAZEPOSE_DETECTOR_IMAGE_TO_TENSOR_CONFIG = exports.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG = exports.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION = exports.BLAZEPOSE_TENSORS_TO_DETECTION_CONFIGURATION = exports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = exports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = exports.BLAZEPOSE_DETECTOR_ANCHOR_CONFIGURATION = exports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_HEAVY = exports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_LITE = exports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL = exports.DEFAULT_BLAZEPOSE_DETECTOR_MODEL_URL = void 0;\nexports.DEFAULT_BLAZEPOSE_DETECTOR_MODEL_URL = 'https://tfhub.dev/mediapipe/tfjs-model/blazepose_3d/detector/1';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL = 'https://tfhub.dev/mediapipe/tfjs-model/blazepose_3d/landmark/full/2';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_LITE = 'https://tfhub.dev/mediapipe/tfjs-model/blazepose_3d/landmark/lite/2';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_HEAVY = 'https://tfhub.dev/mediapipe/tfjs-model/blazepose_3d/landmark/heavy/2';\nexports.BLAZEPOSE_DETECTOR_ANCHOR_CONFIGURATION = {\n    reduceBoxesInLowestlayer: false,\n    interpolatedScaleAspectRatio: 1.0,\n    featureMapHeight: [],\n    featureMapWidth: [],\n    numLayers: 5,\n    minScale: 0.1484375,\n    maxScale: 0.75,\n    inputSizeHeight: 224,\n    inputSizeWidth: 224,\n    anchorOffsetX: 0.5,\n    anchorOffsetY: 0.5,\n    strides: [8, 16, 32, 32, 32],\n    aspectRatios: [1.0],\n    fixedAnchorSize: true\n};\nexports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = {\n    runtime: 'tfjs',\n    modelType: 'full',\n    enableSmoothing: true,\n    enableSegmentation: false,\n    smoothSegmentation: true,\n    detectorModelUrl: exports.DEFAULT_BLAZEPOSE_DETECTOR_MODEL_URL,\n    landmarkModelUrl: exports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL\n};\nexports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\nexports.BLAZEPOSE_TENSORS_TO_DETECTION_CONFIGURATION = {\n    applyExponentialOnBoxSize: false,\n    flipVertically: false,\n    ignoreClasses: [],\n    numClasses: 1,\n    numBoxes: 2254,\n    numCoords: 12,\n    boxCoordOffset: 0,\n    keypointCoordOffset: 4,\n    numKeypoints: 4,\n    numValuesPerKeypoint: 2,\n    sigmoidScore: true,\n    scoreClippingThresh: 100.0,\n    reverseOutputOrder: true,\n    xScale: 224.0,\n    yScale: 224.0,\n    hScale: 224.0,\n    wScale: 224.0,\n    minScoreThresh: 0.5\n};\nexports.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION = {\n    minSuppressionThreshold: 0.3,\n    overlapType: 'intersection-over-union'\n};\nexports.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG = {\n    shiftX: 0,\n    shiftY: 0,\n    scaleX: 1.25,\n    scaleY: 1.25,\n    squareLong: true\n};\nexports.BLAZEPOSE_DETECTOR_IMAGE_TO_TENSOR_CONFIG = {\n    outputTensorSize: { width: 224, height: 224 },\n    keepAspectRatio: true,\n    outputTensorFloatRange: [-1, 1],\n    borderMode: 'zero'\n};\nexports.BLAZEPOSE_LANDMARK_IMAGE_TO_TENSOR_CONFIG = {\n    outputTensorSize: { width: 256, height: 256 },\n    keepAspectRatio: true,\n    outputTensorFloatRange: [0, 1],\n    borderMode: 'zero'\n};\nexports.BLAZEPOSE_POSE_PRESENCE_SCORE = 0.5;\nexports.BLAZEPOSE_TENSORS_TO_LANDMARKS_CONFIG = {\n    numLandmarks: 39,\n    inputImageWidth: 256,\n    inputImageHeight: 256,\n    visibilityActivation: 'sigmoid',\n    flipHorizontally: false,\n    flipVertically: false\n};\nexports.BLAZEPOSE_TENSORS_TO_WORLD_LANDMARKS_CONFIG = {\n    numLandmarks: 39,\n    inputImageWidth: 1,\n    inputImageHeight: 1,\n    visibilityActivation: 'sigmoid',\n    flipHorizontally: false,\n    flipVertically: false\n};\nexports.BLAZEPOSE_REFINE_LANDMARKS_FROM_HEATMAP_CONFIG = {\n    kernelSize: 7,\n    minConfidenceToRefine: 0.5\n};\nexports.BLAZEPOSE_NUM_KEYPOINTS = 33;\nexports.BLAZEPOSE_NUM_AUXILIARY_KEYPOINTS = 35;\nexports.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG = {\n    alpha: 0.1\n};\nexports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_ACTUAL = {\n    oneEuroFilter: {\n        frequency: 30,\n        minCutOff: 0.05,\n        // filter when landmark is static.\n        beta: 80,\n        // alpha in landmark EMA filter when landmark is moving fast.\n        derivateCutOff: 1.0,\n        // landmark velocity EMA filter.,\n        minAllowedObjectScale: 1e-6\n    }\n};\n// Auxiliary landmarks are smoothed heavier than main landmarks to make ROI\n// crop for pose landmarks prediction very stable when object is not moving but\n// responsive enough in case of sudden movements.\nexports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_AUXILIARY = {\n    oneEuroFilter: {\n        frequency: 30,\n        minCutOff: 0.01,\n        // EMA filter when landmark is static.\n        beta: 10.0,\n        // ~0.68 alpha in landmark EMA filter when landmark is moving\n        // fast.\n        derivateCutOff: 1.0,\n        // landmark velocity EMA filter.\n        minAllowedObjectScale: 1e-6\n    }\n};\nexports.BLAZEPOSE_WORLD_LANDMARKS_SMOOTHING_CONFIG_ACTUAL = {\n    oneEuroFilter: {\n        frequency: 30,\n        minCutOff: 0.1,\n        // filter when landmark is static.\n        beta: 40,\n        // alpha in landmark EMA filter when landmark is moving fast.\n        derivateCutOff: 1.0,\n        // landmark velocity EMA filter.\n        minAllowedObjectScale: 1e-6,\n        disableValueScaling: true // As world landmarks are predicted in real world 3D coordintates\n        // in meters (rather than in pixels of input image) prediction\n        // scale does not depend on the pose size in the image.\n    }\n};\nexports.BLAZEPOSE_TENSORS_TO_SEGMENTATION_CONFIG = {\n    activation: 'none',\n};\nexports.BLAZEPOSE_SEGMENTATION_SMOOTHING_CONFIG = {\n    combineWithPreviousRatio: 0.7\n};\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.validateEstimationConfig = exports.validateModelConfig = void 0;\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig == null ?\n        __assign({}, constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG) : __assign({}, modelConfig);\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSmoothing;\n    }\n    if (config.enableSegmentation == null) {\n        config.enableSegmentation =\n            constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSegmentation;\n    }\n    if (config.smoothSegmentation == null) {\n        config.smoothSegmentation =\n            constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.smoothSegmentation;\n    }\n    if (config.modelType == null) {\n        config.modelType = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.modelType;\n    }\n    if (config.detectorModelUrl == null) {\n        config.detectorModelUrl = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.detectorModelUrl;\n    }\n    if (config.landmarkModelUrl == null) {\n        switch (config.modelType) {\n            case 'lite':\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_LITE;\n                break;\n            case 'heavy':\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_HEAVY;\n                break;\n            case 'full':\n            default:\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL;\n                break;\n        }\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    var config;\n    if (estimationConfig == null) {\n        config = constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG;\n    }\n    else {\n        config = __assign({}, estimationConfig);\n    }\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        throw new Error('Multi-pose detection is not implemented yet. Please set maxPoses ' +\n            'to 1.');\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.load = void 0;\nvar tfc = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar bounding_box_tracker_1 = require(\"../calculators/bounding_box_tracker\");\nvar keypoint_tracker_1 = require(\"../calculators/keypoint_tracker\");\nvar types_1 = require(\"../calculators/types\");\nvar constants_1 = require(\"../constants\");\nvar constants_2 = require(\"../shared/calculators/constants\");\nvar image_utils_1 = require(\"../shared/calculators/image_utils\");\nvar is_video_1 = require(\"../shared/calculators/is_video\");\nvar keypoints_one_euro_filter_1 = require(\"../shared/filters/keypoints_one_euro_filter\");\nvar low_pass_filter_1 = require(\"../shared/filters/low_pass_filter\");\nvar types_2 = require(\"../types\");\nvar util_1 = require(\"../util\");\nvar constants_3 = require(\"./constants\");\nvar crop_utils_1 = require(\"./crop_utils\");\nvar detector_utils_1 = require(\"./detector_utils\");\n/**\n * MoveNet detector class.\n */\nvar MoveNetDetector = /** @class */ (function () {\n    function MoveNetDetector(moveNetModel, config) {\n        this.moveNetModel = moveNetModel;\n        this.modelInputResolution = { height: 0, width: 0 };\n        this.keypointIndexByName = util_1.getKeypointIndexByName(types_2.SupportedModels.MoveNet);\n        // Only single-pose models have a fixed input resolution.\n        if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {\n            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;\n            this.modelInputResolution.height =\n                constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;\n        }\n        else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {\n            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;\n            this.modelInputResolution.height = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;\n        }\n        this.multiPoseModel = config.modelType === constants_3.MULTIPOSE_LIGHTNING;\n        if (!this.multiPoseModel) {\n            this.keypointFilter = new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(constants_3.KEYPOINT_FILTER_CONFIG);\n            this.cropRegionFilterYMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n            this.cropRegionFilterXMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n            this.cropRegionFilterYMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n            this.cropRegionFilterXMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n        }\n        this.enableSmoothing = config.enableSmoothing;\n        if (config.minPoseScore) {\n            this.minPoseScore = config.minPoseScore;\n        }\n        else {\n            this.minPoseScore = constants_3.DEFAULT_MIN_POSE_SCORE;\n        }\n        if (config.multiPoseMaxDimension) {\n            this.multiPoseMaxDimension = config.multiPoseMaxDimension;\n        }\n        else {\n            this.multiPoseMaxDimension = constants_3.MOVENET_MULTIPOSE_DEFAULT_MAX_DIMENSION;\n        }\n        this.enableTracking = config.enableTracking;\n        if (this.multiPoseModel && this.enableTracking) {\n            if (config.trackerType === types_1.TrackerType.Keypoint) {\n                this.tracker = new keypoint_tracker_1.KeypointTracker(config.trackerConfig);\n            }\n            else if (config.trackerType === types_1.TrackerType.BoundingBox) {\n                this.tracker = new bounding_box_tracker_1.BoundingBoxTracker(config.trackerConfig);\n            }\n            if (this.enableSmoothing) {\n                this.keypointFilterMap = new Map();\n            }\n        }\n    }\n    /**\n     * Runs inference on an image using a model that is assumed to be a single\n     * person keypoint model that outputs 17 keypoints.\n     *\n     * @param inputImage 4D tensor containing the input image. Should be of size\n     * [1, modelHeight, modelWidth, 3].\n     * @return A `Pose`.\n     */\n    MoveNetDetector.prototype.runSinglePersonPoseModel = function (inputImage) {\n        return __awaiter(this, void 0, void 0, function () {\n            var outputTensor, inferenceResult, pose, numValidKeypoints, i;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        outputTensor = this.moveNetModel.execute(inputImage);\n                        // We expect an output tensor of shape [1, 1, 17, 3] (batch, person,\n                        // keypoint, (y, x, score)).\n                        if (outputTensor.shape.length !== 4 || outputTensor.shape[0] !== 1 ||\n                            outputTensor.shape[1] !== 1 ||\n                            outputTensor.shape[2] !== constants_3.NUM_KEYPOINTS ||\n                            outputTensor.shape[3] !== constants_3.NUM_KEYPOINT_VALUES) {\n                            outputTensor.dispose();\n                            throw new Error(\"Unexpected output shape from model: [\" + outputTensor.shape + \"]\");\n                        }\n                        if (!(tf.getBackend() !== 'webgpu')) return [3 /*break*/, 1];\n                        inferenceResult = outputTensor.dataSync();\n                        return [3 /*break*/, 3];\n                    case 1: return [4 /*yield*/, outputTensor.data()];\n                    case 2:\n                        inferenceResult = _a.sent();\n                        _a.label = 3;\n                    case 3:\n                        outputTensor.dispose();\n                        pose = { keypoints: [], score: 0.0 };\n                        numValidKeypoints = 0;\n                        for (i = 0; i < constants_3.NUM_KEYPOINTS; ++i) {\n                            pose.keypoints[i] = {\n                                y: inferenceResult[i * constants_3.NUM_KEYPOINT_VALUES],\n                                x: inferenceResult[i * constants_3.NUM_KEYPOINT_VALUES + 1],\n                                score: inferenceResult[i * constants_3.NUM_KEYPOINT_VALUES + 2]\n                            };\n                            if (pose.keypoints[i].score > constants_3.MIN_CROP_KEYPOINT_SCORE) {\n                                ++numValidKeypoints;\n                                pose.score += pose.keypoints[i].score;\n                            }\n                        }\n                        if (numValidKeypoints > 0) {\n                            pose.score /= numValidKeypoints;\n                        }\n                        return [2 /*return*/, pose];\n                }\n            });\n        });\n    };\n    /**\n     * Runs inference on an image using a model that is assumed to be a\n     * multi-person keypoint model that outputs 17 keypoints and a box for a\n     * multiple persons.\n     *\n     * @param inputImage 4D tensor containing the input image. Should be of size\n     * [1, width, height, 3], where width and height are divisible by 32.\n     * @return An array of `Pose`s.\n     */\n    MoveNetDetector.prototype.runMultiPersonPoseModel = function (inputImage) {\n        return __awaiter(this, void 0, void 0, function () {\n            var outputTensor, inferenceResult, poses, numInstances, i, boxIndex, scoreIndex, j;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        outputTensor = this.moveNetModel.execute(inputImage);\n                        // Multi-pose model output is a [1, n, 56] tensor ([batch, num_instances,\n                        // instance_keypoints_and_box]).\n                        if (outputTensor.shape.length !== 3 || outputTensor.shape[0] !== 1 ||\n                            outputTensor.shape[2] !== constants_3.MULTIPOSE_INSTANCE_SIZE) {\n                            outputTensor.dispose();\n                            throw new Error(\"Unexpected output shape from model: [\" + outputTensor.shape + \"]\");\n                        }\n                        if (!(tf.getBackend() !== 'webgpu')) return [3 /*break*/, 1];\n                        inferenceResult = outputTensor.dataSync();\n                        return [3 /*break*/, 3];\n                    case 1: return [4 /*yield*/, outputTensor.data()];\n                    case 2:\n                        inferenceResult = _a.sent();\n                        _a.label = 3;\n                    case 3:\n                        outputTensor.dispose();\n                        poses = [];\n                        numInstances = inferenceResult.length / constants_3.MULTIPOSE_INSTANCE_SIZE;\n                        for (i = 0; i < numInstances; ++i) {\n                            poses[i] = { keypoints: [] };\n                            boxIndex = i * constants_3.MULTIPOSE_INSTANCE_SIZE + constants_3.MULTIPOSE_BOX_IDX;\n                            poses[i].box = {\n                                yMin: inferenceResult[boxIndex],\n                                xMin: inferenceResult[boxIndex + 1],\n                                yMax: inferenceResult[boxIndex + 2],\n                                xMax: inferenceResult[boxIndex + 3],\n                                width: inferenceResult[boxIndex + 3] - inferenceResult[boxIndex + 1],\n                                height: inferenceResult[boxIndex + 2] - inferenceResult[boxIndex]\n                            };\n                            scoreIndex = i * constants_3.MULTIPOSE_INSTANCE_SIZE + constants_3.MULTIPOSE_BOX_SCORE_IDX;\n                            poses[i].score = inferenceResult[scoreIndex];\n                            poses[i].keypoints = [];\n                            for (j = 0; j < constants_3.NUM_KEYPOINTS; ++j) {\n                                poses[i].keypoints[j] = {\n                                    y: inferenceResult[i * constants_3.MULTIPOSE_INSTANCE_SIZE + j * constants_3.NUM_KEYPOINT_VALUES],\n                                    x: inferenceResult[i * constants_3.MULTIPOSE_INSTANCE_SIZE + j * constants_3.NUM_KEYPOINT_VALUES + 1],\n                                    score: inferenceResult[i * constants_3.MULTIPOSE_INSTANCE_SIZE + j * constants_3.NUM_KEYPOINT_VALUES + 2]\n                                };\n                            }\n                        }\n                        return [2 /*return*/, poses];\n                }\n            });\n        });\n    };\n    /**\n     * Estimates poses for an image or video frame. This does standard ImageNet\n     * pre-processing before inferring through the model. The image pixels should\n     * have values [0-255]. It returns an array of poses.\n     *\n     * @param image ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement\n     * The input image to feed through the network.\n     * @param config Optional. Currently not used.\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     * a tensor, which doesn't have timestamp info. Or to override timestamp in a\n     * video.\n     * @return An array of `Pose`s.\n     */\n    MoveNetDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        if (estimationConfig === void 0) { estimationConfig = constants_3.MOVENET_ESTIMATION_CONFIG; }\n        return __awaiter(this, void 0, void 0, function () {\n            var imageTensor3D, imageSize, imageTensor4D, poses, poseIdx, keypointIdx;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        estimationConfig = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        if (timestamp == null) {\n                            if (is_video_1.isVideo(image)) {\n                                timestamp = image.currentTime * constants_2.SECOND_TO_MICRO_SECONDS;\n                            }\n                        }\n                        else {\n                            timestamp = timestamp * constants_2.MILLISECOND_TO_MICRO_SECONDS;\n                        }\n                        imageTensor3D = image_utils_1.toImageTensor(image);\n                        imageSize = image_utils_1.getImageSize(imageTensor3D);\n                        imageTensor4D = tf.expandDims(imageTensor3D, 0);\n                        // Make sure we don't dispose the input image if it's already a tensor.\n                        if (!(image instanceof tf.Tensor)) {\n                            imageTensor3D.dispose();\n                        }\n                        poses = [];\n                        if (!!this.multiPoseModel) return [3 /*break*/, 2];\n                        return [4 /*yield*/, this.estimateSinglePose(imageTensor4D, imageSize, timestamp)];\n                    case 1:\n                        poses =\n                            _a.sent();\n                        return [3 /*break*/, 4];\n                    case 2: return [4 /*yield*/, this.estimateMultiplePoses(imageTensor4D, imageSize, timestamp)];\n                    case 3:\n                        poses =\n                            _a.sent();\n                        _a.label = 4;\n                    case 4:\n                        // Convert keypoint coordinates from normalized coordinates to image space\n                        // and add keypoint names.\n                        for (poseIdx = 0; poseIdx < poses.length; ++poseIdx) {\n                            for (keypointIdx = 0; keypointIdx < poses[poseIdx].keypoints.length; ++keypointIdx) {\n                                poses[poseIdx].keypoints[keypointIdx].name =\n                                    constants_1.COCO_KEYPOINTS[keypointIdx];\n                                poses[poseIdx].keypoints[keypointIdx].y *= imageSize.height;\n                                poses[poseIdx].keypoints[keypointIdx].x *= imageSize.width;\n                            }\n                        }\n                        return [2 /*return*/, poses];\n                }\n            });\n        });\n    };\n    /**\n     * Runs a single-person keypoint model on an image, including the image\n     * cropping and keypoint filtering logic.\n     *\n     * @param imageTensor4D A tf.Tensor4D that contains the input image.\n     * @param imageSize: The width and height of the input image.\n     * @param timestamp Image timestamp in microseconds.\n     * @return An array of `Pose`s.\n     */\n    MoveNetDetector.prototype.estimateSinglePose = function (imageTensor4D, imageSize, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            var croppedImage, pose, i, nextCropRegion;\n            var _this = this;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        if (!this.cropRegion) {\n                            this.cropRegion = crop_utils_1.initCropRegion(this.cropRegion == null, imageSize);\n                        }\n                        croppedImage = tf.tidy(function () {\n                            // Crop region is a [batch, 4] size tensor.\n                            var cropRegionTensor = tf.tensor2d([[\n                                    _this.cropRegion.yMin, _this.cropRegion.xMin, _this.cropRegion.yMax,\n                                    _this.cropRegion.xMax\n                                ]]);\n                            // The batch index that the crop should operate on. A [batch] size\n                            // tensor.\n                            var boxInd = tf.zeros([1], 'int32');\n                            // Target size of each crop.\n                            var cropSize = [_this.modelInputResolution.height, _this.modelInputResolution.width];\n                            return tf.cast(tf.image.cropAndResize(imageTensor4D, cropRegionTensor, boxInd, cropSize, 'bilinear', 0), 'int32');\n                        });\n                        imageTensor4D.dispose();\n                        return [4 /*yield*/, this.runSinglePersonPoseModel(croppedImage)];\n                    case 1:\n                        pose = _a.sent();\n                        croppedImage.dispose();\n                        if (pose.score < this.minPoseScore) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        // Convert keypoints from crop coordinates to image coordinates.\n                        for (i = 0; i < pose.keypoints.length; ++i) {\n                            pose.keypoints[i].y =\n                                this.cropRegion.yMin + pose.keypoints[i].y * this.cropRegion.height;\n                            pose.keypoints[i].x =\n                                this.cropRegion.xMin + pose.keypoints[i].x * this.cropRegion.width;\n                        }\n                        // Apply the sequential filter before estimating the cropping area to make\n                        // it more stable.\n                        if (timestamp != null && this.enableSmoothing) {\n                            pose.keypoints = this.keypointFilter.apply(pose.keypoints, timestamp, 1 /* objectScale */);\n                        }\n                        nextCropRegion = crop_utils_1.determineNextCropRegion(this.cropRegion, pose.keypoints, this.keypointIndexByName, imageSize);\n                        this.cropRegion = this.filterCropRegion(nextCropRegion);\n                        return [2 /*return*/, [pose]];\n                }\n            });\n        });\n    };\n    /**\n     * Runs a multi-person keypoint model on an image, including image\n     * preprocessing.\n     *\n     * @param imageTensor4D A tf.Tensor4D that contains the input image.\n     * @param imageSize: The width and height of the input image.\n     * @param timestamp Image timestamp in microseconds.\n     * @return An array of `Pose`s.\n     */\n    MoveNetDetector.prototype.estimateMultiplePoses = function (imageTensor4D, imageSize, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            var resizedImage, resizedWidth, resizedHeight, paddedImage, paddedWidth, paddedHeight, dimensionDivisor, paddedImageInt32, poses, i, j, i, trackIDs_1;\n            var _this = this;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        dimensionDivisor = 32;\n                        if (imageSize.width > imageSize.height) {\n                            resizedWidth = this.multiPoseMaxDimension;\n                            resizedHeight = Math.round(this.multiPoseMaxDimension * imageSize.height / imageSize.width);\n                            resizedImage =\n                                tf.image.resizeBilinear(imageTensor4D, [resizedHeight, resizedWidth]);\n                            paddedWidth = resizedWidth;\n                            paddedHeight =\n                                Math.ceil(resizedHeight / dimensionDivisor) * dimensionDivisor;\n                            paddedImage = tf.pad(resizedImage, [[0, 0], [0, paddedHeight - resizedHeight], [0, 0], [0, 0]]);\n                        }\n                        else {\n                            resizedWidth = Math.round(this.multiPoseMaxDimension * imageSize.width / imageSize.height);\n                            resizedHeight = this.multiPoseMaxDimension;\n                            resizedImage =\n                                tf.image.resizeBilinear(imageTensor4D, [resizedHeight, resizedWidth]);\n                            paddedWidth =\n                                Math.ceil(resizedWidth / dimensionDivisor) * dimensionDivisor;\n                            paddedHeight = resizedHeight;\n                            paddedImage = tf.pad(resizedImage, [[0, 0], [0, 0], [0, paddedWidth - resizedWidth], [0, 0]]);\n                        }\n                        resizedImage.dispose();\n                        imageTensor4D.dispose();\n                        paddedImageInt32 = tf.cast(paddedImage, 'int32');\n                        paddedImage.dispose();\n                        return [4 /*yield*/, this.runMultiPersonPoseModel(paddedImageInt32)];\n                    case 1:\n                        poses = _a.sent();\n                        paddedImageInt32.dispose();\n                        poses = poses.filter(function (pose) { return pose.score >= _this.minPoseScore; });\n                        // Convert keypoints from padded coordinates to normalized coordinates.\n                        for (i = 0; i < poses.length; ++i) {\n                            for (j = 0; j < poses[i].keypoints.length; ++j) {\n                                poses[i].keypoints[j].y *= paddedHeight / resizedHeight;\n                                poses[i].keypoints[j].x *= paddedWidth / resizedWidth;\n                            }\n                        }\n                        if (this.enableTracking) {\n                            this.tracker.apply(poses, timestamp);\n                            if (this.enableSmoothing) {\n                                for (i = 0; i < poses.length; ++i) {\n                                    if (!this.keypointFilterMap.has(poses[i].id)) {\n                                        this.keypointFilterMap.set(poses[i].id, new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(constants_3.KEYPOINT_FILTER_CONFIG));\n                                    }\n                                    poses[i].keypoints =\n                                        this.keypointFilterMap.get(poses[i].id)\n                                            .apply(poses[i].keypoints, timestamp, 1 /* objectScale */);\n                                }\n                                trackIDs_1 = this.tracker.getTrackIDs();\n                                this.keypointFilterMap.forEach(function (_, trackID) {\n                                    if (!trackIDs_1.has(trackID)) {\n                                        _this.keypointFilterMap.delete(trackID);\n                                    }\n                                });\n                            }\n                        }\n                        return [2 /*return*/, poses];\n                }\n            });\n        });\n    };\n    MoveNetDetector.prototype.filterCropRegion = function (newCropRegion) {\n        if (!newCropRegion) {\n            this.cropRegionFilterYMin.reset();\n            this.cropRegionFilterXMin.reset();\n            this.cropRegionFilterYMax.reset();\n            this.cropRegionFilterXMax.reset();\n            return null;\n        }\n        else {\n            var filteredYMin = this.cropRegionFilterYMin.apply(newCropRegion.yMin);\n            var filteredXMin = this.cropRegionFilterXMin.apply(newCropRegion.xMin);\n            var filteredYMax = this.cropRegionFilterYMax.apply(newCropRegion.yMax);\n            var filteredXMax = this.cropRegionFilterXMax.apply(newCropRegion.xMax);\n            return {\n                yMin: filteredYMin,\n                xMin: filteredXMin,\n                yMax: filteredYMax,\n                xMax: filteredXMax,\n                height: filteredYMax - filteredYMin,\n                width: filteredXMax - filteredXMin\n            };\n        }\n    };\n    MoveNetDetector.prototype.dispose = function () {\n        this.moveNetModel.dispose();\n    };\n    MoveNetDetector.prototype.reset = function () {\n        this.cropRegion = null;\n        this.resetFilters();\n    };\n    MoveNetDetector.prototype.resetFilters = function () {\n        this.keypointFilter.reset();\n        this.cropRegionFilterYMin.reset();\n        this.cropRegionFilterXMin.reset();\n        this.cropRegionFilterYMax.reset();\n        this.cropRegionFilterXMax.reset();\n    };\n    return MoveNetDetector;\n}());\n/**\n * Loads the MoveNet model instance from a checkpoint. The model to be loaded\n * is configurable using the config dictionary `ModelConfig`. Please find more\n * details in the documentation of the `ModelConfig`.\n *\n * @param config `ModelConfig` dictionary that contains parameters for\n * the MoveNet loading process. Please find more details of each parameter\n * in the documentation of the `ModelConfig` interface.\n */\nfunction load(modelConfig) {\n    if (modelConfig === void 0) { modelConfig = constants_3.MOVENET_CONFIG; }\n    return __awaiter(this, void 0, void 0, function () {\n        var config, model, fromTFHub, modelUrl;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    fromTFHub = true;\n                    if (!!!config.modelUrl) return [3 /*break*/, 2];\n                    fromTFHub = typeof config.modelUrl === 'string' &&\n                        config.modelUrl.indexOf('https://tfhub.dev') > -1;\n                    return [4 /*yield*/, tfc.loadGraphModel(config.modelUrl, { fromTFHub: fromTFHub })];\n                case 1:\n                    model = _a.sent();\n                    return [3 /*break*/, 4];\n                case 2:\n                    modelUrl = void 0;\n                    if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {\n                        modelUrl = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_URL;\n                    }\n                    else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {\n                        modelUrl = constants_3.MOVENET_SINGLEPOSE_THUNDER_URL;\n                    }\n                    else if (config.modelType === constants_3.MULTIPOSE_LIGHTNING) {\n                        modelUrl = constants_3.MOVENET_MULTIPOSE_LIGHTNING_URL;\n                    }\n                    return [4 /*yield*/, tfc.loadGraphModel(modelUrl, { fromTFHub: fromTFHub })];\n                case 3:\n                    model = _a.sent();\n                    _a.label = 4;\n                case 4:\n                    if (tf.getBackend() === 'webgl') {\n                        // MoveNet has a top-k op that runs faster on GPU for the size of our last\n                        // dimension (6400). There are three checks that could make the top-k op run\n                        // on CPU (see\n                        // https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-webgl/src/kernels/TopK.ts)\n                        //\n                        // 1. All input shapes < 128\n                        // 2. lastDim < TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD\n                        // 3. k > TOPK_K_CPU_HANDOFF_THRESHOLD\n                        //\n                        // In our case, setting TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD = 0 will\n                        // will disable the CPU forwarding.\n                        tf.env().set('TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD', 0);\n                    }\n                    return [2 /*return*/, new MoveNetDetector(model, config)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BoundingBoxTracker = void 0;\nvar tracker_1 = require(\"./tracker\");\n/**\n * BoundingBoxTracker, which tracks objects based on bounding box similarity,\n * currently defined as intersection-over-union (IoU).\n */\nvar BoundingBoxTracker = /** @class */ (function (_super) {\n    __extends(BoundingBoxTracker, _super);\n    function BoundingBoxTracker(config) {\n        return _super.call(this, config) || this;\n    }\n    /**\n     * Computes similarity based on intersection-over-union (IoU). See `Tracker`\n     * for more details.\n     */\n    BoundingBoxTracker.prototype.computeSimilarity = function (poses) {\n        var _this = this;\n        if (poses.length === 0 || this.tracks.length === 0) {\n            return [[]];\n        }\n        var simMatrix = poses.map(function (pose) {\n            return _this.tracks.map(function (track) {\n                return _this.iou(pose, track);\n            });\n        });\n        return simMatrix;\n    };\n    /**\n     * Computes the intersection-over-union (IoU) between a pose and a track.\n     * @param pose A `Pose`.\n     * @param track A `Track`.\n     * @returns The IoU  between the pose and the track. This number is\n     * between 0 and 1, and larger values indicate more box similarity.\n     */\n    BoundingBoxTracker.prototype.iou = function (pose, track) {\n        var xMin = Math.max(pose.box.xMin, track.box.xMin);\n        var yMin = Math.max(pose.box.yMin, track.box.yMin);\n        var xMax = Math.min(pose.box.xMax, track.box.xMax);\n        var yMax = Math.min(pose.box.yMax, track.box.yMax);\n        if (xMin >= xMax || yMin >= yMax) {\n            return 0.0;\n        }\n        var intersection = (xMax - xMin) * (yMax - yMin);\n        var areaPose = pose.box.width * pose.box.height;\n        var areaTrack = track.box.width * track.box.height;\n        return intersection / (areaPose + areaTrack - intersection);\n    };\n    return BoundingBoxTracker;\n}(tracker_1.Tracker));\nexports.BoundingBoxTracker = BoundingBoxTracker;\n//# sourceMappingURL=bounding_box_tracker.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __spreadArrays = (this && this.__spreadArrays) || function () {\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\n            r[k] = a[j];\n    return r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Tracker = void 0;\nvar tracker_utils_1 = require(\"./tracker_utils\");\n/**\n * A stateful tracker for associating detections between frames. This is an\n * abstract base class that performs generic mechanics. Implementations must\n * inherit from this class.\n */\nvar Tracker = /** @class */ (function () {\n    function Tracker(config) {\n        tracker_utils_1.validateTrackerConfig(config);\n        this.tracks = [];\n        this.maxTracks = config.maxTracks;\n        this.maxAge = config.maxAge * 1000; // Convert msec to usec.\n        this.minSimilarity = config.minSimilarity;\n        this.nextID = 1;\n    }\n    /**\n     * Tracks person instances across frames based on detections.\n     * @param poses An array of detected `Pose`s.\n     * @param timestamp The timestamp associated with the incoming poses, in\n     * microseconds.\n     * @returns An updated array of `Pose`s with tracking id properties.\n     */\n    Tracker.prototype.apply = function (poses, timestamp) {\n        this.filterOldTracks(timestamp);\n        var simMatrix = this.computeSimilarity(poses);\n        this.assignTracks(poses, simMatrix, timestamp);\n        this.updateTracks(timestamp);\n        return poses;\n    };\n    /**\n     * Returns a copy of the stored tracks.\n     */\n    Tracker.prototype.getTracks = function () {\n        return this.tracks.slice();\n    };\n    /**\n     * Returns a Set of active track IDs.\n     */\n    Tracker.prototype.getTrackIDs = function () {\n        return new Set(this.tracks.map(function (track) { return track.id; }));\n    };\n    /**\n     * Filters tracks based on their age.\n     * @param timestamp The current timestamp in microseconds.\n     */\n    Tracker.prototype.filterOldTracks = function (timestamp) {\n        var _this = this;\n        this.tracks = this.tracks.filter(function (track) {\n            return timestamp - track.lastTimestamp <= _this.maxAge;\n        });\n    };\n    /**\n     * Performs a greedy optimization to link detections with tracks. The `poses`\n     * array is updated in place by providing an `id` property. If incoming\n     * detections are not linked with existing tracks, new tracks will be created.\n     * @param poses An array of detected `Pose`s. It's assumed that poses are\n     * sorted from most confident to least confident.\n     * @param simMatrix A 2D array of shape [num_det, num_tracks] with pairwise\n     * similarity scores between detections and tracks.\n     * @param timestamp The current timestamp in microseconds.\n     */\n    Tracker.prototype.assignTracks = function (poses, simMatrix, timestamp) {\n        var unmatchedTrackIndices = Array.from(Array(simMatrix[0].length).keys());\n        var detectionIndices = Array.from(Array(poses.length).keys());\n        var unmatchedDetectionIndices = [];\n        for (var _i = 0, detectionIndices_1 = detectionIndices; _i < detectionIndices_1.length; _i++) {\n            var detectionIndex = detectionIndices_1[_i];\n            if (unmatchedTrackIndices.length === 0) {\n                unmatchedDetectionIndices.push(detectionIndex);\n                continue;\n            }\n            // Assign the detection to the track which produces the highest pairwise\n            // similarity score, assuming the score exceeds the minimum similarity\n            // threshold.\n            var maxTrackIndex = -1;\n            var maxSimilarity = -1;\n            for (var _a = 0, unmatchedTrackIndices_1 = unmatchedTrackIndices; _a < unmatchedTrackIndices_1.length; _a++) {\n                var trackIndex = unmatchedTrackIndices_1[_a];\n                var similarity = simMatrix[detectionIndex][trackIndex];\n                if (similarity >= this.minSimilarity && similarity > maxSimilarity) {\n                    maxTrackIndex = trackIndex;\n                    maxSimilarity = similarity;\n                }\n            }\n            if (maxTrackIndex >= 0) {\n                // Link the detection with the highest scoring track.\n                var linkedTrack = this.tracks[maxTrackIndex];\n                linkedTrack = Object.assign(linkedTrack, this.createTrack(poses[detectionIndex], timestamp, linkedTrack.id));\n                poses[detectionIndex].id = linkedTrack.id;\n                var index = unmatchedTrackIndices.indexOf(maxTrackIndex);\n                unmatchedTrackIndices.splice(index, 1);\n            }\n            else {\n                unmatchedDetectionIndices.push(detectionIndex);\n            }\n        }\n        // Spawn new tracks for all unmatched detections.\n        for (var _b = 0, unmatchedDetectionIndices_1 = unmatchedDetectionIndices; _b < unmatchedDetectionIndices_1.length; _b++) {\n            var detectionIndex = unmatchedDetectionIndices_1[_b];\n            var newTrack = this.createTrack(poses[detectionIndex], timestamp);\n            this.tracks.push(newTrack);\n            poses[detectionIndex].id = newTrack.id;\n        }\n    };\n    /**\n     * Updates the stored tracks in the tracker. Specifically, the following\n     * operations are applied in order:\n     * 1. Tracks are sorted based on freshness (i.e. the most recently linked\n     *    tracks are placed at the beginning of the array and the most stale are\n     *    at the end).\n     * 2. The tracks array is sliced to only contain `maxTracks` tracks (i.e. the\n     *    most fresh tracks).\n     * @param timestamp The current timestamp in microseconds.\n     */\n    Tracker.prototype.updateTracks = function (timestamp) {\n        // Sort tracks from most recent to most stale, and then only keep the top\n        // `maxTracks` tracks.\n        this.tracks.sort(function (ta, tb) { return tb.lastTimestamp - ta.lastTimestamp; });\n        this.tracks = this.tracks.slice(0, this.maxTracks);\n    };\n    /**\n     * Creates a track from information in a pose.\n     * @param pose A `Pose`.\n     * @param timestamp The current timestamp in microseconds.\n     * @param trackID The id to assign to the new track. If not provided,\n     * will assign the next available id.\n     * @returns A `Track`.\n     */\n    Tracker.prototype.createTrack = function (pose, timestamp, trackID) {\n        var track = {\n            id: trackID || this.nextTrackID(),\n            lastTimestamp: timestamp,\n            keypoints: __spreadArrays(pose.keypoints).map(function (keypoint) { return (__assign({}, keypoint)); })\n        };\n        if (pose.box !== undefined) {\n            track.box = __assign({}, pose.box);\n        }\n        return track;\n    };\n    /**\n     * Returns the next free track ID.\n     */\n    Tracker.prototype.nextTrackID = function () {\n        var nextID = this.nextID;\n        this.nextID += 1;\n        return nextID;\n    };\n    /**\n     * Removes specific tracks, based on their ids.\n     */\n    Tracker.prototype.remove = function () {\n        var ids = [];\n        for (var _i = 0; _i < arguments.length; _i++) {\n            ids[_i] = arguments[_i];\n        }\n        this.tracks = this.tracks.filter(function (track) { return !ids.includes(track.id); });\n    };\n    /**\n     * Resets tracks.\n     */\n    Tracker.prototype.reset = function () {\n        this.tracks = [];\n    };\n    return Tracker;\n}());\nexports.Tracker = Tracker;\n//# sourceMappingURL=tracker.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.validateTrackerConfig = void 0;\nfunction validateTrackerConfig(config) {\n    if (config.maxTracks < 1) {\n        throw new Error(\"Must specify 'maxTracks' to be at least 1, but \" +\n            (\"encountered \" + config.maxTracks));\n    }\n    if (config.maxAge <= 0) {\n        throw new Error(\"Must specify 'maxAge' to be positive, but \" +\n            (\"encountered \" + config.maxAge));\n    }\n    if (config.keypointTrackerParams !== undefined) {\n        if (config.keypointTrackerParams.keypointConfidenceThreshold < 0 ||\n            config.keypointTrackerParams.keypointConfidenceThreshold > 1) {\n            throw new Error(\"Must specify 'keypointConfidenceThreshold' to be in the range \" +\n                \"[0, 1], but encountered \" +\n                (\"\" + config.keypointTrackerParams.keypointConfidenceThreshold));\n        }\n        if (config.keypointTrackerParams.minNumberOfKeypoints < 1) {\n            throw new Error(\"Must specify 'minNumberOfKeypoints' to be at least 1, but \" +\n                (\"encountered \" + config.keypointTrackerParams.minNumberOfKeypoints));\n        }\n        for (var _i = 0, _a = config.keypointTrackerParams.keypointFalloff; _i < _a.length; _i++) {\n            var falloff = _a[_i];\n            if (falloff <= 0.0) {\n                throw new Error(\"Must specify each keypoint falloff parameterto be positive \" +\n                    (\"but encountered \" + falloff));\n            }\n        }\n    }\n}\nexports.validateTrackerConfig = validateTrackerConfig;\n//# sourceMappingURL=tracker_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nvar __spreadArrays = (this && this.__spreadArrays) || function () {\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\n            r[k] = a[j];\n    return r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.KeypointTracker = void 0;\nvar tracker_1 = require(\"./tracker\");\n/**\n * KeypointTracker, which tracks poses based on keypoint similarity. This\n * tracker assumes that keypoints are provided in normalized image\n * coordinates.\n */\nvar KeypointTracker = /** @class */ (function (_super) {\n    __extends(KeypointTracker, _super);\n    function KeypointTracker(config) {\n        var _this = _super.call(this, config) || this;\n        _this.keypointThreshold =\n            config.keypointTrackerParams.keypointConfidenceThreshold;\n        _this.keypointFalloff = config.keypointTrackerParams.keypointFalloff;\n        _this.minNumKeyoints = config.keypointTrackerParams.minNumberOfKeypoints;\n        return _this;\n    }\n    /**\n     * Computes similarity based on Object Keypoint Similarity (OKS). It's\n     * assumed that the keypoints within each `Pose` are in normalized image\n     * coordinates. See `Tracker` for more details.\n     */\n    KeypointTracker.prototype.computeSimilarity = function (poses) {\n        if (poses.length === 0 || this.tracks.length === 0) {\n            return [[]];\n        }\n        var simMatrix = [];\n        for (var _i = 0, poses_1 = poses; _i < poses_1.length; _i++) {\n            var pose = poses_1[_i];\n            var row = [];\n            for (var _a = 0, _b = this.tracks; _a < _b.length; _a++) {\n                var track = _b[_a];\n                row.push(this.oks(pose, track));\n            }\n            simMatrix.push(row);\n        }\n        return simMatrix;\n    };\n    /**\n     * Computes the Object Keypoint Similarity (OKS) between a pose and track.\n     * This is similar in spirit to the calculation used by COCO keypoint eval:\n     * https://cocodataset.org/#keypoints-eval\n     * In this case, OKS is calculated as:\n     * (1/sum_i d(c_i, c_ti)) * \\sum_i exp(-d_i^2/(2*a_ti*x_i^2))*d(c_i, c_ti)\n     * where\n     *   d(x, y) is an indicator function which only produces 1 if x and y\n     *     exceed a given threshold (i.e. keypointThreshold), otherwise 0.\n     *   c_i is the confidence of keypoint i from the new pose\n     *   c_ti is the confidence of keypoint i from the track\n     *   d_i is the Euclidean distance between the pose and track keypoint\n     *   a_ti is the area of the track object (the box covering the keypoints)\n     *   x_i is a constant that controls falloff in a Gaussian distribution,\n     *    computed as 2*keypointFalloff[i].\n     * @param pose A `Pose`.\n     * @param track A `Track`.\n     * @returns The OKS score between the pose and the track. This number is\n     * between 0 and 1, and larger values indicate more keypoint similarity.\n     */\n    KeypointTracker.prototype.oks = function (pose, track) {\n        var boxArea = this.area(track.keypoints) + 1e-6;\n        var oksTotal = 0;\n        var numValidKeypoints = 0;\n        for (var i = 0; i < pose.keypoints.length; ++i) {\n            var poseKpt = pose.keypoints[i];\n            var trackKpt = track.keypoints[i];\n            if (poseKpt.score < this.keypointThreshold ||\n                trackKpt.score < this.keypointThreshold) {\n                continue;\n            }\n            numValidKeypoints += 1;\n            var dSquared = Math.pow(poseKpt.x - trackKpt.x, 2) +\n                Math.pow(poseKpt.y - trackKpt.y, 2);\n            var x = 2 * this.keypointFalloff[i];\n            oksTotal += Math.exp(-1 * dSquared / (2 * boxArea * Math.pow(x, 2)));\n        }\n        if (numValidKeypoints < this.minNumKeyoints) {\n            return 0.0;\n        }\n        return oksTotal / numValidKeypoints;\n    };\n    /**\n     * Computes the area of a bounding box that tightly covers keypoints.\n     * @param Keypoint[] An array of `Keypoint`s.\n     * @returns The area of the object.\n     */\n    KeypointTracker.prototype.area = function (keypoints) {\n        var _this = this;\n        var validKeypoint = keypoints.filter(function (kpt) { return kpt.score > _this.keypointThreshold; });\n        var minX = Math.min.apply(Math, __spreadArrays([1.0], validKeypoint.map(function (kpt) { return kpt.x; })));\n        var maxX = Math.max.apply(Math, __spreadArrays([0.0], validKeypoint.map(function (kpt) { return kpt.x; })));\n        var minY = Math.min.apply(Math, __spreadArrays([1.0], validKeypoint.map(function (kpt) { return kpt.y; })));\n        var maxY = Math.max.apply(Math, __spreadArrays([0.0], validKeypoint.map(function (kpt) { return kpt.y; })));\n        return (maxX - minX) * (maxY - minY);\n    };\n    return KeypointTracker;\n}(tracker_1.Tracker));\nexports.KeypointTracker = KeypointTracker;\n//# sourceMappingURL=keypoint_tracker.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.TrackerType = void 0;\n/**\n * All supported tracker types.\n */\nvar TrackerType;\n(function (TrackerType) {\n    TrackerType[\"Keypoint\"] = \"keypoint\";\n    TrackerType[\"BoundingBox\"] = \"boundingBox\";\n})(TrackerType = exports.TrackerType || (exports.TrackerType = {}));\n//# sourceMappingURL=types.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.SupportedModels = void 0;\nvar SupportedModels;\n(function (SupportedModels) {\n    SupportedModels[\"MoveNet\"] = \"MoveNet\";\n    SupportedModels[\"BlazePose\"] = \"BlazePose\";\n    SupportedModels[\"PoseNet\"] = \"PoseNet\";\n})(SupportedModels = exports.SupportedModels || (exports.SupportedModels = {}));\n//# sourceMappingURL=types.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getKeypointIndexByName = exports.getAdjacentPairs = exports.getKeypointIndexBySide = void 0;\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants = require(\"./constants\");\nvar types_1 = require(\"./types\");\nfunction getKeypointIndexBySide(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_KEYPOINTS_BY_SIDE;\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_KEYPOINTS_BY_SIDE;\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getKeypointIndexBySide = getKeypointIndexBySide;\nfunction getAdjacentPairs(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_CONNECTED_KEYPOINTS_PAIRS;\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_CONNECTED_KEYPOINTS_PAIRS;\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getAdjacentPairs = getAdjacentPairs;\nfunction getKeypointIndexByName(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_KEYPOINTS.reduce(function (map, name, i) {\n                map[name] = i;\n                return map;\n            }, {});\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_KEYPOINTS.reduce(function (map, name, i) {\n                map[name] = i;\n                return map;\n            }, {});\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getKeypointIndexByName = getKeypointIndexByName;\n//# sourceMappingURL=util.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DEFAULT_BOUNDING_BOX_TRACKER_CONFIG = exports.DEFAULT_KEYPOINT_TRACKER_CONFIG = exports.MULTIPOSE_INSTANCE_SIZE = exports.MULTIPOSE_BOX_SCORE_IDX = exports.MULTIPOSE_BOX_IDX = exports.MULTIPOSE_BOX_SIZE = exports.NUM_KEYPOINT_VALUES = exports.NUM_KEYPOINTS = exports.DEFAULT_MIN_POSE_SCORE = exports.MIN_CROP_KEYPOINT_SCORE = exports.CROP_FILTER_ALPHA = exports.KEYPOINT_FILTER_CONFIG = exports.MOVENET_ESTIMATION_CONFIG = exports.MOVENET_CONFIG = exports.MOVENET_MULTIPOSE_DEFAULT_MAX_DIMENSION = exports.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION = exports.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION = exports.MOVENET_MULTIPOSE_LIGHTNING_URL = exports.MOVENET_SINGLEPOSE_THUNDER_URL = exports.MOVENET_SINGLEPOSE_LIGHTNING_URL = exports.VALID_MODELS = exports.MULTIPOSE_LIGHTNING = exports.SINGLEPOSE_THUNDER = exports.SINGLEPOSE_LIGHTNING = void 0;\nexports.SINGLEPOSE_LIGHTNING = 'SinglePose.Lightning';\nexports.SINGLEPOSE_THUNDER = 'SinglePose.Thunder';\nexports.MULTIPOSE_LIGHTNING = 'MultiPose.Lightning';\nexports.VALID_MODELS = [exports.SINGLEPOSE_LIGHTNING, exports.SINGLEPOSE_THUNDER, exports.MULTIPOSE_LIGHTNING];\nexports.MOVENET_SINGLEPOSE_LIGHTNING_URL = 'https://tfhub.dev/google/tfjs-model/movenet/singlepose/lightning/4';\nexports.MOVENET_SINGLEPOSE_THUNDER_URL = 'https://tfhub.dev/google/tfjs-model/movenet/singlepose/thunder/4';\nexports.MOVENET_MULTIPOSE_LIGHTNING_URL = 'https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1';\nexports.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION = 192;\nexports.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION = 256;\nexports.MOVENET_MULTIPOSE_DEFAULT_MAX_DIMENSION = 256;\n// The default configuration for loading MoveNet.\nexports.MOVENET_CONFIG = {\n    modelType: exports.SINGLEPOSE_LIGHTNING,\n    enableSmoothing: true\n};\nexports.MOVENET_ESTIMATION_CONFIG = {};\nexports.KEYPOINT_FILTER_CONFIG = {\n    frequency: 30,\n    minCutOff: 2.5,\n    beta: 300.0,\n    derivateCutOff: 2.5,\n    thresholdCutOff: 0.5,\n    thresholdBeta: 5.0,\n    disableValueScaling: true,\n};\nexports.CROP_FILTER_ALPHA = 0.9;\nexports.MIN_CROP_KEYPOINT_SCORE = 0.2;\nexports.DEFAULT_MIN_POSE_SCORE = 0.25;\nexports.NUM_KEYPOINTS = 17;\nexports.NUM_KEYPOINT_VALUES = 3; // [y, x, score]\nexports.MULTIPOSE_BOX_SIZE = 5; // [ymin, xmin, ymax, xmax, score]\nexports.MULTIPOSE_BOX_IDX = exports.NUM_KEYPOINTS * exports.NUM_KEYPOINT_VALUES;\nexports.MULTIPOSE_BOX_SCORE_IDX = exports.MULTIPOSE_BOX_IDX + 4;\nexports.MULTIPOSE_INSTANCE_SIZE = exports.NUM_KEYPOINTS * exports.NUM_KEYPOINT_VALUES + exports.MULTIPOSE_BOX_SIZE;\nexports.DEFAULT_KEYPOINT_TRACKER_CONFIG = {\n    maxTracks: 18,\n    maxAge: 1000,\n    minSimilarity: 0.2,\n    keypointTrackerParams: {\n        keypointConfidenceThreshold: 0.3,\n        // From COCO:\n        // https://cocodataset.org/#keypoints-eval\n        keypointFalloff: [\n            0.026, 0.025, 0.025, 0.035, 0.035, 0.079, 0.079, 0.072, 0.072, 0.062,\n            0.062, 0.107, 0.107, 0.087, 0.087, 0.089, 0.089\n        ],\n        minNumberOfKeypoints: 4\n    }\n};\nexports.DEFAULT_BOUNDING_BOX_TRACKER_CONFIG = {\n    maxTracks: 18,\n    maxAge: 1000,\n    minSimilarity: 0.15,\n    trackerParams: {}\n};\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.initCropRegion = exports.determineNextCropRegion = exports.torsoVisible = void 0;\nvar constants_1 = require(\"../constants\");\nvar constants_2 = require(\"./constants\");\n/**\n * Determines whether the torso of a person is visible.\n *\n * @param keypoints An array of `Keypoint`s associated with a person.\n * @param keypointIndexByName A map from keypoint name to index in the keypoints\n *     array.\n * @return A boolean indicating whether the torso is visible.\n */\nfunction torsoVisible(keypoints, keypointIndexByName) {\n    return ((keypoints[keypointIndexByName['left_hip']].score >\n        constants_2.MIN_CROP_KEYPOINT_SCORE ||\n        keypoints[keypointIndexByName['right_hip']].score >\n            constants_2.MIN_CROP_KEYPOINT_SCORE) &&\n        (keypoints[keypointIndexByName['left_shoulder']].score >\n            constants_2.MIN_CROP_KEYPOINT_SCORE ||\n            keypoints[keypointIndexByName['right_shoulder']].score >\n                constants_2.MIN_CROP_KEYPOINT_SCORE));\n}\nexports.torsoVisible = torsoVisible;\n/**\n * Calculates the maximum distance from each keypoint to the center location.\n * The function returns the maximum distances from the two sets of keypoints:\n * full 17 keypoints and 4 torso keypoints. The returned information will be\n * used to determine the crop size. See determineCropRegion for more detail.\n *\n * @param keypoints An array of `Keypoint`s associated with a person.\n * @param keypointIndexByName A map from keypoint name to index in the keypoints\n *     array.\n * @param targetKeypoints Maps from joint names to coordinates.\n * @param centerY The Y coordinate of the center of the person.\n * @param centerX The X coordinate of the center of the person.\n * @return An array containing information about the torso and body range in the\n *     image: [maxTorsoYrange, maxTorsoXrange, maxBodyYrange, maxBodyXrange].\n */\nfunction determineTorsoAndBodyRange(keypoints, keypointIndexByName, targetKeypoints, centerY, centerX) {\n    var torsoJoints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'];\n    var maxTorsoYrange = 0.0;\n    var maxTorsoXrange = 0.0;\n    for (var i = 0; i < torsoJoints.length; i++) {\n        var distY = Math.abs(centerY - targetKeypoints[torsoJoints[i]][0]);\n        var distX = Math.abs(centerX - targetKeypoints[torsoJoints[i]][1]);\n        if (distY > maxTorsoYrange) {\n            maxTorsoYrange = distY;\n        }\n        if (distX > maxTorsoXrange) {\n            maxTorsoXrange = distX;\n        }\n    }\n    var maxBodyYrange = 0.0;\n    var maxBodyXrange = 0.0;\n    for (var _i = 0, _a = Object.keys(targetKeypoints); _i < _a.length; _i++) {\n        var key = _a[_i];\n        if (keypoints[keypointIndexByName[key]].score < constants_2.MIN_CROP_KEYPOINT_SCORE) {\n            continue;\n        }\n        var distY = Math.abs(centerY - targetKeypoints[key][0]);\n        var distX = Math.abs(centerX - targetKeypoints[key][1]);\n        if (distY > maxBodyYrange) {\n            maxBodyYrange = distY;\n        }\n        if (distX > maxBodyXrange) {\n            maxBodyXrange = distX;\n        }\n    }\n    return [maxTorsoYrange, maxTorsoXrange, maxBodyYrange, maxBodyXrange];\n}\n/**\n * Determines the region to crop the image for the model to run inference on.\n * The algorithm uses the detected joints from the previous frame to estimate\n * the square region that encloses the full body of the target person and\n * centers at the midpoint of two hip joints. The crop size is determined by\n * the distances between each joint and the center point.\n * When the model is not confident with the four torso joint predictions, the\n * function returns a default crop which is the full image padded to square.\n *\n * @param currentCropRegion The crop region that was used for the current frame.\n *     Can be null for the very first frame that is handled by the detector.\n * @param keypoints An array of `Keypoint`s associated with a person.\n * @param keypointIndexByName A map from keypoint name to index in the keypoints\n *     array.\n * @param imageSize The size of the image that is being processed.\n * @return A `BoundingBox` that contains the new crop region.\n */\nfunction determineNextCropRegion(currentCropRegion, keypoints, keypointIndexByName, imageSize) {\n    var targetKeypoints = {};\n    for (var _i = 0, COCO_KEYPOINTS_1 = constants_1.COCO_KEYPOINTS; _i < COCO_KEYPOINTS_1.length; _i++) {\n        var key = COCO_KEYPOINTS_1[_i];\n        targetKeypoints[key] = [\n            keypoints[keypointIndexByName[key]].y * imageSize.height,\n            keypoints[keypointIndexByName[key]].x * imageSize.width\n        ];\n    }\n    if (torsoVisible(keypoints, keypointIndexByName)) {\n        var centerY = (targetKeypoints['left_hip'][0] + targetKeypoints['right_hip'][0]) / 2;\n        var centerX = (targetKeypoints['left_hip'][1] + targetKeypoints['right_hip'][1]) / 2;\n        var _a = determineTorsoAndBodyRange(keypoints, keypointIndexByName, targetKeypoints, centerY, centerX), maxTorsoYrange = _a[0], maxTorsoXrange = _a[1], maxBodyYrange = _a[2], maxBodyXrange = _a[3];\n        var cropLengthHalf = Math.max(maxTorsoXrange * 1.9, maxTorsoYrange * 1.9, maxBodyYrange * 1.2, maxBodyXrange * 1.2);\n        cropLengthHalf = Math.min(cropLengthHalf, Math.max(centerX, imageSize.width - centerX, centerY, imageSize.height - centerY));\n        var cropCorner = [centerY - cropLengthHalf, centerX - cropLengthHalf];\n        if (cropLengthHalf > Math.max(imageSize.width, imageSize.height) / 2) {\n            return initCropRegion(currentCropRegion == null, imageSize);\n        }\n        else {\n            var cropLength = cropLengthHalf * 2;\n            return {\n                yMin: cropCorner[0] / imageSize.height,\n                xMin: cropCorner[1] / imageSize.width,\n                yMax: (cropCorner[0] + cropLength) / imageSize.height,\n                xMax: (cropCorner[1] + cropLength) / imageSize.width,\n                height: (cropCorner[0] + cropLength) / imageSize.height -\n                    cropCorner[0] / imageSize.height,\n                width: (cropCorner[1] + cropLength) / imageSize.width -\n                    cropCorner[1] / imageSize.width\n            };\n        }\n    }\n    else {\n        return initCropRegion(currentCropRegion == null, imageSize);\n    }\n}\nexports.determineNextCropRegion = determineNextCropRegion;\n/**\n * Provides initial crop region.\n *\n * The function provides the initial crop region when the algorithm cannot\n * reliably determine the crop region from the previous frame. There are two\n * scenarios:\n *   1) The very first frame: the function returns the best guess by cropping\n *      a square in the middle of the image.\n *   2) Not enough reliable keypoints detected from the previous frame: the\n *      function pads the full image from both sides to make it a square\n *      image.\n *\n * @param firstFrame A boolean indicating whether we are initializing a crop\n *     region for the very first frame.\n * @param imageSize The size of the image that is being processed.\n * @return A `BoundingBox` that contains the initial crop region.\n */\nfunction initCropRegion(firstFrame, imageSize) {\n    var boxHeight, boxWidth, yMin, xMin;\n    if (firstFrame) {\n        // If it is the first frame, perform a best guess by making the square\n        // crop at the image center to better utilize the image pixels and\n        // create higher chance to enter the cropping loop.\n        if (imageSize.width > imageSize.height) {\n            boxHeight = 1.0;\n            boxWidth = imageSize.height / imageSize.width;\n            yMin = 0.0;\n            xMin = (imageSize.width / 2 - imageSize.height / 2) / imageSize.width;\n        }\n        else {\n            boxHeight = imageSize.width / imageSize.height;\n            boxWidth = 1.0;\n            yMin = (imageSize.height / 2 - imageSize.width / 2) / imageSize.height;\n            xMin = 0.0;\n        }\n    }\n    else {\n        // No cropRegion was available from a previous estimatePoses() call, so\n        // run the model on the full image with padding on both sides.\n        if (imageSize.width > imageSize.height) {\n            boxHeight = imageSize.width / imageSize.height;\n            boxWidth = 1.0;\n            yMin = (imageSize.height / 2 - imageSize.width / 2) / imageSize.height;\n            xMin = 0.0;\n        }\n        else {\n            boxHeight = 1.0;\n            boxWidth = imageSize.height / imageSize.width;\n            yMin = 0.0;\n            xMin = (imageSize.width / 2 - imageSize.height / 2) / imageSize.width;\n        }\n    }\n    return {\n        yMin: yMin,\n        xMin: xMin,\n        yMax: yMin + boxHeight,\n        xMax: xMin + boxWidth,\n        height: boxHeight,\n        width: boxWidth\n    };\n}\nexports.initCropRegion = initCropRegion;\n//# sourceMappingURL=crop_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.mergeBoundingBoxTrackerConfig = exports.mergeKeypointTrackerConfig = exports.validateEstimationConfig = exports.validateModelConfig = void 0;\nvar types_1 = require(\"../calculators/types\");\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig == null ? constants_1.MOVENET_CONFIG : __assign({}, modelConfig);\n    if (config.modelType == null) {\n        config.modelType = 'SinglePose.Lightning';\n    }\n    else if (constants_1.VALID_MODELS.indexOf(config.modelType) < 0) {\n        throw new Error(\"Invalid architecture \" + config.modelType + \". \" +\n            (\"Should be one of \" + constants_1.VALID_MODELS));\n    }\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = true;\n    }\n    if (config.minPoseScore != null &&\n        (config.minPoseScore < 0.0 || config.minPoseScore > 1.0)) {\n        throw new Error(\"minPoseScore should be between 0.0 and 1.0\");\n    }\n    if (config.multiPoseMaxDimension != null &&\n        (config.multiPoseMaxDimension % 32 !== 0 ||\n            config.multiPoseMaxDimension < 32)) {\n        throw new Error(\"multiPoseMaxDimension must be a multiple of 32 and higher than 0\");\n    }\n    if (config.modelType === constants_1.MULTIPOSE_LIGHTNING &&\n        config.enableTracking == null) {\n        config.enableTracking = true;\n    }\n    if (config.modelType === constants_1.MULTIPOSE_LIGHTNING &&\n        config.enableTracking === true) {\n        if (config.trackerType == null) {\n            config.trackerType = types_1.TrackerType.BoundingBox;\n        }\n        if (config.trackerType === types_1.TrackerType.Keypoint) {\n            if (config.trackerConfig != null) {\n                config.trackerConfig = mergeKeypointTrackerConfig(config.trackerConfig);\n            }\n            else {\n                config.trackerConfig = constants_1.DEFAULT_KEYPOINT_TRACKER_CONFIG;\n            }\n        }\n        else if (config.trackerType === types_1.TrackerType.BoundingBox) {\n            if (config.trackerConfig != null) {\n                config.trackerConfig =\n                    mergeBoundingBoxTrackerConfig(config.trackerConfig);\n            }\n            else {\n                config.trackerConfig = constants_1.DEFAULT_BOUNDING_BOX_TRACKER_CONFIG;\n            }\n        }\n        else {\n            throw new Error('Tracker type not supported by MoveNet');\n        }\n        // We don't need to validate the trackerConfig here because the tracker will\n        // take care of that.\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    var config = estimationConfig == null ? constants_1.MOVENET_ESTIMATION_CONFIG : __assign({}, estimationConfig);\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\nfunction mergeBaseTrackerConfig(defaultConfig, userConfig) {\n    var mergedConfig = {\n        maxTracks: defaultConfig.maxTracks,\n        maxAge: defaultConfig.maxAge,\n        minSimilarity: defaultConfig.minSimilarity,\n    };\n    if (userConfig.maxTracks != null) {\n        mergedConfig.maxTracks = userConfig.maxTracks;\n    }\n    if (userConfig.maxAge != null) {\n        mergedConfig.maxAge = userConfig.maxAge;\n    }\n    if (userConfig.minSimilarity != null) {\n        mergedConfig.minSimilarity = userConfig.minSimilarity;\n    }\n    return mergedConfig;\n}\nfunction mergeKeypointTrackerConfig(userConfig) {\n    var mergedConfig = mergeBaseTrackerConfig(constants_1.DEFAULT_KEYPOINT_TRACKER_CONFIG, userConfig);\n    mergedConfig.keypointTrackerParams = __assign({}, constants_1.DEFAULT_KEYPOINT_TRACKER_CONFIG.keypointTrackerParams);\n    if (userConfig.keypointTrackerParams != null) {\n        if (userConfig.keypointTrackerParams.keypointConfidenceThreshold != null) {\n            mergedConfig.keypointTrackerParams.keypointConfidenceThreshold =\n                userConfig.keypointTrackerParams.keypointConfidenceThreshold;\n        }\n        if (userConfig.keypointTrackerParams.keypointFalloff != null) {\n            mergedConfig.keypointTrackerParams.keypointFalloff =\n                userConfig.keypointTrackerParams.keypointFalloff;\n        }\n        if (userConfig.keypointTrackerParams.minNumberOfKeypoints != null) {\n            mergedConfig.keypointTrackerParams.minNumberOfKeypoints =\n                userConfig.keypointTrackerParams.minNumberOfKeypoints;\n        }\n    }\n    return mergedConfig;\n}\nexports.mergeKeypointTrackerConfig = mergeKeypointTrackerConfig;\nfunction mergeBoundingBoxTrackerConfig(userConfig) {\n    var mergedConfig = mergeBaseTrackerConfig(constants_1.DEFAULT_BOUNDING_BOX_TRACKER_CONFIG, userConfig);\n    return mergedConfig;\n}\nexports.mergeBoundingBoxTrackerConfig = mergeBoundingBoxTrackerConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.load = void 0;\nvar tfconv = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar convert_image_to_tensor_1 = require(\"../shared/calculators/convert_image_to_tensor\");\nvar image_utils_1 = require(\"../shared/calculators/image_utils\");\nvar shift_image_value_1 = require(\"../shared/calculators/shift_image_value\");\nvar decode_multiple_poses_1 = require(\"./calculators/decode_multiple_poses\");\nvar decode_single_pose_1 = require(\"./calculators/decode_single_pose\");\nvar flip_poses_1 = require(\"./calculators/flip_poses\");\nvar scale_poses_1 = require(\"./calculators/scale_poses\");\nvar constants_1 = require(\"./constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\nvar load_utils_1 = require(\"./load_utils\");\n/**\n * PoseNet detector class.\n */\nvar PosenetDetector = /** @class */ (function () {\n    function PosenetDetector(posenetModel, config) {\n        this.posenetModel = posenetModel;\n        // validate params.\n        var inputShape = this.posenetModel.inputs[0].shape;\n        tf.util.assert((inputShape[1] === -1) && (inputShape[2] === -1), function () { return \"Input shape [\" + inputShape[1] + \", \" + inputShape[2] + \"] \" +\n            \"must both be equal to or -1\"; });\n        var validInputResolution = load_utils_1.getValidInputResolutionDimensions(config.inputResolution, config.outputStride);\n        detector_utils_1.assertValidOutputStride(config.outputStride);\n        detector_utils_1.assertValidResolution(validInputResolution, config.outputStride);\n        this.inputResolution = validInputResolution;\n        this.outputStride = config.outputStride;\n        this.architecture = config.architecture;\n    }\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * This does standard ImageNet pre-processing before inferring through the\n     * model. The image should pixels should have values [0-255]. It returns a\n     * single pose or multiple poses based on the maxPose parameter from the\n     * `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param config\n     *       maxPoses: Optional. Max number of poses to estimate.\n     *       When maxPoses = 1, a single pose is detected, it is usually much more\n     *       efficient than maxPoses > 1. When maxPoses > 1, multiple poses are\n     *       detected.\n     *\n     *       flipHorizontal: Optional. Default to false. When image data comes\n     *       from camera, the result has to flip horizontally.\n     *\n     * @return An array of `Pose`s.\n     */\n    PosenetDetector.prototype.estimatePoses = function (image, estimationConfig) {\n        if (estimationConfig === void 0) { estimationConfig = constants_1.SINGLE_PERSON_ESTIMATION_CONFIG; }\n        return __awaiter(this, void 0, void 0, function () {\n            var config, _a, imageTensor, padding, imageValueShifted, results, offsets, heatmap, displacementFwd, displacementBwd, heatmapScores, poses, pose, imageSize, scaledPoses;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        config = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            return [2 /*return*/, []];\n                        }\n                        this.maxPoses = config.maxPoses;\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, {\n                            outputTensorSize: this.inputResolution,\n                            keepAspectRatio: true,\n                            borderMode: 'replicate'\n                        }), imageTensor = _a.imageTensor, padding = _a.padding;\n                        imageValueShifted = this.architecture === 'ResNet50' ?\n                            tf.add(imageTensor, constants_1.RESNET_MEAN) :\n                            shift_image_value_1.shiftImageValue(imageTensor, [-1, 1]);\n                        results = this.posenetModel.predict(imageValueShifted);\n                        if (this.architecture === 'ResNet50') {\n                            offsets = tf.squeeze(results[2], [0]);\n                            heatmap = tf.squeeze(results[3], [0]);\n                            displacementFwd = tf.squeeze(results[0], [0]);\n                            displacementBwd = tf.squeeze(results[1], [0]);\n                        }\n                        else {\n                            offsets = tf.squeeze(results[0], [0]);\n                            heatmap = tf.squeeze(results[1], [0]);\n                            displacementFwd = tf.squeeze(results[2], [0]);\n                            displacementBwd = tf.squeeze(results[3], [0]);\n                        }\n                        heatmapScores = tf.sigmoid(heatmap);\n                        if (!(this.maxPoses === 1)) return [3 /*break*/, 2];\n                        return [4 /*yield*/, decode_single_pose_1.decodeSinglePose(heatmapScores, offsets, this.outputStride)];\n                    case 1:\n                        pose = _b.sent();\n                        poses = [pose];\n                        return [3 /*break*/, 4];\n                    case 2: return [4 /*yield*/, decode_multiple_poses_1.decodeMultiplePoses(heatmapScores, offsets, displacementFwd, displacementBwd, this.outputStride, this.maxPoses, config.scoreThreshold, config.nmsRadius)];\n                    case 3:\n                        poses = _b.sent();\n                        _b.label = 4;\n                    case 4:\n                        imageSize = image_utils_1.getImageSize(image);\n                        scaledPoses = scale_poses_1.scalePoses(poses, imageSize, this.inputResolution, padding);\n                        if (config.flipHorizontal) {\n                            scaledPoses = flip_poses_1.flipPosesHorizontal(scaledPoses, imageSize);\n                        }\n                        imageTensor.dispose();\n                        imageValueShifted.dispose();\n                        tf.dispose(results);\n                        offsets.dispose();\n                        heatmap.dispose();\n                        displacementFwd.dispose();\n                        displacementBwd.dispose();\n                        heatmapScores.dispose();\n                        return [2 /*return*/, scaledPoses];\n                }\n            });\n        });\n    };\n    PosenetDetector.prototype.dispose = function () {\n        this.posenetModel.dispose();\n    };\n    PosenetDetector.prototype.reset = function () {\n        // No-op. There's no global state.\n    };\n    return PosenetDetector;\n}());\n/**\n * Loads the PoseNet model instance from a checkpoint, with the ResNet\n * or MobileNet architecture. The model to be loaded is configurable using the\n * config dictionary ModelConfig. Please find more details in the\n * documentation of the ModelConfig.\n *\n * @param config ModelConfig dictionary that contains parameters for\n * the PoseNet loading process. Please find more details of each parameters\n * in the documentation of the ModelConfig interface. The predefined\n * `MOBILENET_V1_CONFIG` and `RESNET_CONFIG` can also be used as references\n * for defining your customized config.\n */\nfunction load(modelConfig) {\n    if (modelConfig === void 0) { modelConfig = constants_1.MOBILENET_V1_CONFIG; }\n    return __awaiter(this, void 0, void 0, function () {\n        var config, defaultUrl_1, model_1, defaultUrl, model;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    if (!(config.architecture === 'ResNet50')) return [3 /*break*/, 2];\n                    defaultUrl_1 = load_utils_1.resNet50Checkpoint(config.outputStride, config.quantBytes);\n                    return [4 /*yield*/, tfconv.loadGraphModel(config.modelUrl || defaultUrl_1)];\n                case 1:\n                    model_1 = _a.sent();\n                    return [2 /*return*/, new PosenetDetector(model_1, config)];\n                case 2:\n                    defaultUrl = load_utils_1.mobileNetCheckpoint(config.outputStride, config.multiplier, config.quantBytes);\n                    return [4 /*yield*/, tfconv.loadGraphModel(config.modelUrl || defaultUrl)];\n                case 3:\n                    model = _a.sent();\n                    return [2 /*return*/, new PosenetDetector(model, config)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.decodeMultiplePoses = void 0;\nvar constants_1 = require(\"../constants\");\nvar build_part_with_score_queue_1 = require(\"./build_part_with_score_queue\");\nvar decode_multiple_poses_util_1 = require(\"./decode_multiple_poses_util\");\n/**\n * Detects multiple poses and finds their parts from part scores and\n * displacement vectors. It returns up to `maxDetections` object instance\n * detections in decreasing root score order. It works as follows: We first\n * create a priority queue with local part score maxima above\n * `scoreThreshold`, considering all parts at the same time. Then we\n * iteratively pull the top  element of the queue (in decreasing score order)\n * and treat it as a root candidate for a new object instance. To avoid\n * duplicate detections, we reject the root candidate if it is within a disk\n * of `nmsRadius` pixels from the corresponding part of a previously detected\n * instance, which is a form of part-based non-maximum suppression (NMS). If\n * the root candidate passes the NMS check, we start a new object instance\n * detection, treating the corresponding part as root and finding the\n * positions of the remaining parts by following the displacement vectors\n * along the tree-structured part graph. We assign to the newly detected\n * instance a score equal to the sum of scores of its parts which have not\n * been claimed by a previous instance (i.e., those at least `nmsRadius`\n * pixels away from the corresponding part of all previously detected\n * instances), divided by the total number of parts `numParts`.\n *\n * @param heatmapScores 3-D tensor with shape `[height, width, numParts]`.\n * The value of heatmapScores[y, x, k]` is the score of placing the `k`-th\n * object part at position `(y, x)`.\n *\n * @param offsets 3-D tensor with shape `[height, width, numParts * 2]`.\n * The value of [offsets[y, x, k], offsets[y, x, k + numParts]]` is the\n * short range offset vector of the `k`-th  object part at heatmap\n * position `(y, x)`.\n *\n * @param displacementsFwd 3-D tensor of shape\n * `[height, width, 2 * num_edges]`, where `num_edges = num_parts - 1` is the\n * number of edges (parent-child pairs) in the tree. It contains the forward\n * displacements between consecutive part from the root towards the leaves.\n *\n * @param displacementsBwd 3-D tensor of shape\n * `[height, width, 2 * num_edges]`, where `num_edges = num_parts - 1` is the\n * number of edges (parent-child pairs) in the tree. It contains the backward\n * displacements between consecutive part from the root towards the leaves.\n *\n * @param outputStride The output stride that was used when feed-forwarding\n * through the PoseNet model.  Must be 32, 16, or 8.\n *\n * @param maxPoseDetections Maximum number of returned instance detections per\n * image.\n *\n * @param scoreThreshold Only return instance detections that have root part\n * score greater or equal to this value. Defaults to 0.5.\n *\n * @param nmsRadius Non-maximum suppression part distance. It needs to be\n * strictly positive. Two parts suppress each other if they are less than\n * `nmsRadius` pixels away. Defaults to 20.\n *\n * @return An array of poses and their scores, each containing keypoints and\n * the corresponding keypoint scores.\n */\nfunction decodeMultiplePoses(heatmapScores, offsets, displacementFwd, displacementBwd, outputStride, maxPoseDetections, scoreThreshold, nmsRadius) {\n    if (scoreThreshold === void 0) { scoreThreshold = 0.5; }\n    if (nmsRadius === void 0) { nmsRadius = 20; }\n    return __awaiter(this, void 0, void 0, function () {\n        var _a, scoresBuffer, offsetsBuffer, displacementsFwdBuffer, displacementsBwdBuffer, poses, queue, squaredNmsRadius, root, rootImageCoords, keypoints, score;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0: return [4 /*yield*/, decode_multiple_poses_util_1.toTensorBuffers3D([heatmapScores, offsets, displacementFwd, displacementBwd])];\n                case 1:\n                    _a = _b.sent(), scoresBuffer = _a[0], offsetsBuffer = _a[1], displacementsFwdBuffer = _a[2], displacementsBwdBuffer = _a[3];\n                    poses = [];\n                    queue = build_part_with_score_queue_1.buildPartWithScoreQueue(scoreThreshold, constants_1.K_LOCAL_MAXIMUM_RADIUS, scoresBuffer);\n                    squaredNmsRadius = nmsRadius * nmsRadius;\n                    // Generate at most maxDetections object instances per image in\n                    // decreasing root part score order.\n                    while (poses.length < maxPoseDetections && !queue.empty()) {\n                        root = queue.dequeue();\n                        rootImageCoords = decode_multiple_poses_util_1.getImageCoords(root.part, outputStride, offsetsBuffer);\n                        if (decode_multiple_poses_util_1.withinNmsRadiusOfCorrespondingPoint(poses, squaredNmsRadius, rootImageCoords, root.part.id)) {\n                            continue;\n                        }\n                        keypoints = decode_multiple_poses_util_1.decodePose(root, scoresBuffer, offsetsBuffer, outputStride, displacementsFwdBuffer, displacementsBwdBuffer);\n                        score = decode_multiple_poses_util_1.getInstanceScore(poses, squaredNmsRadius, keypoints);\n                        poses.push({ keypoints: keypoints, score: score });\n                    }\n                    return [2 /*return*/, poses];\n            }\n        });\n    });\n}\nexports.decodeMultiplePoses = decodeMultiplePoses;\n//# sourceMappingURL=decode_multiple_poses.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.POSE_CHAIN = exports.NUM_KEYPOINTS = exports.K_LOCAL_MAXIMUM_RADIUS = exports.RESNET_MEAN = exports.MULTI_PERSON_ESTIMATION_CONFIG = exports.SINGLE_PERSON_ESTIMATION_CONFIG = exports.VALID_QUANT_BYTES = exports.VALID_MULTIPLIER = exports.VALID_OUTPUT_STRIDES = exports.VALID_STRIDE = exports.VALID_ARCHITECTURE = exports.MOBILENET_V1_CONFIG = void 0;\n// The default configuration for loading MobileNetV1 based PoseNet.\n//\n// (And for references, the default configuration for loading ResNet\n// based PoseNet is also included).\n//\n// ```\n// const RESNET_CONFIG = {\n//   architecture: 'ResNet50',\n//   outputStride: 32,\n//   quantBytes: 2,\n// } as ModelConfig;\n// ```\nexports.MOBILENET_V1_CONFIG = {\n    architecture: 'MobileNetV1',\n    outputStride: 16,\n    multiplier: 0.75,\n    inputResolution: { height: 257, width: 257 }\n};\nexports.VALID_ARCHITECTURE = ['MobileNetV1', 'ResNet50'];\nexports.VALID_STRIDE = {\n    'MobileNetV1': [8, 16],\n    'ResNet50': [16]\n};\nexports.VALID_OUTPUT_STRIDES = [8, 16, 32];\nexports.VALID_MULTIPLIER = {\n    'MobileNetV1': [0.50, 0.75, 1.0],\n    'ResNet50': [1.0]\n};\nexports.VALID_QUANT_BYTES = [1, 2, 4];\nexports.SINGLE_PERSON_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\nexports.MULTI_PERSON_ESTIMATION_CONFIG = {\n    maxPoses: 5,\n    flipHorizontal: false,\n    scoreThreshold: 0.5,\n    nmsRadius: 20\n};\nexports.RESNET_MEAN = [-123.15, -115.90, -103.06];\n// A point (y, x) is considered as root part candidate if its score is a\n// maximum in a window |y - y'| <= kLocalMaximumRadius, |x - x'| <=\n// kLocalMaximumRadius.\nexports.K_LOCAL_MAXIMUM_RADIUS = 1;\nexports.NUM_KEYPOINTS = 17;\n/*\n * Define the skeleton. This defines the parent->child relationships of our\n * tree. Arbitrarily this defines the nose as the root of the tree, however\n * since we will infer the displacement for both parent->child and\n * child->parent, we can define the tree root as any node.\n */\nexports.POSE_CHAIN = [\n    ['nose', 'left_eye'], ['left_eye', 'left_ear'], ['nose', 'right_eye'],\n    ['right_eye', 'right_ear'], ['nose', 'left_shoulder'],\n    ['left_shoulder', 'left_elbow'], ['left_elbow', 'left_wrist'],\n    ['left_shoulder', 'left_hip'], ['left_hip', 'left_knee'],\n    ['left_knee', 'left_ankle'], ['nose', 'right_shoulder'],\n    ['right_shoulder', 'right_elbow'], ['right_elbow', 'right_wrist'],\n    ['right_shoulder', 'right_hip'], ['right_hip', 'right_knee'],\n    ['right_knee', 'right_ankle']\n];\n//# sourceMappingURL=constants.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.buildPartWithScoreQueue = void 0;\nvar max_heap_1 = require(\"./max_heap\");\nfunction scoreIsMaximumInLocalWindow(keypointId, score, heatmapY, heatmapX, localMaximumRadius, scores) {\n    var _a = scores.shape, height = _a[0], width = _a[1];\n    var localMaximum = true;\n    var yStart = Math.max(heatmapY - localMaximumRadius, 0);\n    var yEnd = Math.min(heatmapY + localMaximumRadius + 1, height);\n    for (var yCurrent = yStart; yCurrent < yEnd; ++yCurrent) {\n        var xStart = Math.max(heatmapX - localMaximumRadius, 0);\n        var xEnd = Math.min(heatmapX + localMaximumRadius + 1, width);\n        for (var xCurrent = xStart; xCurrent < xEnd; ++xCurrent) {\n            if (scores.get(yCurrent, xCurrent, keypointId) > score) {\n                localMaximum = false;\n                break;\n            }\n        }\n        if (!localMaximum) {\n            break;\n        }\n    }\n    return localMaximum;\n}\n/**\n * Builds a priority queue with part candidate positions for a specific image in\n * the batch. For this we find all local maxima in the score maps with score\n * values above a threshold. We create a single priority queue across all parts.\n */\nfunction buildPartWithScoreQueue(scoreThreshold, localMaximumRadius, scores) {\n    var _a = scores.shape, height = _a[0], width = _a[1], numKeypoints = _a[2];\n    var queue = new max_heap_1.MaxHeap(height * width * numKeypoints, function (_a) {\n        var score = _a.score;\n        return score;\n    });\n    for (var heatmapY = 0; heatmapY < height; ++heatmapY) {\n        for (var heatmapX = 0; heatmapX < width; ++heatmapX) {\n            for (var keypointId = 0; keypointId < numKeypoints; ++keypointId) {\n                var score = scores.get(heatmapY, heatmapX, keypointId);\n                // Only consider parts with score greater or equal to threshold as\n                // root candidates.\n                if (score < scoreThreshold) {\n                    continue;\n                }\n                // Only consider keypoints whose score is maximum in a local window.\n                if (scoreIsMaximumInLocalWindow(keypointId, score, heatmapY, heatmapX, localMaximumRadius, scores)) {\n                    queue.enqueue({ score: score, part: { heatmapY: heatmapY, heatmapX: heatmapX, id: keypointId } });\n                }\n            }\n        }\n    }\n    return queue;\n}\nexports.buildPartWithScoreQueue = buildPartWithScoreQueue;\n//# sourceMappingURL=build_part_with_score_queue.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MaxHeap = void 0;\n// algorithm based on Coursera Lecture from Algorithms, Part 1:\n// https://www.coursera.org/learn/algorithms-part1/lecture/ZjoSM/heapsort\nfunction half(k) {\n    return Math.floor(k / 2);\n}\nvar MaxHeap = /** @class */ (function () {\n    function MaxHeap(maxSize, getElementValue) {\n        this.priorityQueue = new Array(maxSize);\n        this.numberOfElements = -1;\n        this.getElementValue = getElementValue;\n    }\n    MaxHeap.prototype.enqueue = function (x) {\n        this.priorityQueue[++this.numberOfElements] = x;\n        this.swim(this.numberOfElements);\n    };\n    MaxHeap.prototype.dequeue = function () {\n        var max = this.priorityQueue[0];\n        this.exchange(0, this.numberOfElements--);\n        this.sink(0);\n        this.priorityQueue[this.numberOfElements + 1] = null;\n        return max;\n    };\n    MaxHeap.prototype.empty = function () {\n        return this.numberOfElements === -1;\n    };\n    MaxHeap.prototype.size = function () {\n        return this.numberOfElements + 1;\n    };\n    MaxHeap.prototype.all = function () {\n        return this.priorityQueue.slice(0, this.numberOfElements + 1);\n    };\n    MaxHeap.prototype.max = function () {\n        return this.priorityQueue[0];\n    };\n    MaxHeap.prototype.swim = function (k) {\n        while (k > 0 && this.less(half(k), k)) {\n            this.exchange(k, half(k));\n            k = half(k);\n        }\n    };\n    MaxHeap.prototype.sink = function (k) {\n        while (2 * k <= this.numberOfElements) {\n            var j = 2 * k;\n            if (j < this.numberOfElements && this.less(j, j + 1)) {\n                j++;\n            }\n            if (!this.less(k, j)) {\n                break;\n            }\n            this.exchange(k, j);\n            k = j;\n        }\n    };\n    MaxHeap.prototype.getValueAt = function (i) {\n        return this.getElementValue(this.priorityQueue[i]);\n    };\n    MaxHeap.prototype.less = function (i, j) {\n        return this.getValueAt(i) < this.getValueAt(j);\n    };\n    MaxHeap.prototype.exchange = function (i, j) {\n        var t = this.priorityQueue[i];\n        this.priorityQueue[i] = this.priorityQueue[j];\n        this.priorityQueue[j] = t;\n    };\n    return MaxHeap;\n}());\nexports.MaxHeap = MaxHeap;\n//# sourceMappingURL=max_heap.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getInstanceScore = exports.decodePose = exports.addVectors = exports.withinNmsRadiusOfCorrespondingPoint = exports.squaredDistance = exports.getImageCoords = exports.getOffsetPoint = exports.toTensorBuffers3D = void 0;\nvar constants_1 = require(\"../../constants\");\nvar constants_2 = require(\"../constants\");\nfunction toTensorBuffers3D(tensors) {\n    return __awaiter(this, void 0, void 0, function () {\n        return __generator(this, function (_a) {\n            return [2 /*return*/, Promise.all(tensors.map(function (tensor) { return tensor.buffer(); }))];\n        });\n    });\n}\nexports.toTensorBuffers3D = toTensorBuffers3D;\nfunction getOffsetPoint(y, x, keypoint, offsets) {\n    return {\n        y: offsets.get(y, x, keypoint),\n        x: offsets.get(y, x, keypoint + constants_2.NUM_KEYPOINTS)\n    };\n}\nexports.getOffsetPoint = getOffsetPoint;\nfunction getImageCoords(part, outputStride, offsets) {\n    var heatmapY = part.heatmapY, heatmapX = part.heatmapX, keypoint = part.id;\n    var _a = getOffsetPoint(heatmapY, heatmapX, keypoint, offsets), y = _a.y, x = _a.x;\n    return {\n        x: part.heatmapX * outputStride + x,\n        y: part.heatmapY * outputStride + y\n    };\n}\nexports.getImageCoords = getImageCoords;\nfunction squaredDistance(y1, x1, y2, x2) {\n    var dy = y2 - y1;\n    var dx = x2 - x1;\n    return dy * dy + dx * dx;\n}\nexports.squaredDistance = squaredDistance;\nfunction withinNmsRadiusOfCorrespondingPoint(poses, squaredNmsRadius, _a, keypointId) {\n    var x = _a.x, y = _a.y;\n    return poses.some(function (_a) {\n        var keypoints = _a.keypoints;\n        return squaredDistance(y, x, keypoints[keypointId].y, keypoints[keypointId].x) <=\n            squaredNmsRadius;\n    });\n}\nexports.withinNmsRadiusOfCorrespondingPoint = withinNmsRadiusOfCorrespondingPoint;\nvar partIds = \n// tslint:disable-next-line: no-unnecessary-type-assertion\nconstants_1.COCO_KEYPOINTS.reduce(function (result, jointName, i) {\n    result[jointName] = i;\n    return result;\n}, {});\nvar parentChildrenTuples = constants_2.POSE_CHAIN.map(function (_a) {\n    var parentJoinName = _a[0], childJoinName = _a[1];\n    return ([partIds[parentJoinName], partIds[childJoinName]]);\n});\nvar parentToChildEdges = parentChildrenTuples.map(function (_a) {\n    var childJointId = _a[1];\n    return childJointId;\n});\nvar childToParentEdges = parentChildrenTuples.map(function (_a) {\n    var parentJointId = _a[0];\n    return parentJointId;\n});\nfunction clamp(a, min, max) {\n    if (a < min) {\n        return min;\n    }\n    if (a > max) {\n        return max;\n    }\n    return a;\n}\nfunction getStridedIndexNearPoint(point, outputStride, height, width) {\n    return {\n        y: clamp(Math.round(point.y / outputStride), 0, height - 1),\n        x: clamp(Math.round(point.x / outputStride), 0, width - 1)\n    };\n}\nfunction getDisplacement(edgeId, point, displacements) {\n    var numEdges = displacements.shape[2] / 2;\n    return {\n        y: displacements.get(point.y, point.x, edgeId),\n        x: displacements.get(point.y, point.x, numEdges + edgeId)\n    };\n}\nfunction addVectors(a, b) {\n    return { x: a.x + b.x, y: a.y + b.y };\n}\nexports.addVectors = addVectors;\n/**\n * We get a new keypoint along the `edgeId` for the pose instance, assuming\n * that the position of the `idSource` part is already known. For this, we\n * follow the displacement vector from the source to target part (stored in\n * the `i`-t channel of the displacement tensor). The displaced keypoint\n * vector is refined using the offset vector by `offsetRefineStep` times.\n */\nfunction traverseToTargetKeypoint(edgeId, sourceKeypoint, targetKeypointId, scoresBuffer, offsets, outputStride, displacements, offsetRefineStep) {\n    if (offsetRefineStep === void 0) { offsetRefineStep = 2; }\n    var _a = scoresBuffer.shape, height = _a[0], width = _a[1];\n    var point = { y: sourceKeypoint.y, x: sourceKeypoint.x };\n    // Nearest neighbor interpolation for the source->target displacements.\n    var sourceKeypointIndices = getStridedIndexNearPoint(point, outputStride, height, width);\n    var displacement = getDisplacement(edgeId, sourceKeypointIndices, displacements);\n    var displacedPoint = addVectors(point, displacement);\n    var targetKeypoint = displacedPoint;\n    for (var i = 0; i < offsetRefineStep; i++) {\n        var targetKeypointIndices = getStridedIndexNearPoint(targetKeypoint, outputStride, height, width);\n        var offsetPoint = getOffsetPoint(targetKeypointIndices.y, targetKeypointIndices.x, targetKeypointId, offsets);\n        targetKeypoint = addVectors({\n            x: targetKeypointIndices.x * outputStride,\n            y: targetKeypointIndices.y * outputStride\n        }, { x: offsetPoint.x, y: offsetPoint.y });\n    }\n    var targetKeyPointIndices = getStridedIndexNearPoint(targetKeypoint, outputStride, height, width);\n    var score = scoresBuffer.get(targetKeyPointIndices.y, targetKeyPointIndices.x, targetKeypointId);\n    return {\n        y: targetKeypoint.y,\n        x: targetKeypoint.x,\n        name: constants_1.COCO_KEYPOINTS[targetKeypointId],\n        score: score\n    };\n}\n/**\n * Follows the displacement fields to decode the full pose of the object\n * instance given the position of a part that acts as root.\n *\n * @return An array of decoded keypoints and their scores for a single pose\n */\nfunction decodePose(root, scores, offsets, outputStride, displacementsFwd, displacementsBwd) {\n    var numParts = scores.shape[2];\n    var numEdges = parentToChildEdges.length;\n    var instanceKeypoints = new Array(numParts);\n    // Start a new detection instance at the position of the root.\n    var rootPart = root.part, rootScore = root.score;\n    var rootPoint = getImageCoords(rootPart, outputStride, offsets);\n    instanceKeypoints[rootPart.id] = {\n        score: rootScore,\n        name: constants_1.COCO_KEYPOINTS[rootPart.id],\n        y: rootPoint.y,\n        x: rootPoint.x\n    };\n    // Decode the part positions upwards in the tree, following the backward\n    // displacements.\n    for (var edge = numEdges - 1; edge >= 0; --edge) {\n        var sourceKeypointId = parentToChildEdges[edge];\n        var targetKeypointId = childToParentEdges[edge];\n        if (instanceKeypoints[sourceKeypointId] &&\n            !instanceKeypoints[targetKeypointId]) {\n            instanceKeypoints[targetKeypointId] = traverseToTargetKeypoint(edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores, offsets, outputStride, displacementsBwd);\n        }\n    }\n    // Decode the part positions downwards in the tree, following the forward\n    // displacements.\n    for (var edge = 0; edge < numEdges; ++edge) {\n        var sourceKeypointId = childToParentEdges[edge];\n        var targetKeypointId = parentToChildEdges[edge];\n        if (instanceKeypoints[sourceKeypointId] &&\n            !instanceKeypoints[targetKeypointId]) {\n            instanceKeypoints[targetKeypointId] = traverseToTargetKeypoint(edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores, offsets, outputStride, displacementsFwd);\n        }\n    }\n    return instanceKeypoints;\n}\nexports.decodePose = decodePose;\n/* Score the newly proposed object instance without taking into account\n * the scores of the parts that overlap with any previously detected\n * instance.\n */\nfunction getInstanceScore(existingPoses, squaredNmsRadius, instanceKeypoints) {\n    var notOverlappedKeypointScores = instanceKeypoints.reduce(function (result, _a, keypointId) {\n        var y = _a.y, x = _a.x, score = _a.score;\n        if (!withinNmsRadiusOfCorrespondingPoint(existingPoses, squaredNmsRadius, { y: y, x: x }, keypointId)) {\n            result += score;\n        }\n        return result;\n    }, 0.0);\n    return notOverlappedKeypointScores /= instanceKeypoints.length;\n}\nexports.getInstanceScore = getInstanceScore;\n//# sourceMappingURL=decode_multiple_poses_util.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.decodeSinglePose = void 0;\nvar constants_1 = require(\"../../constants\");\nvar decode_single_pose_util_1 = require(\"./decode_single_pose_util\");\n/**\n * Detects a single pose and finds its parts from part scores and offset\n * vectors. It returns a single pose detection. It works as follows:\n * argmax2d is done on the scores to get the y and x index in the heatmap\n * with the highest score for each part, which is essentially where the\n * part is most likely to exist. This produces a tensor of size 17x2, with\n * each row being the y and x index in the heatmap for each keypoint.\n * The offset vector for each part is retrieved by getting the\n * y and x from the offsets corresponding to the y and x index in the\n * heatmap for that part. This produces a tensor of size 17x2, with each\n * row being the offset vector for the corresponding keypoint.\n * To get the keypoint, each parts heatmap y and x are multiplied\n * by the output stride then added to their corresponding offset vector,\n * which is in the same scale as the original image.\n *\n * @param heatmapScores 3-D tensor with shape `[height, width, numParts]`.\n * The value of heatmapScores[y, x, k]` is the score of placing the `k`-th\n * object part at position `(y, x)`.\n *\n * @param offsets 3-D tensor with shape `[height, width, numParts * 2]`.\n * The value of [offsets[y, x, k], offsets[y, x, k + numParts]]` is the\n * short range offset vector of the `k`-th  object part at heatmap\n * position `(y, x)`.\n *\n * @param outputStride The output stride that was used when feed-forwarding\n * through the PoseNet model.  Must be 32, 16, or 8.\n *\n * @return A promise that resolves with single pose with a confidence score,\n * which contains an array of keypoints indexed by part id, each with a score\n * and position.\n */\nfunction decodeSinglePose(heatmapScores, offsets, outputStride) {\n    return __awaiter(this, void 0, void 0, function () {\n        var totalScore, heatmapValues, allTensorBuffers, scoresBuffer, offsetsBuffer, heatmapValuesBuffer, offsetPoints, offsetPointsBuffer, keypointConfidence, keypoints;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    totalScore = 0.0;\n                    heatmapValues = decode_single_pose_util_1.argmax2d(heatmapScores);\n                    return [4 /*yield*/, Promise.all([heatmapScores.buffer(), offsets.buffer(), heatmapValues.buffer()])];\n                case 1:\n                    allTensorBuffers = _a.sent();\n                    scoresBuffer = allTensorBuffers[0];\n                    offsetsBuffer = allTensorBuffers[1];\n                    heatmapValuesBuffer = allTensorBuffers[2];\n                    offsetPoints = decode_single_pose_util_1.getOffsetPoints(heatmapValuesBuffer, outputStride, offsetsBuffer);\n                    return [4 /*yield*/, offsetPoints.buffer()];\n                case 2:\n                    offsetPointsBuffer = _a.sent();\n                    keypointConfidence = Array.from(decode_single_pose_util_1.getPointsConfidence(scoresBuffer, heatmapValuesBuffer));\n                    keypoints = keypointConfidence.map(function (score, keypointId) {\n                        totalScore += score;\n                        return {\n                            y: offsetPointsBuffer.get(keypointId, 0),\n                            x: offsetPointsBuffer.get(keypointId, 1),\n                            score: score,\n                            name: constants_1.COCO_KEYPOINTS[keypointId]\n                        };\n                    });\n                    heatmapValues.dispose();\n                    offsetPoints.dispose();\n                    return [2 /*return*/, { keypoints: keypoints, score: totalScore / keypoints.length }];\n            }\n        });\n    });\n}\nexports.decodeSinglePose = decodeSinglePose;\n//# sourceMappingURL=decode_single_pose.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getOffsetVectors = exports.getOffsetPoints = exports.getPointsConfidence = exports.argmax2d = void 0;\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../../constants\");\nfunction mod(a, b) {\n    return tf.tidy(function () {\n        var floored = tf.div(a, tf.scalar(b, 'int32'));\n        return tf.sub(a, tf.mul(floored, tf.scalar(b, 'int32')));\n    });\n}\nfunction argmax2d(inputs) {\n    var _a = inputs.shape, height = _a[0], width = _a[1], depth = _a[2];\n    return tf.tidy(function () {\n        var reshaped = tf.reshape(inputs, [height * width, depth]);\n        var coords = tf.argMax(reshaped, 0);\n        var yCoords = tf.expandDims(tf.div(coords, tf.scalar(width, 'int32')), 1);\n        var xCoords = tf.expandDims(mod(coords, width), 1);\n        return tf.concat([yCoords, xCoords], 1);\n    });\n}\nexports.argmax2d = argmax2d;\nfunction getPointsConfidence(heatmapScores, heatMapCoords) {\n    var numKeypoints = heatMapCoords.shape[0];\n    var result = new Float32Array(numKeypoints);\n    for (var keypoint = 0; keypoint < numKeypoints; keypoint++) {\n        var y = heatMapCoords.get(keypoint, 0);\n        var x = heatMapCoords.get(keypoint, 1);\n        result[keypoint] = heatmapScores.get(y, x, keypoint);\n    }\n    return result;\n}\nexports.getPointsConfidence = getPointsConfidence;\nfunction getOffsetPoints(heatMapCoordsBuffer, outputStride, offsetsBuffer) {\n    return tf.tidy(function () {\n        var offsetVectors = getOffsetVectors(heatMapCoordsBuffer, offsetsBuffer);\n        return tf.add(tf.cast(tf.mul(heatMapCoordsBuffer.toTensor(), tf.scalar(outputStride, 'int32')), 'float32'), offsetVectors);\n    });\n}\nexports.getOffsetPoints = getOffsetPoints;\nfunction getOffsetVectors(heatMapCoordsBuffer, offsetsBuffer) {\n    var result = [];\n    for (var keypoint = 0; keypoint < constants_1.COCO_KEYPOINTS.length; keypoint++) {\n        var heatmapY = heatMapCoordsBuffer.get(keypoint, 0).valueOf();\n        var heatmapX = heatMapCoordsBuffer.get(keypoint, 1).valueOf();\n        var _a = getOffsetPoint(heatmapY, heatmapX, keypoint, offsetsBuffer), x = _a.x, y = _a.y;\n        result.push(y);\n        result.push(x);\n    }\n    return tf.tensor2d(result, [constants_1.COCO_KEYPOINTS.length, 2]);\n}\nexports.getOffsetVectors = getOffsetVectors;\nfunction getOffsetPoint(y, x, keypoint, offsetsBuffer) {\n    return {\n        y: offsetsBuffer.get(y, x, keypoint),\n        x: offsetsBuffer.get(y, x, keypoint + constants_1.COCO_KEYPOINTS.length)\n    };\n}\n//# sourceMappingURL=decode_single_pose_util.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.flipPosesHorizontal = void 0;\nfunction flipPosesHorizontal(poses, imageSize) {\n    for (var _i = 0, poses_1 = poses; _i < poses_1.length; _i++) {\n        var pose = poses_1[_i];\n        for (var _a = 0, _b = pose.keypoints; _a < _b.length; _a++) {\n            var kp = _b[_a];\n            kp.x = imageSize.width - 1 - kp.x;\n        }\n    }\n    return poses;\n}\nexports.flipPosesHorizontal = flipPosesHorizontal;\n//# sourceMappingURL=flip_poses.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.scalePoses = void 0;\nfunction scalePoses(poses, imageSize, inputResolution, padding) {\n    var height = imageSize.height, width = imageSize.width;\n    var scaleY = height / (inputResolution.height * (1 - padding.top - padding.bottom));\n    var scaleX = width / (inputResolution.width * (1 - padding.left - padding.right));\n    var offsetY = -padding.top * inputResolution.height;\n    var offsetX = -padding.left * inputResolution.width;\n    if (scaleX === 1 && scaleY === 1 && offsetY === 0 && offsetX === 0) {\n        return poses;\n    }\n    for (var _i = 0, poses_1 = poses; _i < poses_1.length; _i++) {\n        var pose = poses_1[_i];\n        for (var _a = 0, _b = pose.keypoints; _a < _b.length; _a++) {\n            var kp = _b[_a];\n            kp.x = (kp.x + offsetX) * scaleX;\n            kp.y = (kp.y + offsetY) * scaleY;\n        }\n    }\n    return poses;\n}\nexports.scalePoses = scalePoses;\n//# sourceMappingURL=scale_poses.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.validateEstimationConfig = exports.assertValidResolution = exports.assertValidOutputStride = exports.validateModelConfig = void 0;\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig || constants_1.MOBILENET_V1_CONFIG;\n    if (config.architecture == null) {\n        config.architecture = 'MobileNetV1';\n    }\n    if (constants_1.VALID_ARCHITECTURE.indexOf(config.architecture) < 0) {\n        throw new Error(\"Invalid architecture \" + config.architecture + \". \" +\n            (\"Should be one of \" + constants_1.VALID_ARCHITECTURE));\n    }\n    if (config.inputResolution == null) {\n        config.inputResolution = { height: 257, width: 257 };\n    }\n    if (config.outputStride == null) {\n        config.outputStride = 16;\n    }\n    if (constants_1.VALID_STRIDE[config.architecture].indexOf(config.outputStride) < 0) {\n        throw new Error(\"Invalid outputStride \" + config.outputStride + \". \" +\n            (\"Should be one of \" + constants_1.VALID_STRIDE[config.architecture] + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.multiplier == null) {\n        config.multiplier = 1.0;\n    }\n    if (constants_1.VALID_MULTIPLIER[config.architecture].indexOf(config.multiplier) < 0) {\n        throw new Error(\"Invalid multiplier \" + config.multiplier + \". \" +\n            (\"Should be one of \" + constants_1.VALID_MULTIPLIER[config.architecture] + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.quantBytes == null) {\n        config.quantBytes = 4;\n    }\n    if (constants_1.VALID_QUANT_BYTES.indexOf(config.quantBytes) < 0) {\n        throw new Error(\"Invalid quantBytes \" + config.quantBytes + \". \" +\n            (\"Should be one of \" + constants_1.VALID_QUANT_BYTES + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.architecture === 'MobileNetV1' && config.outputStride === 32 &&\n        config.multiplier !== 1) {\n        throw new Error(\"When using an output stride of 32, \" +\n            \"you must select 1 as the multiplier.\");\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction assertValidOutputStride(outputStride) {\n    tf.util.assert(constants_1.VALID_OUTPUT_STRIDES.indexOf(outputStride) >= 0, function () { return \"outputStride of \" + outputStride + \" is invalid. \" +\n        \"It must be either 8 or 16.\"; });\n}\nexports.assertValidOutputStride = assertValidOutputStride;\nfunction isValidInputResolution(resolution, outputStride) {\n    return (resolution - 1) % outputStride === 0;\n}\nfunction assertValidResolution(resolution, outputStride) {\n    tf.util.assert(isValidInputResolution(resolution.height, outputStride), function () { return \"height of \" + resolution.height + \" is invalid for output stride \" +\n        (outputStride + \".\"); });\n    tf.util.assert(isValidInputResolution(resolution.width, outputStride), function () { return \"width of \" + resolution.width + \" is invalid for output stride \" +\n        (outputStride + \".\"); });\n}\nexports.assertValidResolution = assertValidResolution;\nfunction validateEstimationConfig(estimationConfig) {\n    var config = estimationConfig;\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        // Multi-poses estimation, needs additional check for multi-poses\n        // parameters.\n        config = __assign(__assign({}, constants_1.MULTI_PERSON_ESTIMATION_CONFIG), config);\n        if (config.scoreThreshold < 0.0 || config.scoreThreshold > 1.0) {\n            throw new Error(\"Invalid scoreThreshold \" + config.scoreThreshold + \". \" +\n                \"Should be in range [0.0, 1.0]\");\n        }\n        if (config.nmsRadius <= 0) {\n            throw new Error(\"Invalid nmsRadius \" + config.nmsRadius + \".\");\n        }\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.toValidInputResolution = exports.getValidInputResolutionDimensions = exports.mobileNetCheckpoint = exports.resNet50Checkpoint = void 0;\nvar MOBILENET_BASE_URL = 'https://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/';\nvar RESNET50_BASE_URL = 'https://storage.googleapis.com/tfjs-models/savedmodel/posenet/resnet50/';\n// The PoseNet 2.0 ResNet50 models use the latest TensorFlow.js 1.0 model\n// format.\nfunction resNet50Checkpoint(stride, quantBytes) {\n    var graphJson = \"model-stride\" + stride + \".json\";\n    // quantBytes=4 corresponding to the non-quantized full-precision checkpoints.\n    if (quantBytes === 4) {\n        return RESNET50_BASE_URL + \"float/\" + graphJson;\n    }\n    else {\n        return RESNET50_BASE_URL + (\"quant\" + quantBytes + \"/\") + graphJson;\n    }\n}\nexports.resNet50Checkpoint = resNet50Checkpoint;\n// The PoseNet 2.0 MobileNetV1 models use the latest TensorFlow.js 1.0 model\n// format.\nfunction mobileNetCheckpoint(stride, multiplier, quantBytes) {\n    var toStr = { 1.0: '100', 0.75: '075', 0.50: '050' };\n    var graphJson = \"model-stride\" + stride + \".json\";\n    // quantBytes=4 corresponding to the non-quantized full-precision checkpoints.\n    if (quantBytes === 4) {\n        return MOBILENET_BASE_URL + (\"float/\" + toStr[multiplier] + \"/\") + graphJson;\n    }\n    else {\n        return MOBILENET_BASE_URL + (\"quant\" + quantBytes + \"/\" + toStr[multiplier] + \"/\") +\n            graphJson;\n    }\n}\nexports.mobileNetCheckpoint = mobileNetCheckpoint;\nfunction getValidInputResolutionDimensions(inputResolution, outputStride) {\n    return {\n        height: toValidInputResolution(inputResolution.height, outputStride),\n        width: toValidInputResolution(inputResolution.width, outputStride)\n    };\n}\nexports.getValidInputResolutionDimensions = getValidInputResolutionDimensions;\nfunction toValidInputResolution(inputResolution, outputStride) {\n    if (isValidInputResolution(inputResolution, outputStride)) {\n        return inputResolution;\n    }\n    return Math.floor(inputResolution / outputStride) * outputStride + 1;\n}\nexports.toValidInputResolution = toValidInputResolution;\nfunction isValidInputResolution(resolution, outputStride) {\n    return (resolution - 1) % outputStride === 0;\n}\n//# sourceMappingURL=load_utils.js.map"]}